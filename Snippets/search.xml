<?xml version="1.0" encoding="utf-8"?>
<dictionary>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>BINDING_data_OBJECTS_to_FORMS_and_CONTROLS</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>1_DataBinding_WinForm_with_Simulated_Data_in_ObjectSource__ObjectSource.cs</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>BINDING_data_OBJECTS_to_FORMS_and_CONTROLS</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>1_DataBinding_WinForm_with_Simulated_Data_in_ObjectSource__ObjectSource.cs</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>1_DataBinding_WinForm_with_Simulated_Data_in_ObjectSource__ObjectSource.cs

using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace Data
{
    public class ObjectSource
    {
        private List&lt;Category&gt; _categories;
        private List&lt;Product&gt; _products;

        public ObjectSource()
        {
            _categories = new List&lt;Category&gt;();
            _categories.Add(new Category(1, "Beverages"));
            _categories.Add(new Category(2, "Condiments"));
            _categories.Add(new Category(3, "Confections"));
            _categories.Add(new Category(4, "Dairy Products"));
            _categories.Add(new Category(5, "Grains/Cereals"));
            _categories.Add(new Category(6, "Meat/Poultry"));
            _categories.Add(new Category(7, "Produce"));
            _categories.Add(new Category(8, "Seafood"));

            _products = new List&lt;Product&gt;();
            _products.Add(new Product(1, "Chai", 1, "10 boxes x 20 bags", 18.0m, 39, 0, false));
            _products.Add(new Product(2, "Chang", 1, "24 - 12 oz bottles", 19.0m, 17, 40, false));
            _products.Add(new Product(3, "Aniseed Syrup", 2, "12 - 550 ml bottles", 10.0m, 13, 70, false));
            _products.Add(new Product(4, "Chef Anton's Cajun Seasoning", 2, "48 - 6 oz jars", 22.0m, 53, 0, false));
            _products.Add(new Product(5, "Chef Anton's Gumbo Mix", 2, "36 boxes", 21.35m, 0, 0, true));
            _products.Add(new Product(6, "Grandma's Boysenberry Spread", 2, "12 - 8 oz jars", 25.0m, 120, 0, false));
            _products.Add(new Product(7, "Uncle Bob's Organic Dried Pears", 7, "12 - 1 lb pkgs.", 30.0m, 15, 0, false));
            _products.Add(new Product(8, "Northwoods Cranberry Sauce", 2, "12 - 12 oz jars", 40.0m, 6, 0, false));
            _products.Add(new Product(9, "Mishi Kobe Niku", 6, "18 - 500 g pkgs.", 97.0m, 29, 0, true));
            _products.Add(new Product(10, "Ikura", 8, "12 - 200 ml jars", 31.0m, 31, 0, false));
            _products.Add(new Product(11, "Queso Cabrales", 4, "1 kg pkg.", 21.0m, 22, 30, false));
            _products.Add(new Product(12, "Queso Manchego La Pastora", 4, "10 - 500 g pkgs.", 38.0m, 86, 0, false));
            _products.Add(new Product(13, "Konbu", 8, "2 kg box", 6.0m, 24, 0, false));
            _products.Add(new Product(14, "Tofu", 7, "40 - 100 g pkgs.", 23.25m, 35, 0, false));
            _products.Add(new Product(15, "Genen Shouyu", 2, "24 - 250 ml bottles", 15.5m, 39, 0, false));
            _products.Add(new Product(16, "Pavlova", 3, "32 - 500 g boxes", 17.45m, 29, 0, false));
            _products.Add(new Product(17, "Alice Mutton", 6, "20 - 1 kg tins", 39.0m, 0, 0, true));
            _products.Add(new Product(18, "Carnarvon Tigers", 8, "16 kg pkg.", 62.5m, 42, 0, false));
            _products.Add(new Product(19, "Teatime Chocolate Biscuits", 3, "10 boxes x 12 pieces", 9.2m, 25, 0, false));
            _products.Add(new Product(20, "Sir Rodney's Marmalade", 3, "30 gift boxes", 81.0m, 40, 0, false));
            _products.Add(new Product(21, "Sir Rodney's Scones", 3, "24 pkgs. x 4 pieces", 10.0m, 3, 40, false));
            _products.Add(new Product(22, "Gustaf's Knäckebröd", 5, "24 - 500 g pkgs.", 21.0m, 104, 0, false));
            _products.Add(new Product(23, "Tunnbröd", 5, "12 - 250 g pkgs.", 9.0m, 61, 0, false));
            _products.Add(new Product(24, "Guaraná Fantástica", 1, "12 - 355 ml cans", 4.5m, 20, 0, true));
            _products.Add(new Product(25, "NuNuCa Nuß-Nougat-Creme", 3, "20 - 450 g glasses", 14.0m, 76, 0, false));
            _products.Add(new Product(26, "Gumbär Gummibärchen", 3, "100 - 250 g bags", 31.23m, 15, 0, false));
            _products.Add(new Product(27, "Schoggi Schokolade", 3, "100 - 100 g pieces", 43.9m, 49, 0, false));
            _products.Add(new Product(28, "Rössle Sauerkraut", 7, "25 - 825 g cans", 45.6m, 26, 0, true));
            _products.Add(new Product(29, "Thüringer Rostbratwurst", 6, "50 bags x 30 sausgs.", 123.79m, 0, 0, true));
            _products.Add(new Product(30, "Nord-Ost Matjeshering", 8, "10 - 200 g glasses", 25.89m, 10, 0, false));
            _products.Add(new Product(31, "Gorgonzola Telino", 4, "12 - 100 g pkgs", 12.5m, 0, 70, false));
            _products.Add(new Product(32, "Mascarpone Fabioli", 4, "24 - 200 g pkgs.", 32.0m, 9, 40, false));
            _products.Add(new Product(33, "Geitost", 4, "500 g", 2.5m, 112, 0, false));
            _products.Add(new Product(34, "Sasquatch Ale", 1, "24 - 12 oz bottles", 14.0m, 111, 0, false));
            _products.Add(new Product(35, "Steeleye Stout", 1, "24 - 12 oz bottles", 18.0m, 20, 0, false));
            _products.Add(new Product(36, "Inlagd Sill", 8, "24 - 250 g  jars", 19.0m, 112, 0, false));
            _products.Add(new Product(37, "Gravad lax", 8, "12 - 500 g pkgs.", 26.0m, 11, 50, false));
            _products.Add(new Product(38, "Côte de Blaye", 1, "12 - 75 cl bottles", 263.5m, 17, 0, false));
            _products.Add(new Product(39, "Chartreuse verte", 1, "750 cc per bottle", 18.0m, 69, 0, false));
            _products.Add(new Product(40, "Boston Crab Meat", 8, "24 - 4 oz tins", 18.4m, 123, 0, false));
            _products.Add(new Product(41, "Jack's New England Clam Chowder", 8, "12 - 12 oz cans", 9.65m, 85, 0, false));
            _products.Add(new Product(42, "Singaporean Hokkien Fried Mee", 5, "32 - 1 kg pkgs.", 14.0m, 26, 0, true));
            _products.Add(new Product(43, "Ipoh Coffee", 1, "16 - 500 g tins", 46.0m, 17, 10, false));
            _products.Add(new Product(44, "Gula Malacca", 2, "20 - 2 kg bags", 19.45m, 27, 0, false));
            _products.Add(new Product(45, "Rogede sild", 8, "1k pkg.", 9.5m, 5, 70, false));
            _products.Add(new Product(46, "Spegesild", 8, "4 - 450 g glasses", 12.0m, 95, 0, false));
            _products.Add(new Product(47, "Zaanse koeken", 3, "10 - 4 oz boxes", 9.5m, 36, 0, false));
            _products.Add(new Product(48, "Chocolade", 3, "10 pkgs.", 12.75m, 15, 70, false));
            _products.Add(new Product(49, "Maxilaku", 3, "24 - 50 g pkgs.", 20.0m, 10, 60, false));
            _products.Add(new Product(50, "Valkoinen suklaa", 3, "12 - 100 g bars", 16.25m, 65, 0, false));
            _products.Add(new Product(51, "Manjimup Dried Apples", 7, "50 - 300 g pkgs.", 53.0m, 20, 0, false));
            _products.Add(new Product(52, "Filo Mix", 5, "16 - 2 kg boxes", 7.0m, 38, 0, false));
            _products.Add(new Product(53, "Perth Pasties", 6, "48 pieces", 32.8m, 0, 0, true));
            _products.Add(new Product(54, "Tourtière", 6, "16 pies", 7.45m, 21, 0, false));
            _products.Add(new Product(55, "Pâté chinois", 6, "24 boxes x 2 pies", 24.0m, 115, 0, false));
            _products.Add(new Product(56, "Gnocchi di nonna Alice", 5, "24 - 250 g pkgs.", 38.0m, 21, 10, false));
            _products.Add(new Product(57, "Ravioli Angelo", 5, "24 - 250 g pkgs.", 19.5m, 36, 0, false));
            _products.Add(new Product(58, "Escargots de Bourgogne", 8, "24 pieces", 13.25m, 62, 0, false));
            _products.Add(new Product(59, "Raclette Courdavault", 4, "5 kg pkg.", 55.0m, 79, 0, false));
            _products.Add(new Product(60, "Camembert Pierrot", 4, "15 - 300 g rounds", 34.0m, 19, 0, false));
            _products.Add(new Product(61, "Sirop d'érable", 2, "24 - 500 ml bottles", 28.5m, 113, 0, false));
            _products.Add(new Product(62, "Tarte au sucre", 3, "48 pies", 49.3m, 17, 0, false));
            _products.Add(new Product(63, "Vegie-spread", 2, "15 - 625 g jars", 43.9m, 24, 0, false));
            _products.Add(new Product(64, "Wimmers gute Semmelknödel", 5, "20 bags x 4 pieces", 33.25m, 22, 80, false));
            _products.Add(new Product(65, "Louisiana Fiery Hot Pepper Sauce", 2, "32 - 8 oz bottles", 21.05m, 76, 0, false));
            _products.Add(new Product(66, "Louisiana Hot Spiced Okra", 2, "24 - 8 oz jars", 17.0m, 4, 100, false));
            _products.Add(new Product(67, "Laughing Lumberjack Lager", 1, "24 - 12 oz bottles", 14.0m, 52, 0, false));
            _products.Add(new Product(68, "Scottish Longbreads", 3, "10 boxes x 8 pieces", 12.5m, 6, 10, false));
            _products.Add(new Product(69, "Gudbrandsdalsost", 4, "10 kg pkg.", 36.0m, 26, 0, false));
            _products.Add(new Product(70, "Outback Lager", 1, "24 - 355 ml bottles", 15.0m, 15, 10, false));
            _products.Add(new Product(71, "Flotemysost", 4, "10 - 500 g pkgs.", 21.5m, 26, 0, false));
            _products.Add(new Product(72, "Mozzarella di Giovanni", 4, "24 - 200 g pkgs.", 34.8m, 14, 0, false));
            _products.Add(new Product(73, "Röd Kaviar", 8, "24 - 150 g jars", 15.0m, 101, 0, false));
            _products.Add(new Product(74, "Longlife Tofu", 7, "5 kg pkg.", 10.0m, 4, 20, false));
            _products.Add(new Product(75, "Rhönbräu Klosterbier", 1, "24 - 0.5 l bottles", 7.75m, 125, 0, false));
            _products.Add(new Product(76, "Lakkalikööri", 1, "500 ml", 18.0m, 57, 0, false));
            _products.Add(new Product(77, "Original Frankfurter grüne Soße", 2, "12 boxes", 13.0m, 32, 0, false));
        }

        public IList&lt;Category&gt; GetCategories()
        {
            return _categories;
        }

        public IList&lt;Product&gt; GetProducts(int categoryId)
        {
            var result = from p in _products
                         where p.CategoryId == categoryId
                         select p;

            return result.ToList();
        }

        public void DeleteProduct(Product product)
        {
            _products.Remove(product);
        }

        public void AddProduct(Product product)
        {
            _products.Add(product);
        }

    }
}
</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>BINDING_DataSets_to_FORMS_and_CONTROLS_synching_with_a_DataBase</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>1_simulated_data_DataSet_to_FORMS_and_CONTROLS__ObjectSource.cs</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>BINDING_DataSets_to_FORMS_and_CONTROLS_synching_with_a_DataBase</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>1_simulated_data_DataSet_to_FORMS_and_CONTROLS__ObjectSource.cs</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>1_simulated_data_DataSet_to_FORMS_and_CONTROLS__ObjectSource.cs

using System.Collections.Generic;
using System.Linq;
using System.Windows.Forms;

namespace Data
{
    public class ObjectSource : ISource	//&lt;== Now implements Interface
    {
        private List&lt;Category&gt; _categories;
        private List&lt;Product&gt; _products;

        public ObjectSource()
        {
            _categories = new List&lt;Category&gt;();
            _categories.Add(new Category(1, "Beverages"));
            _categories.Add(new Category(2, "Condiments"));
            _categories.Add(new Category(3, "Confections"));
            _categories.Add(new Category(4, "Dairy Products"));
            _categories.Add(new Category(5, "Grains/Cereals"));
            _categories.Add(new Category(6, "Meat/Poultry"));
            _categories.Add(new Category(7, "Produce"));
            _categories.Add(new Category(8, "Seafood"));

            _products = new List&lt;Product&gt;();
            _products.Add(new Product(1, "Chai", 1, "10 boxes x 20 bags", 18.0m, 39, 0, false));
            _products.Add(new Product(2, "Chang", 1, "24 - 12 oz bottles", 19.0m, 17, 40, false));
            _products.Add(new Product(3, "Aniseed Syrup", 2, "12 - 550 ml bottles", 10.0m, 13, 70, false));
            _products.Add(new Product(4, "Chef Anton's Cajun Seasoning", 2, "48 - 6 oz jars", 22.0m, 53, 0, false));
            _products.Add(new Product(5, "Chef Anton's Gumbo Mix", 2, "36 boxes", 21.35m, 0, 0, true));
            _products.Add(new Product(6, "Grandma's Boysenberry Spread", 2, "12 - 8 oz jars", 25.0m, 120, 0, false));
            _products.Add(new Product(7, "Uncle Bob's Organic Dried Pears", 7, "12 - 1 lb pkgs.", 30.0m, 15, 0, false));
            _products.Add(new Product(8, "Northwoods Cranberry Sauce", 2, "12 - 12 oz jars", 40.0m, 6, 0, false));
            _products.Add(new Product(9, "Mishi Kobe Niku", 6, "18 - 500 g pkgs.", 97.0m, 29, 0, true));
            _products.Add(new Product(10, "Ikura", 8, "12 - 200 ml jars", 31.0m, 31, 0, false));
            _products.Add(new Product(11, "Queso Cabrales", 4, "1 kg pkg.", 21.0m, 22, 30, false));
            _products.Add(new Product(12, "Queso Manchego La Pastora", 4, "10 - 500 g pkgs.", 38.0m, 86, 0, false));
            _products.Add(new Product(13, "Konbu", 8, "2 kg box", 6.0m, 24, 0, false));
            _products.Add(new Product(14, "Tofu", 7, "40 - 100 g pkgs.", 23.25m, 35, 0, false));
            _products.Add(new Product(15, "Genen Shouyu", 2, "24 - 250 ml bottles", 15.5m, 39, 0, false));
            _products.Add(new Product(16, "Pavlova", 3, "32 - 500 g boxes", 17.45m, 29, 0, false));
            _products.Add(new Product(17, "Alice Mutton", 6, "20 - 1 kg tins", 39.0m, 0, 0, true));
            _products.Add(new Product(18, "Carnarvon Tigers", 8, "16 kg pkg.", 62.5m, 42, 0, false));
            _products.Add(new Product(19, "Teatime Chocolate Biscuits", 3, "10 boxes x 12 pieces", 9.2m, 25, 0, false));
            _products.Add(new Product(20, "Sir Rodney's Marmalade", 3, "30 gift boxes", 81.0m, 40, 0, false));
            _products.Add(new Product(21, "Sir Rodney's Scones", 3, "24 pkgs. x 4 pieces", 10.0m, 3, 40, false));
            _products.Add(new Product(22, "Gustaf's Knäckebröd", 5, "24 - 500 g pkgs.", 21.0m, 104, 0, false));
            _products.Add(new Product(23, "Tunnbröd", 5, "12 - 250 g pkgs.", 9.0m, 61, 0, false));
            _products.Add(new Product(24, "Guaraná Fantástica", 1, "12 - 355 ml cans", 4.5m, 20, 0, true));
            _products.Add(new Product(25, "NuNuCa Nuß-Nougat-Creme", 3, "20 - 450 g glasses", 14.0m, 76, 0, false));
            _products.Add(new Product(26, "Gumbär Gummibärchen", 3, "100 - 250 g bags", 31.23m, 15, 0, false));
            _products.Add(new Product(27, "Schoggi Schokolade", 3, "100 - 100 g pieces", 43.9m, 49, 0, false));
            _products.Add(new Product(28, "Rössle Sauerkraut", 7, "25 - 825 g cans", 45.6m, 26, 0, true));
            _products.Add(new Product(29, "Thüringer Rostbratwurst", 6, "50 bags x 30 sausgs.", 123.79m, 0, 0, true));
            _products.Add(new Product(30, "Nord-Ost Matjeshering", 8, "10 - 200 g glasses", 25.89m, 10, 0, false));
            _products.Add(new Product(31, "Gorgonzola Telino", 4, "12 - 100 g pkgs", 12.5m, 0, 70, false));
            _products.Add(new Product(32, "Mascarpone Fabioli", 4, "24 - 200 g pkgs.", 32.0m, 9, 40, false));
            _products.Add(new Product(33, "Geitost", 4, "500 g", 2.5m, 112, 0, false));
            _products.Add(new Product(34, "Sasquatch Ale", 1, "24 - 12 oz bottles", 14.0m, 111, 0, false));
            _products.Add(new Product(35, "Steeleye Stout", 1, "24 - 12 oz bottles", 18.0m, 20, 0, false));
            _products.Add(new Product(36, "Inlagd Sill", 8, "24 - 250 g  jars", 19.0m, 112, 0, false));
            _products.Add(new Product(37, "Gravad lax", 8, "12 - 500 g pkgs.", 26.0m, 11, 50, false));
            _products.Add(new Product(38, "Côte de Blaye", 1, "12 - 75 cl bottles", 263.5m, 17, 0, false));
            _products.Add(new Product(39, "Chartreuse verte", 1, "750 cc per bottle", 18.0m, 69, 0, false));
            _products.Add(new Product(40, "Boston Crab Meat", 8, "24 - 4 oz tins", 18.4m, 123, 0, false));
            _products.Add(new Product(41, "Jack's New England Clam Chowder", 8, "12 - 12 oz cans", 9.65m, 85, 0, false));
            _products.Add(new Product(42, "Singaporean Hokkien Fried Mee", 5, "32 - 1 kg pkgs.", 14.0m, 26, 0, true));
            _products.Add(new Product(43, "Ipoh Coffee", 1, "16 - 500 g tins", 46.0m, 17, 10, false));
            _products.Add(new Product(44, "Gula Malacca", 2, "20 - 2 kg bags", 19.45m, 27, 0, false));
            _products.Add(new Product(45, "Rogede sild", 8, "1k pkg.", 9.5m, 5, 70, false));
            _products.Add(new Product(46, "Spegesild", 8, "4 - 450 g glasses", 12.0m, 95, 0, false));
            _products.Add(new Product(47, "Zaanse koeken", 3, "10 - 4 oz boxes", 9.5m, 36, 0, false));
            _products.Add(new Product(48, "Chocolade", 3, "10 pkgs.", 12.75m, 15, 70, false));
            _products.Add(new Product(49, "Maxilaku", 3, "24 - 50 g pkgs.", 20.0m, 10, 60, false));
            _products.Add(new Product(50, "Valkoinen suklaa", 3, "12 - 100 g bars", 16.25m, 65, 0, false));
            _products.Add(new Product(51, "Manjimup Dried Apples", 7, "50 - 300 g pkgs.", 53.0m, 20, 0, false));
            _products.Add(new Product(52, "Filo Mix", 5, "16 - 2 kg boxes", 7.0m, 38, 0, false));
            _products.Add(new Product(53, "Perth Pasties", 6, "48 pieces", 32.8m, 0, 0, true));
            _products.Add(new Product(54, "Tourtière", 6, "16 pies", 7.45m, 21, 0, false));
            _products.Add(new Product(55, "Pâté chinois", 6, "24 boxes x 2 pies", 24.0m, 115, 0, false));
            _products.Add(new Product(56, "Gnocchi di nonna Alice", 5, "24 - 250 g pkgs.", 38.0m, 21, 10, false));
            _products.Add(new Product(57, "Ravioli Angelo", 5, "24 - 250 g pkgs.", 19.5m, 36, 0, false));
            _products.Add(new Product(58, "Escargots de Bourgogne", 8, "24 pieces", 13.25m, 62, 0, false));
            _products.Add(new Product(59, "Raclette Courdavault", 4, "5 kg pkg.", 55.0m, 79, 0, false));
            _products.Add(new Product(60, "Camembert Pierrot", 4, "15 - 300 g rounds", 34.0m, 19, 0, false));
            _products.Add(new Product(61, "Sirop d'érable", 2, "24 - 500 ml bottles", 28.5m, 113, 0, false));
            _products.Add(new Product(62, "Tarte au sucre", 3, "48 pies", 49.3m, 17, 0, false));
            _products.Add(new Product(63, "Vegie-spread", 2, "15 - 625 g jars", 43.9m, 24, 0, false));
            _products.Add(new Product(64, "Wimmers gute Semmelknödel", 5, "20 bags x 4 pieces", 33.25m, 22, 80, false));
            _products.Add(new Product(65, "Louisiana Fiery Hot Pepper Sauce", 2, "32 - 8 oz bottles", 21.05m, 76, 0, false));
            _products.Add(new Product(66, "Louisiana Hot Spiced Okra", 2, "24 - 8 oz jars", 17.0m, 4, 100, false));
            _products.Add(new Product(67, "Laughing Lumberjack Lager", 1, "24 - 12 oz bottles", 14.0m, 52, 0, false));
            _products.Add(new Product(68, "Scottish Longbreads", 3, "10 boxes x 8 pieces", 12.5m, 6, 10, false));
            _products.Add(new Product(69, "Gudbrandsdalsost", 4, "10 kg pkg.", 36.0m, 26, 0, false));
            _products.Add(new Product(70, "Outback Lager", 1, "24 - 355 ml bottles", 15.0m, 15, 10, false));
            _products.Add(new Product(71, "Flotemysost", 4, "10 - 500 g pkgs.", 21.5m, 26, 0, false));
            _products.Add(new Product(72, "Mozzarella di Giovanni", 4, "24 - 200 g pkgs.", 34.8m, 14, 0, false));
            _products.Add(new Product(73, "Röd Kaviar", 8, "24 - 150 g jars", 15.0m, 101, 0, false));
            _products.Add(new Product(74, "Longlife Tofu", 7, "5 kg pkg.", 10.0m, 4, 20, false));
            _products.Add(new Product(75, "Rhönbräu Klosterbier", 1, "24 - 0.5 l bottles", 7.75m, 125, 0, false));
            _products.Add(new Product(76, "Lakkalikööri", 1, "500 ml", 18.0m, 57, 0, false));
            _products.Add(new Product(77, "Original Frankfurter grüne Soße", 2, "12 boxes", 13.0m, 32, 0, false));
        }

        public object GetCategories()
        {
            return _categories;
        }

        public object GetProducts(int categoryId)
        {
            var result = from p in _products
                         where p.CategoryId == categoryId
                         select p;

            return result.ToList();
        }

        public void DeleteProduct(BindingSource bindingSource, int productId)
        {
            var query = from p in _products
                        where p.ProductID == productId
                        select p;
            var product = query.Single();

            bindingSource.Remove(product);		//&lt;== Use the BindingSource
        }

        public void AddProduct(BindingSource bindingSource, Product product)
        {
            var maxId = (
                from p in _products
                select p).Max(p =&gt; p.ProductID);
            product.ProductID = maxId + 1;	//The GUI does not auto-increment the ID, that would be done by the dataset
            								// So this increments the ID before adding it via the BindingSource

            bindingSource.Add(product);		//&lt;== Use the BindingSource
        }

        public void Save()
        {
            //Nothing to do	//For the example with Objects in Memory, there is nothing to do.
        }
    }
}
</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>Data_Simulation</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>01_SimulateHeaderDetail_with_RandomNumericAndCharacterStrings__Configuration.cs</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>Data_Simulation</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>01_SimulateHeaderDetail_with_RandomNumericAndCharacterStrings__Configuration.cs</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>using System.Collections.Generic;
using Microsoft.Ajax.Utilities;
using Test_01.Models;
using globalCommon;

namespace Test_01.Migrations
	{
	using System;
	using System.Data.Entity;
	using System.Data.Entity.Migrations;
	using System.Linq;

	internal sealed class Configuration : DbMigrationsConfiguration&lt;Test_01.Models.Test_01_Db&gt;
		{
		public Configuration()
			{
			AutomaticMigrationsEnabled = true;
			ContextKey = "Test_01.Models.Test_01_Db";
			}

		protected override void Seed(Test_01.Models.Test_01_Db context)
			{

			Random rnd = new Random();

			for (int i = 0; i &lt; 10; i++)
				{
				context.Header_Record.AddOrUpdate(h =&gt; h.Header_PK, new Header_R
				{
					Percent_Done_0_00_to_1_00 = (decimal?)(rnd.NextDouble()),
					TheName = rnd.Random_LowerCaseLetters(5),
					PhoneNumber = rnd.Random_Numbers(10),
					Detail_R =
						new List&lt;Detail_R&gt;
						{
							
							new Detail_R {
											Detail_Category_Integer = rnd.Next(1,10),
											Detail_Category_nvarchar = rnd.Random_UpperCaseLetters(5),
										  },
							new Detail_R {
											Detail_Category_Integer = rnd.Next(1,10),
											Detail_Category_nvarchar = rnd.Random_UpperCaseLetters(5),
										  }
						}

				});

				}


			}
		}
	}
</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>Data_Simulation</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>02_DataSet_interface_ISource_example_with_LINQ_Methods__ObjectSource.cs</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>Data_Simulation</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>02_DataSet_interface_ISource_example_with_LINQ_Methods__ObjectSource.cs</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>1_simulated_data_DataSet_to_FORMS_and_CONTROLS__ObjectSource.cs

using System.Collections.Generic;
using System.Linq;
using System.Windows.Forms;

namespace Data
{
    public class ObjectSource : ISource	//&lt;== Now implements Interface
    {
        private List&lt;Category&gt; _categories;
        private List&lt;Product&gt; _products;

        public ObjectSource()
        {
            _categories = new List&lt;Category&gt;();
            _categories.Add(new Category(1, "Beverages"));
            _categories.Add(new Category(2, "Condiments"));
            _categories.Add(new Category(3, "Confections"));
            _categories.Add(new Category(4, "Dairy Products"));
            _categories.Add(new Category(5, "Grains/Cereals"));
            _categories.Add(new Category(6, "Meat/Poultry"));
            _categories.Add(new Category(7, "Produce"));
            _categories.Add(new Category(8, "Seafood"));

            _products = new List&lt;Product&gt;();
            _products.Add(new Product(1, "Chai", 1, "10 boxes x 20 bags", 18.0m, 39, 0, false));
            _products.Add(new Product(2, "Chang", 1, "24 - 12 oz bottles", 19.0m, 17, 40, false));
            _products.Add(new Product(3, "Aniseed Syrup", 2, "12 - 550 ml bottles", 10.0m, 13, 70, false));
            _products.Add(new Product(4, "Chef Anton's Cajun Seasoning", 2, "48 - 6 oz jars", 22.0m, 53, 0, false));
            _products.Add(new Product(5, "Chef Anton's Gumbo Mix", 2, "36 boxes", 21.35m, 0, 0, true));
            _products.Add(new Product(6, "Grandma's Boysenberry Spread", 2, "12 - 8 oz jars", 25.0m, 120, 0, false));
            _products.Add(new Product(7, "Uncle Bob's Organic Dried Pears", 7, "12 - 1 lb pkgs.", 30.0m, 15, 0, false));
            _products.Add(new Product(8, "Northwoods Cranberry Sauce", 2, "12 - 12 oz jars", 40.0m, 6, 0, false));
            _products.Add(new Product(9, "Mishi Kobe Niku", 6, "18 - 500 g pkgs.", 97.0m, 29, 0, true));
            _products.Add(new Product(10, "Ikura", 8, "12 - 200 ml jars", 31.0m, 31, 0, false));
            _products.Add(new Product(11, "Queso Cabrales", 4, "1 kg pkg.", 21.0m, 22, 30, false));
            _products.Add(new Product(12, "Queso Manchego La Pastora", 4, "10 - 500 g pkgs.", 38.0m, 86, 0, false));
            _products.Add(new Product(13, "Konbu", 8, "2 kg box", 6.0m, 24, 0, false));
            _products.Add(new Product(14, "Tofu", 7, "40 - 100 g pkgs.", 23.25m, 35, 0, false));
            _products.Add(new Product(15, "Genen Shouyu", 2, "24 - 250 ml bottles", 15.5m, 39, 0, false));
            _products.Add(new Product(16, "Pavlova", 3, "32 - 500 g boxes", 17.45m, 29, 0, false));
            _products.Add(new Product(17, "Alice Mutton", 6, "20 - 1 kg tins", 39.0m, 0, 0, true));
            _products.Add(new Product(18, "Carnarvon Tigers", 8, "16 kg pkg.", 62.5m, 42, 0, false));
            _products.Add(new Product(19, "Teatime Chocolate Biscuits", 3, "10 boxes x 12 pieces", 9.2m, 25, 0, false));
            _products.Add(new Product(20, "Sir Rodney's Marmalade", 3, "30 gift boxes", 81.0m, 40, 0, false));
            _products.Add(new Product(21, "Sir Rodney's Scones", 3, "24 pkgs. x 4 pieces", 10.0m, 3, 40, false));
            _products.Add(new Product(22, "Gustaf's Knäckebröd", 5, "24 - 500 g pkgs.", 21.0m, 104, 0, false));
            _products.Add(new Product(23, "Tunnbröd", 5, "12 - 250 g pkgs.", 9.0m, 61, 0, false));
            _products.Add(new Product(24, "Guaraná Fantástica", 1, "12 - 355 ml cans", 4.5m, 20, 0, true));
            _products.Add(new Product(25, "NuNuCa Nuß-Nougat-Creme", 3, "20 - 450 g glasses", 14.0m, 76, 0, false));
            _products.Add(new Product(26, "Gumbär Gummibärchen", 3, "100 - 250 g bags", 31.23m, 15, 0, false));
            _products.Add(new Product(27, "Schoggi Schokolade", 3, "100 - 100 g pieces", 43.9m, 49, 0, false));
            _products.Add(new Product(28, "Rössle Sauerkraut", 7, "25 - 825 g cans", 45.6m, 26, 0, true));
            _products.Add(new Product(29, "Thüringer Rostbratwurst", 6, "50 bags x 30 sausgs.", 123.79m, 0, 0, true));
            _products.Add(new Product(30, "Nord-Ost Matjeshering", 8, "10 - 200 g glasses", 25.89m, 10, 0, false));
            _products.Add(new Product(31, "Gorgonzola Telino", 4, "12 - 100 g pkgs", 12.5m, 0, 70, false));
            _products.Add(new Product(32, "Mascarpone Fabioli", 4, "24 - 200 g pkgs.", 32.0m, 9, 40, false));
            _products.Add(new Product(33, "Geitost", 4, "500 g", 2.5m, 112, 0, false));
            _products.Add(new Product(34, "Sasquatch Ale", 1, "24 - 12 oz bottles", 14.0m, 111, 0, false));
            _products.Add(new Product(35, "Steeleye Stout", 1, "24 - 12 oz bottles", 18.0m, 20, 0, false));
            _products.Add(new Product(36, "Inlagd Sill", 8, "24 - 250 g  jars", 19.0m, 112, 0, false));
            _products.Add(new Product(37, "Gravad lax", 8, "12 - 500 g pkgs.", 26.0m, 11, 50, false));
            _products.Add(new Product(38, "Côte de Blaye", 1, "12 - 75 cl bottles", 263.5m, 17, 0, false));
            _products.Add(new Product(39, "Chartreuse verte", 1, "750 cc per bottle", 18.0m, 69, 0, false));
            _products.Add(new Product(40, "Boston Crab Meat", 8, "24 - 4 oz tins", 18.4m, 123, 0, false));
            _products.Add(new Product(41, "Jack's New England Clam Chowder", 8, "12 - 12 oz cans", 9.65m, 85, 0, false));
            _products.Add(new Product(42, "Singaporean Hokkien Fried Mee", 5, "32 - 1 kg pkgs.", 14.0m, 26, 0, true));
            _products.Add(new Product(43, "Ipoh Coffee", 1, "16 - 500 g tins", 46.0m, 17, 10, false));
            _products.Add(new Product(44, "Gula Malacca", 2, "20 - 2 kg bags", 19.45m, 27, 0, false));
            _products.Add(new Product(45, "Rogede sild", 8, "1k pkg.", 9.5m, 5, 70, false));
            _products.Add(new Product(46, "Spegesild", 8, "4 - 450 g glasses", 12.0m, 95, 0, false));
            _products.Add(new Product(47, "Zaanse koeken", 3, "10 - 4 oz boxes", 9.5m, 36, 0, false));
            _products.Add(new Product(48, "Chocolade", 3, "10 pkgs.", 12.75m, 15, 70, false));
            _products.Add(new Product(49, "Maxilaku", 3, "24 - 50 g pkgs.", 20.0m, 10, 60, false));
            _products.Add(new Product(50, "Valkoinen suklaa", 3, "12 - 100 g bars", 16.25m, 65, 0, false));
            _products.Add(new Product(51, "Manjimup Dried Apples", 7, "50 - 300 g pkgs.", 53.0m, 20, 0, false));
            _products.Add(new Product(52, "Filo Mix", 5, "16 - 2 kg boxes", 7.0m, 38, 0, false));
            _products.Add(new Product(53, "Perth Pasties", 6, "48 pieces", 32.8m, 0, 0, true));
            _products.Add(new Product(54, "Tourtière", 6, "16 pies", 7.45m, 21, 0, false));
            _products.Add(new Product(55, "Pâté chinois", 6, "24 boxes x 2 pies", 24.0m, 115, 0, false));
            _products.Add(new Product(56, "Gnocchi di nonna Alice", 5, "24 - 250 g pkgs.", 38.0m, 21, 10, false));
            _products.Add(new Product(57, "Ravioli Angelo", 5, "24 - 250 g pkgs.", 19.5m, 36, 0, false));
            _products.Add(new Product(58, "Escargots de Bourgogne", 8, "24 pieces", 13.25m, 62, 0, false));
            _products.Add(new Product(59, "Raclette Courdavault", 4, "5 kg pkg.", 55.0m, 79, 0, false));
            _products.Add(new Product(60, "Camembert Pierrot", 4, "15 - 300 g rounds", 34.0m, 19, 0, false));
            _products.Add(new Product(61, "Sirop d'érable", 2, "24 - 500 ml bottles", 28.5m, 113, 0, false));
            _products.Add(new Product(62, "Tarte au sucre", 3, "48 pies", 49.3m, 17, 0, false));
            _products.Add(new Product(63, "Vegie-spread", 2, "15 - 625 g jars", 43.9m, 24, 0, false));
            _products.Add(new Product(64, "Wimmers gute Semmelknödel", 5, "20 bags x 4 pieces", 33.25m, 22, 80, false));
            _products.Add(new Product(65, "Louisiana Fiery Hot Pepper Sauce", 2, "32 - 8 oz bottles", 21.05m, 76, 0, false));
            _products.Add(new Product(66, "Louisiana Hot Spiced Okra", 2, "24 - 8 oz jars", 17.0m, 4, 100, false));
            _products.Add(new Product(67, "Laughing Lumberjack Lager", 1, "24 - 12 oz bottles", 14.0m, 52, 0, false));
            _products.Add(new Product(68, "Scottish Longbreads", 3, "10 boxes x 8 pieces", 12.5m, 6, 10, false));
            _products.Add(new Product(69, "Gudbrandsdalsost", 4, "10 kg pkg.", 36.0m, 26, 0, false));
            _products.Add(new Product(70, "Outback Lager", 1, "24 - 355 ml bottles", 15.0m, 15, 10, false));
            _products.Add(new Product(71, "Flotemysost", 4, "10 - 500 g pkgs.", 21.5m, 26, 0, false));
            _products.Add(new Product(72, "Mozzarella di Giovanni", 4, "24 - 200 g pkgs.", 34.8m, 14, 0, false));
            _products.Add(new Product(73, "Röd Kaviar", 8, "24 - 150 g jars", 15.0m, 101, 0, false));
            _products.Add(new Product(74, "Longlife Tofu", 7, "5 kg pkg.", 10.0m, 4, 20, false));
            _products.Add(new Product(75, "Rhönbräu Klosterbier", 1, "24 - 0.5 l bottles", 7.75m, 125, 0, false));
            _products.Add(new Product(76, "Lakkalikööri", 1, "500 ml", 18.0m, 57, 0, false));
            _products.Add(new Product(77, "Original Frankfurter grüne Soße", 2, "12 boxes", 13.0m, 32, 0, false));
        }

        public object GetCategories()
        {
            return _categories;
        }

        public object GetProducts(int categoryId)
        {
            var result = from p in _products
                         where p.CategoryId == categoryId
                         select p;

            return result.ToList();
        }

        public void DeleteProduct(BindingSource bindingSource, int productId)
        {
            var query = from p in _products
                        where p.ProductID == productId
                        select p;
            var product = query.Single();

            bindingSource.Remove(product);
        }

        public void AddProduct(BindingSource bindingSource, Product product)
        {
            var maxId = (
                from p in _products
                select p).Max(p =&gt; p.ProductID);
            product.ProductID = maxId + 1;

            bindingSource.Add(product);
        }

        public void Save()
        {
            //Nothing to do
        }
    }
}
</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>Data_Simulation</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>about_Data_Simulation</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>Data_Simulation</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>about_Data_Simulation</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>about_Data_Simulation
/*
see example under MVC_TESTing: 
	1_IsolateControllersFromInfrastructure_ProgramToDatabaseInterfaces_useSimulatedData__HomeControllerTest.cs
	
see example under: MVC_EntityFramework
	9_SeedDatabase_Configuration.cs</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>DATASET_DATATABLE</Category>
        <Language>VB.NET</Language>
        <Public>false</Public>
        <Name>CreatePrimaryKeyDataColumn</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>DATASET_DATATABLE</Category>
          <Language>VB.NET</Language>
          <Public>false</Public>
          <Name>CreatePrimaryKeyDataColumn</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>Imports System.Data
Imports System.Windows.Forms

Public Class Form1
    Inherits System.Windows.Forms.Form

    Public Shared Sub Main()
        Application.Run(New Form1())
    End Sub
    Public Sub New()
        MyBase.New()
        Me.DataGrid1 = New System.Windows.Forms.DataGrid()
        CType(Me.DataGrid1, System.ComponentModel.ISupportInitialize).BeginInit()
        Me.SuspendLayout()
        '
        Me.DataGrid1.DataMember = ""
        Me.DataGrid1.HeaderForeColor = System.Drawing.SystemColors.ControlText
        Me.DataGrid1.Location = New System.Drawing.Point(0, 8)
        Me.DataGrid1.Size = New System.Drawing.Size(432, 256)
        Me.DataGrid1.TabIndex = 0
        '
        Me.AutoScaleBaseSize = New System.Drawing.Size(5, 13)
        Me.ClientSize = New System.Drawing.Size(440, 273)
        Me.Controls.AddRange(New System.Windows.Forms.Control() {Me.DataGrid1})
        CType(Me.DataGrid1, System.ComponentModel.ISupportInitialize).EndInit()
        Me.ResumeLayout(False)

    End Sub

    Private Sub Form1_Load(ByVal sender As System.Object, ByVal e As System.EventArgs) Handles MyBase.Load
        CreateCustomersTable()
    End Sub

    Private Sub dcConstructorsTest()
        Dim custTable As DataTable = New DataTable("Customers")
        Dim dtSet As DataSet = New DataSet()

        Dim myDataType As System.Type
        myDataType = System.Type.GetType("System.Int32")
        Dim priceCol As DataColumn = New DataColumn("Price", myDataType)
        priceCol.Caption = "Price"
        custTable.Columns.Add(priceCol)

        Dim qtCol As DataColumn = New DataColumn()
        qtCol.ColumnName = "Quantity"
        qtCol.DataType = System.Type.GetType("System.Int32")
        qtCol.Caption = "Quantity"
        custTable.Columns.Add(qtCol)

        Dim strExpr As String = "Price * Quantity"

        Dim totCol As DataColumn = New DataColumn("Total", myDataType, strExpr, MappingType.Attribute)
        totCol.Caption = "Total"
        custTable.Columns.Add(totCol)
        dtSet.Tables.Add(custTable)
        DataGrid1.SetDataBinding(dtSet, "Customers")
    End Sub

    Private Sub CreateCustTable()
        Dim custTable As DataTable = New DataTable("Customers")
        Dim IdCol As DataColumn = New DataColumn()
        IdCol.ColumnName = "ID"
        IdCol.DataType = Type.GetType("System.Int32")
        IdCol.ReadOnly = True
        IdCol.AllowDBNull = False
        IdCol.Unique = True
        IdCol.AutoIncrement = True
        IdCol.AutoIncrementSeed = 1
        IdCol.AutoIncrementStep = 1
        custTable.Columns.Add(IdCol)

        Dim nameCol As DataColumn = New DataColumn()
        nameCol.ColumnName = "Name"
        nameCol.DataType = Type.GetType("System.String")
        custTable.Columns.Add(nameCol)

        Dim addCol As DataColumn = New DataColumn()
        addCol.ColumnName = "Address"
        addCol.DataType = Type.GetType("System.String")
        custTable.Columns.Add(addCol)

        Dim dobCol As DataColumn = New DataColumn()
        dobCol.ColumnName = "DOB"
        dobCol.DataType = Type.GetType("System.DateTime")
        custTable.Columns.Add(dobCol)

        Dim fullTimeCol As DataColumn = New DataColumn()
        fullTimeCol.ColumnName = "VAR"
        fullTimeCol.DataType = Type.GetType("System.Boolean")
        custTable.Columns.Add(fullTimeCol)

        Dim PrimaryKeyColumns() As DataColumn = New DataColumn(1) {}
        PrimaryKeyColumns(0) = custTable.Columns("ID")
        custTable.PrimaryKey = PrimaryKeyColumns
        Dim ds As DataSet = New DataSet("Customers")
        ds.Tables.Add(custTable)
        dataGrid1.DataSource = ds.DefaultViewManager
    End Sub

    Private Sub CreateCustomersTable()
        Dim custTable As System.Data.DataTable = New DataTable("Customers")
        Dim dtColumn As DataColumn

        dtColumn = New DataColumn()
        dtColumn.DataType = System.Type.GetType("System.Int32")
        dtColumn.ColumnName = "id"
        dtColumn.Caption = "Cust ID"
        dtColumn.ReadOnly = True
        dtColumn.Unique = True

        custTable.Columns.Add(dtColumn)

        dtColumn = New DataColumn()
        dtColumn.DataType = System.Type.GetType("System.String")
        dtColumn.ColumnName = "Name"
        dtColumn.Caption = "Cust Name"
        dtColumn.AutoIncrement = False
        dtColumn.ReadOnly = False
        dtColumn.Unique = False

        custTable.Columns.Add(dtColumn)
        dtColumn = New DataColumn()
        dtColumn.DataType = System.Type.GetType("System.String")
        dtColumn.ColumnName = "Address"
        dtColumn.Caption = "Address"
        dtColumn.ReadOnly = False
        dtColumn.Unique = False

        custTable.Columns.Add(dtColumn)

        Dim PrimaryKeyColumns() As DataColumn = New DataColumn(1) {}
        PrimaryKeyColumns(0) = custTable.Columns("id")
        custTable.PrimaryKey = PrimaryKeyColumns

        Dim ds As DataSet = New DataSet("Customers")
        ds.Tables.Add(custTable)

        Dim row1 As DataRow = custTable.NewRow()
        row1("id") = 1001
        row1("Address") = "CA"
        row1("Name") = "A"
        custTable.Rows.Add(row1)
        Dim row2 As DataRow = custTable.NewRow()
        row2("id") = 1002
        row2("Name") = "R"
        row2("Address") = "CA"
        custTable.Rows.Add(row2)
        Dim row3 As DataRow = custTable.NewRow()
        row3("id") = 1003
        row3("Name") = "M "
        row3("Address") = "CA"
        custTable.Rows.Add(row3)
        MessageBox.Show(row2.RowState.ToString())
        row2.Delete()
        MessageBox.Show(row3.RowState.ToString())

        DataGrid1.DataSource = ds.DefaultViewManager
    End Sub

    Friend WithEvents DataGrid1 As System.Windows.Forms.DataGrid

End Class
</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>DATES</Category>
        <Language>SQLSERVER2K SQL</Language>
        <Public>false</Public>
        <Name>Format_DateTime</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>DATES</Category>
          <Language>SQLSERVER2K SQL</Language>
          <Public>false</Public>
          <Name>Format_DateTime</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>https://anubhavg.wordpress.com/2009/06/11/how-to-format-datetime-date-in-sql-server-2005/

SELECT convert(varchar, getdate(), 100) — mon dd yyyy hh:mmAM (or PM)
                                        — Oct  2 2008 11:01AM          
SELECT convert(varchar, getdate(), 101) — mm/dd/yyyy – 10/02/2008                  
SELECT convert(varchar, getdate(), 102) — yyyy.mm.dd – 2008.10.02           
SELECT convert(varchar, getdate(), 103) — dd/mm/yyyy
SELECT convert(varchar, getdate(), 104) — dd.mm.yyyy
SELECT convert(varchar, getdate(), 105) — dd-mm-yyyy
SELECT convert(varchar, getdate(), 106) — dd mon yyyy
SELECT convert(varchar, getdate(), 107) — mon dd, yyyy
SELECT convert(varchar, getdate(), 108) — hh:mm:ss
SELECT convert(varchar, getdate(), 109) — mon dd yyyy hh:mm:ss:mmmAM (or PM)
                                        — Oct  2 2008 11:02:44:013AM   
SELECT convert(varchar, getdate(), 110) — mm-dd-yyyy
SELECT convert(varchar, getdate(), 111) — yyyy/mm/dd
SELECT convert(varchar, getdate(), 112) — yyyymmdd
SELECT convert(varchar, getdate(), 113) — dd mon yyyy hh:mm:ss:mmm
                                        — 02 Oct 2008 11:02:07:577     
SELECT convert(varchar, getdate(), 114) — hh:mm:ss:mmm(24h)
SELECT convert(varchar, getdate(), 120) — yyyy-mm-dd hh:mm:ss(24h)
SELECT convert(varchar, getdate(), 121) — yyyy-mm-dd hh:mm:ss.mmm
SELECT convert(varchar, getdate(), 126) — yyyy-mm-ddThh:mm:ss.mmm
                                        — 2008-10-02T10:52:47.513
— SQL create different date styles with t-sql string functions
SELECT replace(convert(varchar, getdate(), 111), ‘/’, ‘ ‘) — yyyy mm dd
SELECT convert(varchar(7), getdate(), 126)                 — yyyy-mm
SELECT right(convert(varchar, getdate(), 106), 8)          — mon yyyy
————
— SQL Server date formatting function – convert datetime to string
————
— SQL datetime functions
— SQL Server date formats
— T-SQL convert dates
— Formatting dates sql server
CREATE FUNCTION dbo.fnFormatDate (@Datetime DATETIME, @FormatMask VARCHAR(32))
RETURNS VARCHAR(32)
AS
BEGIN
    DECLARE @StringDate VARCHAR(32)
    SET @StringDate = @FormatMask
    IF (CHARINDEX (‘YYYY’,@StringDate) &gt; 0)
       SET @StringDate = REPLACE(@StringDate, ‘YYYY’,
                         DATENAME(YY, @Datetime))
    IF (CHARINDEX (‘YY’,@StringDate) &gt; 0)
       SET @StringDate = REPLACE(@StringDate, ‘YY’,
                         RIGHT(DATENAME(YY, @Datetime),2))
    IF (CHARINDEX (‘Month’,@StringDate) &gt; 0)
       SET @StringDate = REPLACE(@StringDate, ‘Month’,
                         DATENAME(MM, @Datetime))
    IF (CHARINDEX (‘MON’,@StringDate COLLATE SQL_Latin1_General_CP1_CS_AS)&gt;0)
       SET @StringDate = REPLACE(@StringDate, ‘MON’,
                         LEFT(UPPER(DATENAME(MM, @Datetime)),3))
    IF (CHARINDEX (‘Mon’,@StringDate) &gt; 0)
       SET @StringDate = REPLACE(@StringDate, ‘Mon’,
                                     LEFT(DATENAME(MM, @Datetime),3))
    IF (CHARINDEX (‘MM’,@StringDate) &gt; 0)
       SET @StringDate = REPLACE(@StringDate, ‘MM’,
                  RIGHT(‘0’+CONVERT(VARCHAR,DATEPART(MM, @Datetime)),2))
    IF (CHARINDEX (‘M’,@StringDate) &gt; 0)
       SET @StringDate = REPLACE(@StringDate, ‘M’,
                         CONVERT(VARCHAR,DATEPART(MM, @Datetime)))
    IF (CHARINDEX (‘DD’,@StringDate) &gt; 0)
       SET @StringDate = REPLACE(@StringDate, ‘DD’,
                         RIGHT(‘0’+DATENAME(DD, @Datetime),2))
    IF (CHARINDEX (‘D’,@StringDate) &gt; 0)
       SET @StringDate = REPLACE(@StringDate, ‘D’,
                                     DATENAME(DD, @Datetime))   
RETURN @StringDate
END
GO
 
— Microsoft SQL Server date format function test
— MSSQL formatting dates
SELECT dbo.fnFormatDate (getdate(), ‘MM/DD/YYYY’)           — 01/03/2012
SELECT dbo.fnFormatDate (getdate(), ‘DD/MM/YYYY’)           — 03/01/2012
SELECT dbo.fnFormatDate (getdate(), ‘M/DD/YYYY’)            — 1/03/2012
SELECT dbo.fnFormatDate (getdate(), ‘M/D/YYYY’)             — 1/3/2012
SELECT dbo.fnFormatDate (getdate(), ‘M/D/YY’)               — 1/3/12
SELECT dbo.fnFormatDate (getdate(), ‘MM/DD/YY’)             — 01/03/12
SELECT dbo.fnFormatDate (getdate(), ‘MON DD, YYYY’)         — JAN 03, 2012
SELECT dbo.fnFormatDate (getdate(), ‘Mon DD, YYYY’)         — Jan 03, 2012
SELECT dbo.fnFormatDate (getdate(), ‘Month DD, YYYY’)       — January 03, 2012
SELECT dbo.fnFormatDate (getdate(), ‘YYYY/MM/DD’)           — 2012/01/03
SELECT dbo.fnFormatDate (getdate(), ‘YYYYMMDD’)             — 20120103
SELECT dbo.fnFormatDate (getdate(), ‘YYYY-MM-DD’)           — 2012-01-03
— CURRENT_TIMESTAMP returns current system date and time in standard internal format
SELECT dbo.fnFormatDate (CURRENT_TIMESTAMP,‘YY.MM.DD’)      — 12.01.03
GO
————
 
/***** SELECTED SQL DATE/DATETIME FORMATS WITH NAMES *****/
 
— SQL format datetime
— Default format: Oct 23 2006 10:40AM
SELECT [Default]=CONVERT(varchar,GETDATE(),100)
 
— US-Style format: 10/23/2006
SELECT [US-Style]=CONVERT(char,GETDATE(),101)
 
— ANSI format: 2006.10.23
SELECT [ANSI]=CONVERT(char,CURRENT_TIMESTAMP,102)
 
— UK-Style format: 23/10/2006
SELECT [UK-Style]=CONVERT(char,GETDATE(),103)
 
— German format: 23.10.2006
SELECT [German]=CONVERT(varchar,GETDATE(),104)
 
— ISO format: 20061023
SELECT ISO=CONVERT(varchar,GETDATE(),112)
 
— ISO8601 format: 2008-10-23T19:20:16.003
SELECT [ISO8601]=CONVERT(varchar,GETDATE(),126)
————
 
— SQL Server datetime formats
— Century date format MM/DD/YYYY usage in a query
— Format dates SQL Server 2005
SELECT TOP (1)
      SalesOrderID,
      OrderDate = CONVERT(char(10), OrderDate, 101),
      OrderDateTime = OrderDate
FROM AdventureWorks.Sales.SalesOrderHeader
/* Result
 
SalesOrderID      OrderDate               OrderDateTime
43697             07/01/2001          2001-07-01 00:00:00.000
*/
 
— SQL update datetime column
— SQL datetime DATEADD
UPDATE Production.Product
SET ModifiedDate=DATEADD(dd,1, ModifiedDate)
WHERE ProductID = 1001
 
— MM/DD/YY date format
— Datetime format sql
SELECT TOP (1)
      SalesOrderID,
      OrderDate = CONVERT(varchar(8), OrderDate, 1),
      OrderDateTime = OrderDate
FROM AdventureWorks.Sales.SalesOrderHeader
ORDER BY SalesOrderID desc
/* Result
 
SalesOrderID      OrderDate         OrderDateTime
75123             07/31/04          2004-07-31 00:00:00.000
*/
 
— Combining different style formats for date &amp; time
— Datetime formats
— Datetime formats sql
DECLARE @Date DATETIME
SET @Date = ‘2015-12-22 03:51 PM’
SELECT CONVERT(CHAR(10),@Date,110) + SUBSTRING(CONVERT(varchar,@Date,0),12,8)
— Result: 12-22-2015  3:51PM
 
— Microsoft SQL Server cast datetime to string
SELECT stringDateTime=CAST (getdate() as varchar)
— Result: Dec 29 2012  3:47AM
————
— SQL Server date and time functions overview
————
— SQL Server CURRENT_TIMESTAMP function
— SQL Server datetime functions
— local NYC – EST – Eastern Standard Time zone
— SQL DATEADD function – SQL DATEDIFF function
SELECT CURRENT_TIMESTAMP                        — 2012-01-05 07:02:10.577
— SQL Server DATEADD function
SELECT DATEADD(month,2,‘2012-12-09’)            — 2013-02-09 00:00:00.000
— SQL Server DATEDIFF function
SELECT DATEDIFF(day,‘2012-12-09’,‘2013-02-09’)  — 62
— SQL Server DATENAME function
SELECT DATENAME(month,   ‘2012-12-09’)          — December
SELECT DATENAME(weekday, ‘2012-12-09’)          — Sunday
— SQL Server DATEPART function
SELECT DATEPART(month, ‘2012-12-09’)            — 12
— SQL Server DAY function
SELECT DAY(‘2012-12-09’)                        — 9
— SQL Server GETDATE function
— local NYC – EST – Eastern Standard Time zone
SELECT GETDATE()                                — 2012-01-05 07:02:10.577
— SQL Server GETUTCDATE function
— London – Greenwich Mean Time
SELECT GETUTCDATE()                             — 2012-01-05 12:02:10.577
— SQL Server MONTH function
SELECT MONTH(‘2012-12-09’)                      — 12
— SQL Server YEAR function
SELECT YEAR(‘2012-12-09’)                       — 2012
 
 
————
— T-SQL Date and time function application
— CURRENT_TIMESTAMP and getdate() are the same in T-SQL
————
— SQL first day of the month
— SQL first date of the month
— SQL first day of current month – 2012-01-01 00:00:00.000
SELECT DATEADD(dd,0,DATEADD(mm, DATEDIFF(mm,0,CURRENT_TIMESTAMP),0))
— SQL last day of the month
— SQL last date of the month
— SQL last day of current month – 2012-01-31 00:00:00.000
SELECT DATEADD(dd,-1,DATEADD(mm, DATEDIFF(mm,0,CURRENT_TIMESTAMP)+1,0))
— SQL first day of last month
— SQL first day of previous month – 2011-12-01 00:00:00.000
SELECT DATEADD(mm,-1,DATEADD(mm, DATEDIFF(mm,0,CURRENT_TIMESTAMP),0))
— SQL last day of last month
— SQL last day of previous month – 2011-12-31 00:00:00.000
SELECT DATEADD(dd,-1,DATEADD(mm, DATEDIFF(mm,0,DATEADD(MM,-1,GETDATE()))+1,0))
— SQL first day of next month – 2012-02-01 00:00:00.000
SELECT DATEADD(mm,1,DATEADD(mm, DATEDIFF(mm,0,CURRENT_TIMESTAMP),0))
— SQL last day of next month – 2012-02-28 00:00:00.000
SELECT DATEADD(dd,-1,DATEADD(mm, DATEDIFF(mm,0,DATEADD(MM,1,GETDATE()))+1,0))
GO
— SQL first day of a month – 2012-10-01 00:00:00.000
DECLARE @Date datetime; SET @Date = ‘2012-10-23’
SELECT DATEADD(dd,0,DATEADD(mm, DATEDIFF(mm,0,@Date),0))
GO
— SQL last day of a month – 2012-03-31 00:00:00.000
DECLARE @Date datetime; SET @Date = ‘2012-03-15’
SELECT DATEADD(dd,-1,DATEADD(mm, DATEDIFF(mm,0,@Date)+1,0))
GO
— SQL first day of year 
— SQL first day of the year  –  2012-01-01 00:00:00.000
SELECT DATEADD(yy, DATEDIFF(yy,0,CURRENT_TIMESTAMP), 0)
— SQL last day of year  
— SQL last day of the year   – 2012-12-31 00:00:00.000
SELECT DATEADD(yy,1, DATEADD(dd, –1, DATEADD(yy,
                     DATEDIFF(yy,0,CURRENT_TIMESTAMP), 0)))
— SQL last day of last year
— SQL last day of previous year   – 2011-12-31 00:00:00.000
SELECT DATEADD(dd,-1,DATEADD(yy,DATEDIFF(yy,0,CURRENT_TIMESTAMP), 0))
GO
— SQL calculate age in years, months, days
— SQL table-valued function
— SQL user-defined function – UDF
— SQL Server age calculation – date difference
— Format dates SQL Server 2008
USE AdventureWorks2008;
GO
CREATE FUNCTION fnAge  (@BirthDate DATETIME)
RETURNS @Age TABLE(Years  INT,
                   Months INT,
                   Days   INT)
AS
  BEGIN
    DECLARE  @EndDate     DATETIME, @Anniversary DATETIME
    SET @EndDate = Getdate()
    SET @Anniversary = Dateadd(yy,Datediff(yy,@BirthDate,@EndDate),@BirthDate)
    
    INSERT @Age
    SELECT Datediff(yy,@BirthDate,@EndDate) – (CASE
                                                 WHEN @Anniversary &gt; @EndDate THEN 1
                                                 ELSE 0
                                               END), 0, 0
     UPDATE @Age     SET    Months = Month(@EndDate – @Anniversary) – 1
    UPDATE @Age     SET    Days = Day(@EndDate – @Anniversary) – 1
    RETURN
  END
GO
 
— Test table-valued UDF
SELECT * FROM   fnAge(‘1956-10-23’)
SELECT * FROM   dbo.fnAge(‘1956-10-23’)
/* Results
Years       Months      Days
52          4           1
*/
 
———-
— SQL date range between
———-
— SQL between dates
USE AdventureWorks;
— SQL between
SELECT POs=COUNT(*) FROM Purchasing.PurchaseOrderHeader
WHERE OrderDate BETWEEN ‘20040301’ AND ‘20040315’
— Result: 108
 
— BETWEEN operator is equivalent to &gt;=…AND….&lt;=
SELECT POs=COUNT(*) FROM Purchasing.PurchaseOrderHeader
WHERE OrderDate
BETWEEN ‘2004-03-01 00:00:00.000’ AND ‘2004-03-15  00:00:00.000’
/*
Orders with OrderDates
‘2004-03-15  00:00:01.000’  – 1 second after midnight (12:00AM)
‘2004-03-15  00:01:00.000’  – 1 minute after midnight
‘2004-03-15  01:00:00.000’  – 1 hour after midnight
 
are not included in the two queries above.
*/
— To include the entire day of 2004-03-15 use the following two solutions
SELECT POs=COUNT(*) FROM Purchasing.PurchaseOrderHeader
WHERE OrderDate &gt;= ‘20040301’ AND OrderDate &lt; ‘20040316’
 
— SQL between with DATE type (SQL Server 2008)
SELECT POs=COUNT(*) FROM Purchasing.PurchaseOrderHeader
WHERE CONVERT(DATE, OrderDate) BETWEEN ‘20040301’ AND ‘20040315’
———-
— Non-standard format conversion: 2011 December 14
— SQL datetime to string
SELECT [YYYY Month DD] =
CAST(YEAR(GETDATE()) AS VARCHAR(4))+ ‘ ‘+
DATENAME(MM, GETDATE()) + ‘ ‘ +
CAST(DAY(GETDATE()) AS VARCHAR(2))
 
— Converting datetime to YYYYMMDDHHMMSS format: 20121214172638
SELECT replace(convert(varchar, getdate(),111),‘/’,”) +
replace(convert(varchar, getdate(),108),‘:’,”)
 
— Datetime custom format conversion to YYYY_MM_DD
select CurrentDate=rtrim(year(getdate())) + ‘_’ +
right(‘0’ + rtrim(month(getdate())),2) + ‘_’ +
right(‘0’ + rtrim(day(getdate())),2)
 
— Converting seconds to HH:MM:SS format
declare @Seconds int
set @Seconds = 10000
select TimeSpan=right(‘0’ +rtrim(@Seconds / 3600),2) + ‘:’ +
right(‘0’ + rtrim((@Seconds % 3600) / 60),2) + ‘:’ +
right(‘0’ + rtrim(@Seconds % 60),2)
— Result: 02:46:40
 
— Test result
select 2*3600 + 46*60 + 40
— Result: 10000
— Set the time portion of a datetime value to 00:00:00.000
— SQL strip time from date
— SQL strip time from datetime
SELECT CURRENT_TIMESTAMP ,DATEADD(dd, DATEDIFF(dd, 0, CURRENT_TIMESTAMP), 0)
— Results: 2014-01-23 05:35:52.793 2014-01-23 00:00:00.000
/*******
 
VALID DATE RANGES FOR DATE/DATETIME DATA TYPES
 
SMALLDATETIME date range:
January 1, 1900 through June 6, 2079
 
DATETIME date range:
January 1, 1753 through December 31, 9999
 
DATETIME2 date range (SQL Server 2008):
January 1,1 AD through December 31, 9999 AD
 
DATE date range (SQL Server 2008):
January 1, 1 AD through December 31, 9999 AD
 
*******/
— Selecting with CONVERT into different styles
— Note: Only Japan &amp; ISO styles can be used in ORDER BY
SELECT TOP(1)
     Italy  = CONVERT(varchar, OrderDate, 105)
   , USA    = CONVERT(varchar, OrderDate, 110)
   , Japan  = CONVERT(varchar, OrderDate, 111)
   , ISO    = CONVERT(varchar, OrderDate, 112)
FROM AdventureWorks.Purchasing.PurchaseOrderHeader
ORDER BY PurchaseOrderID DESC
/* Results
Italy       USA         Japan       ISO
25-07-2004  07-25-2004  2004/07/25  20040725
*/
— SQL Server convert date to integer
DECLARE @Datetime datetime
SET @Datetime = ‘2012-10-23 10:21:05.345’
SELECT DateAsInteger = CAST (CONVERT(varchar,@Datetime,112) as INT)
— Result: 20121023
 
— SQL Server convert integer to datetime
DECLARE @intDate int
SET @intDate = 20120315
SELECT IntegerToDatetime = CAST(CAST(@intDate as varchar) as datetime)
— Result: 2012-03-15 00:00:00.000
————
— SQL Server CONVERT script applying table INSERT/UPDATE
————
— SQL Server convert date
— Datetime column is converted into date only string column
USE tempdb;
GO
CREATE TABLE sqlConvertDateTime   (
            DatetimeCol datetime,
            DateCol char(8));
INSERT sqlConvertDateTime (DatetimeCol) SELECT GETDATE()
 
UPDATE sqlConvertDateTime
SET DateCol = CONVERT(char(10), DatetimeCol, 112)
SELECT * FROM sqlConvertDateTime
 
— SQL Server convert datetime
— The string date column is converted into datetime column
UPDATE sqlConvertDateTime
SET DatetimeCol = CONVERT(Datetime, DateCol, 112)
SELECT * FROM sqlConvertDateTime
 
— Adding a day to the converted datetime column with DATEADD
UPDATE sqlConvertDateTime
SET DatetimeCol = DATEADD(day, 1, CONVERT(Datetime, DateCol, 112))
SELECT * FROM sqlConvertDateTime
 
— Equivalent formulation
— SQL Server cast datetime
UPDATE sqlConvertDateTime
SET DatetimeCol = DATEADD(dd, 1, CAST(DateCol AS datetime))
SELECT * FROM sqlConvertDateTime
GO
DROP TABLE sqlConvertDateTime
GO
/* First results
DatetimeCol                   DateCol
2014-12-25 16:04:15.373       20141225 */
 
/* Second results:
DatetimeCol                   DateCol
2014-12-25 00:00:00.000       20141225  */
 
/* Third results:
DatetimeCol                   DateCol
2014-12-26 00:00:00.000       20141225  */
————
— SQL month sequence – SQL date sequence generation with table variable
— SQL Server cast string to datetime – SQL Server cast datetime to string
— SQL Server insert default values method
DECLARE @Sequence table (Sequence int identity(1,1))
DECLARE @i int; SET @i = 0
DECLARE @StartDate datetime;
SET @StartDate = CAST(CONVERT(varchar, year(getdate()))+
                 RIGHT(‘0’+convert(varchar,month(getdate())),2) + ’01’ AS DATETIME)
WHILE ( @i &lt; 120)
BEGIN
      INSERT @Sequence DEFAULT VALUES
      SET @i = @i + 1
END
SELECT MonthSequence = CAST(DATEADD(month, Sequence,@StartDate) AS varchar)
FROM @Sequence
GO
/* Partial results:
MonthSequence
Jan  1 2012 12:00AM
Feb  1 2012 12:00AM
Mar  1 2012 12:00AM
Apr  1 2012 12:00AM
*/
————
 
————
— SQL Server Server datetime internal storage
— SQL Server datetime formats
————
— SQL Server datetime to hex
SELECT Now=CURRENT_TIMESTAMP, HexNow=CAST(CURRENT_TIMESTAMP AS BINARY(8))
/* Results
 
Now                     HexNow
2009-01-02 17:35:59.297 0x00009B850122092D
*/
— SQL Server date part – left 4 bytes – Days since 1900-01-01
SELECT Now=DATEADD(DAY, CONVERT(INT, 0x00009B85), ‘19000101’)
GO
— Result: 2009-01-02 00:00:00.000
 
— SQL time part – right 4 bytes – milliseconds since midnight
— 1000/300 is an adjustment factor
— SQL dateadd to Midnight
SELECT Now=DATEADD(MS, (1000.0/300)* CONVERT(BIGINT, 0x0122092D), ‘2009-01-02’)
GO
— Result: 2009-01-02 17:35:59.290
————
————
— String date and datetime date&amp;time columns usage
— SQL Server datetime formats in tables
————
USE tempdb;
SET NOCOUNT ON;
— SQL Server select into table create
SELECT TOP (5)
      FullName=convert(nvarchar(50),FirstName+‘ ‘+LastName),
      BirthDate = CONVERT(char(8), BirthDate,112),
      ModifiedDate = getdate()
INTO Employee
FROM AdventureWorks.HumanResources.Employee e
INNER JOIN AdventureWorks.Person.Contact c
ON c.ContactID = e.ContactID
ORDER BY EmployeeID
GO
— SQL Server alter table
ALTER TABLE Employee ALTER COLUMN FullName nvarchar(50) NOT NULL
GO
ALTER TABLE Employee
ADD CONSTRAINT [PK_Employee] PRIMARY KEY (FullName )
GO
/* Results
 
Table definition for the Employee table
Note: BirthDate is string date (only)
 
CREATE TABLE dbo.Employee(
      FullName nvarchar(50) NOT NULL PRIMARY KEY,
      BirthDate char(8) NULL,
      ModifiedDate datetime NOT NULL
      )
*/
SELECT * FROM Employee ORDER BY FullName
GO
/* Results
FullName                BirthDate   ModifiedDate
Guy Gilbert             19720515    2009-01-03 10:10:19.217
Kevin Brown             19770603    2009-01-03 10:10:19.217
Rob Walters             19650123    2009-01-03 10:10:19.217
Roberto Tamburello      19641213    2009-01-03 10:10:19.217
Thierry D’Hers          19490829    2009-01-03 10:10:19.217
*/
 
— SQL Server age
SELECT FullName, Age = DATEDIFF(YEAR, BirthDate, GETDATE()),
       RowMaintenanceDate = CAST (ModifiedDate AS varchar)
FROM Employee ORDER BY FullName
GO
/* Results
FullName                Age   RowMaintenanceDate
Guy Gilbert             37    Jan  3 2009 10:10AM
Kevin Brown             32    Jan  3 2009 10:10AM
Rob Walters             44    Jan  3 2009 10:10AM
Roberto Tamburello      45    Jan  3 2009 10:10AM
Thierry D’Hers          60    Jan  3 2009 10:10AM
*/
 
— SQL Server age of Rob Walters on specific dates
— SQL Server string to datetime implicit conversion with DATEADD
SELECT AGE50DATE = DATEADD(YY, 50, ‘19650123’)
GO
— Result: 2015-01-23 00:00:00.000
 
— SQL Server datetime to string, Italian format for ModifiedDate
— SQL Server string to datetime implicit conversion with DATEDIFF
SELECT FullName,
         AgeDEC31 = DATEDIFF(YEAR, BirthDate, ‘20141231’),
         AgeJAN01 = DATEDIFF(YEAR, BirthDate, ‘20150101’),
         AgeJAN23 = DATEDIFF(YEAR, BirthDate, ‘20150123’),
         AgeJAN24 = DATEDIFF(YEAR, BirthDate, ‘20150124’),
       ModDate = CONVERT(varchar, ModifiedDate, 105)
FROM Employee
WHERE FullName = ‘Rob Walters’
ORDER BY FullName
GO
/* Results
Important Note: age increments on Jan 1 (not as commonly calculated)
 
FullName    AgeDEC31    AgeJAN01    AgeJAN23    AgeJAN24    ModDate
Rob Walters 49          50          50          50          03-01-2009
*/
 
————
— SQL combine integer date &amp; time into datetime
————
— Datetime format sql
— SQL stuff
DECLARE @DateTimeAsINT TABLE ( ID int identity(1,1) primary key, 
   DateAsINT int, 
   TimeAsINT int 
) 
— NOTE: leading zeroes in time is for readability only!  
INSERT @DateTimeAsINT (DateAsINT, TimeAsINT) VALUES (20121023, 235959)  
INSERT @DateTimeAsINT (DateAsINT, TimeAsINT) VALUES (20121023, 010204)  
INSERT @DateTimeAsINT (DateAsINT, TimeAsINT) VALUES (20121023, 002350)
INSERT @DateTimeAsINT (DateAsINT, TimeAsINT) VALUES (20121023, 000244)  
INSERT @DateTimeAsINT (DateAsINT, TimeAsINT) VALUES (20121023, 000050)  
INSERT @DateTimeAsINT (DateAsINT, TimeAsINT) VALUES (20121023, 000006)  
 
SELECT DateAsINT, TimeAsINT,
  CONVERT(datetime, CONVERT(varchar(8), DateAsINT) + ‘ ‘+
  STUFF(STUFF ( RIGHT(REPLICATE(‘0’, 6) + CONVERT(varchar(6), TimeAsINT), 6),
                  3, 0, ‘:’), 6, 0, ‘:’))  AS DateTimeValue
FROM   @DateTimeAsINT 
ORDER BY ID
GO
/* Results
DateAsINT   TimeAsINT   DateTimeValue
20121023    235959      2012-10-23 23:59:59.000
20121023    10204       2012-10-23 01:02:04.000
20121023    2350        2012-10-23 00:23:50.000
20121023    244         2012-10-23 00:02:44.000
20121023    50          2012-10-23 00:00:50.000
20121023    6           2012-10-23 00:00:06.000
*/
————
 
— SQL Server string to datetime, implicit conversion with assignment
UPDATE Employee SET ModifiedDate = ‘20150123’
WHERE FullName = ‘Rob Walters’
GO
SELECT ModifiedDate FROM Employee WHERE FullName = ‘Rob Walters’
GO
— Result: 2015-01-23 00:00:00.000
 
/* SQL string date, assemble string date from datetime parts  */
— SQL Server cast string to datetime – sql convert string date
— SQL Server number to varchar conversion
— SQL Server leading zeroes for month and day
— SQL Server right string function
UPDATE Employee SET BirthDate =
      CONVERT(char(4),YEAR(CAST(‘1965-01-23’ as DATETIME)))+
      RIGHT(‘0’+CONVERT(varchar,MONTH(CAST(‘1965-01-23’ as DATETIME))),2)+
      RIGHT(‘0’+CONVERT(varchar,DAY(CAST(‘1965-01-23’ as DATETIME))),2)
      WHERE FullName = ‘Rob Walters’
GO
SELECT BirthDate FROM Employee WHERE FullName = ‘Rob Walters’
GO
— Result: 19650123
 
— Perform cleanup action
DROP TABLE Employee
— SQL nocount
SET NOCOUNT OFF;
GO
————
————
— sql isdate function
————
USE tempdb;
— sql newid – random sort
SELECT top(3) SalesOrderID,
stringOrderDate = CAST (OrderDate AS varchar)
INTO DateValidation
FROM AdventureWorks.Sales.SalesOrderHeader
ORDER BY NEWID()
GO
SELECT * FROM DateValidation
/* Results
SalesOrderID      stringOrderDate
56720             Oct 26 2003 12:00AM
73737             Jun 25 2004 12:00AM
70573             May 14 2004 12:00AM
*/
— SQL update with top
UPDATE TOP(1) DateValidation
SET stringOrderDate = ‘Apb 29 2004 12:00AM’
GO
— SQL string to datetime fails without validation
SELECT SalesOrderID, OrderDate = CAST (stringOrderDate as datetime)
FROM DateValidation
GO
/* Msg 242, Level 16, State 3, Line 1
The conversion of a varchar data type to a datetime data type resulted in an
out-of-range value.
*/
— sql isdate – filter for valid dates
SELECT SalesOrderID, OrderDate = CAST (stringOrderDate as datetime)
FROM DateValidation
WHERE ISDATE(stringOrderDate) = 1
GO
/* Results
SalesOrderID      OrderDate
73737             2004-06-25 00:00:00.000
70573             2004-05-14 00:00:00.000
*/
— SQL drop table
DROP TABLE DateValidation
Go
 
————
— SELECT between two specified dates – assumption TIME part is 00:00:00.000
————
— SQL datetime between
— SQL select between two dates
SELECT EmployeeID, RateChangeDate
FROM AdventureWorks.HumanResources.EmployeePayHistory
WHERE RateChangeDate &gt;= ‘1997-11-01’ AND 
      RateChangeDate &lt; DATEADD(dd,1,‘1998-01-05’)
GO
/* Results
EmployeeID  RateChangeDate
3           1997-12-12 00:00:00.000
4           1998-01-05 00:00:00.000
*/
 
/* Equivalent to
 
— SQL datetime range
SELECT EmployeeID, RateChangeDate
FROM AdventureWorks.HumanResources.EmployeePayHistory
WHERE RateChangeDate &gt;= ‘1997-11-01 00:00:00’ AND 
      RateChangeDate &lt;  ‘1998-01-06 00:00:00’
GO
*/
————
— SQL datetime language setting
— SQL Nondeterministic function usage – result varies with language settings
SET LANGUAGE  ‘us_english’;  –– Jan 12 2015 12:00AM 
SELECT US = convert(VARCHAR,convert(DATETIME,’01/12/2015′));
SET LANGUAGE  ‘British’;     –– Dec  1 2015 12:00AM 
SELECT UK = convert(VARCHAR,convert(DATETIME,’01/12/2015′));
SET LANGUAGE  ‘German’;      –– Dez  1 2015 12:00AM 
SET LANGUAGE  ‘Deutsch’;     –– Dez  1 2015 12:00AM 
SELECT Germany = convert(VARCHAR,convert(DATETIME,’01/12/2015′));
SET LANGUAGE  ‘French’;      –– déc  1 2015 12:00AM 
SELECT France = convert(VARCHAR,convert(DATETIME,’01/12/2015′));
SET LANGUAGE  ‘Spanish’;     –– Dic  1 2015 12:00AM 
SELECT Spain = convert(VARCHAR,convert(DATETIME,’01/12/2015′));
SET LANGUAGE  ‘Hungarian’;   –– jan 12 2015 12:00AM 
SELECT Hungary = convert(VARCHAR,convert(DATETIME,’01/12/2015′));
SET LANGUAGE  ‘us_english’;
GO
————
————
— Function for Monday dates calculation
————
USE AdventureWorks2008;
GO
— SQL user-defined function
— SQL scalar function – UDF
CREATE FUNCTION fnMondayDate
               (@Year          INT,
                @Month         INT,
                @MondayOrdinal INT)
RETURNS DATETIME
AS
  BEGIN
    DECLARE  @FirstDayOfMonth CHAR(10),
             @SeedDate        CHAR(10)
    
    SET @FirstDayOfMonth = convert(VARCHAR,@Year) + ‘-‘ + convert(VARCHAR,@Month) + ‘-01’
    SET @SeedDate = ‘1900-01-01’
    
    RETURN DATEADD(DD,DATEDIFF(DD,@SeedDate,DATEADD(DD,(@MondayOrdinal * 7) – 1,
                  @FirstDayOfMonth)) / 7 * 7,  @SeedDate)
  END
GO
 
— Test Datetime UDF
— Third Monday in Feb, 2015
SELECT dbo.fnMondayDate(2016,2,3)
— 2015-02-16 00:00:00.000
 
— First Monday of current month
SELECT dbo.fnMondayDate(Year(getdate()),Month(getdate()),1)
— 2009-02-02 00:00:00.000  </Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>EF6_CodeFirst_with_ReverseEngineerFromDatabase</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>Reverse_Engineer_Northwind_Database___15__c_Use_Migrations_going_Forward</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>EF6_CodeFirst_with_ReverseEngineerFromDatabase</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>Reverse_Engineer_Northwind_Database___15__c_Use_Migrations_going_Forward</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>Reverse_Engineer_Northwind_Database___15__c_Use_Migrations_going_Forward
/*
1] Create an initial Migration */

&gt; Add-Migration initial -IgnoreChanges		//Where 'initial' is the name of the first one

/* If you just submitted this without the "-IgnoreChanges", 
	then it would add all the definitions in the POCOs
	and add them to the UP method
	
	By adding in "-IgnoreChanges" it does not try to upload the prior/existing POCO objects
		b/c in this set of examples, we actually created all the POCOs from
		the legacy objects that were already in the SQL schema

	Hit [enter] to submit
	
	A new 'Migrations' folder is created, with a 201701071940108_initial.cs file in it
	In that file is the following: */
	
	namespace MigrationDemo
	{
	    using System;
	    using System.Data.Entity.Migrations;
	    
	    public partial class initial : DbMigration
	    {
	        public override void Up()
	        {
	        }
	        
	        public override void Down()
	        {
	        }
	    }
	}	
/*
Note that the 'Up' method is empty.
At this point nothing has been changed on the SQL server.
Next update the database:		*/

//2] run the intial migration:

PM&gt; update-database
// the PM windows responds:
Specify the '-Verbose' flag to view the SQL statements being applied to the target database.
Applying explicit migrations: [201701071940108_initial].
Applying explicit migration: 201701071940108_initial.
Running Seed method.

//on the SQL server there is a new table:
[efdemo].[dbo].[__MigrationHistory]

//3] Now make a change in the POCO, add a field, then migrate the field up to the SQL Server

//add field 'PetsName' to class 'EngineerPerson'

public string PetsName { get; set; }

// then migrate change to schema on Server:
PM&gt; Add-Migration AddsPetsNameToEngineerPerson

/* it responds:
			The Designer Code for this migration file includes a snapshot of your current Code First model. 
			This snapshot is used to calculate the changes to your model when you scaffold the next migration. 
			If you make additional changes to your model that you want to include in this migration, 
			then you can re-scaffold it by running 'Add-Migration AddsPetsNameToEngineerPerson' again.
	
	The file '201701071956137_AddsPetsNameToEngineerPerson.cs' is generated in the 'Migrations' folder.
	Note that we left of the attributes for the field. We could have added them in the POCO,
	or we can add them to the generated migration file.
	Before we modify the file it is created like this:		*/ 

	namespace MigrationDemo
	{
	    using System;
	    using System.Data.Entity.Migrations;
	    
	    public partial class AddsPetsNameToEngineerPerson : DbMigration
	    {
	        public override void Up()
	        {
	            AddColumn("dbo.engineer_person", "PetsName", c =&gt; c.String());
	        }
	        
	        public override void Down()
	        {
	            DropColumn("dbo.engineer_person", "PetsName");
	        }
	    }
	}
	
//	Change it to this:
	namespace MigrationDemo
	{
	    using System;
	    using System.Data.Entity.Migrations;
	    
	    public partial class AddsPetsNameToEngineerPerson : DbMigration
	    {
	        public override void Up()
	        {
	            AddColumn("dbo.engineer_person", "PetsName", c =&gt; c.String(nullable:false, maxLength:30, defaultValue:"test"));
	        }
	        
	        public override void Down()
	        {
	            DropColumn("dbo.engineer_person", "PetsName");
	        }
	    }
	}
//Then to implement the change:

PM&gt; Update-Database -Verbose

/*In the output you see this:

		ALTER TABLE [dbo].[engineer_person] ADD [PetsName] [nvarchar](30) NOT NULL DEFAULT 'test'
		INSERT [dbo].[__MigrationHistory]([MigrationId], [ContextKey], [Model], [ProductVersion])
		VALUES (N'201701071956137_AddsPetsNameToEngineerPerson', N'test', </Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>EntityFramework_CodeFirst</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>01_TwoTables_using_Id_ForeignKey__CodeFirst</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>EntityFramework_CodeFirst</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>01_TwoTables_using_Id_ForeignKey__CodeFirst</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>//TwoTables_using_Id_ForeignKey__CodeFirst
//[Table] Attribute is in the System.ComponentModel.DataAnnotations.Schema namespace.

using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Data.Entity;
using System.Data.Entity.ModelConfiguration.Conventions;
using System.ComponentModel.DataAnnotations;
using System.ComponentModel.DataAnnotations.Schema;
using System.Runtime.CompilerServices;

//TwoTables_using_Id_ForeignKey

namespace CodeFirstLib
{
    [Table("BlogPosts")]
    public class BlogPost
    {
        [Key]
        public Guid Id { get; set; }
        public string Title { get; set; }
        public string Content { get; set; }
        public DateTime PublishDate { get; set; }

        [ForeignKey("Category")]				//Note that the parm "Category" is the name of the other table
        public Guid CategoryId { get; set; }	// 'CategoryId' is the name of the field in THIS table that refers to the field "Id" in the "Category" table
        
        public virtual Category Category { get; set; }	// NOT SURE WHAT THIS IS!!!!
    }

    [Table("Categories")]
    public class Category
    {
        [Key]
        public Guid Id { get; set; }			//Note here it is called "Id"
        public string Name { get; set; }
        public virtual ICollection&lt;BlogPost&gt; BlogPosts { get; set; }	//NOT SURE WHAT THIS IS DOING!!!!!
    }																	// Note that one category, and appear in Many Blogposts

    public class BlogContext : DbContext
    {

		public BlogContext()
			: base("BlogDb_HOME")
        {
        }

        public DbSet&lt;BlogPost&gt; BlogPosts { get; set; }
        public DbSet&lt;Category&gt; Categories { get; set; }

    }

    public class BlogContextInitializer : DropCreateDatabaseAlways&lt;BlogContext&gt;
    {
        protected override void Seed(BlogContext context)
        {
            Category cat1 = new Category { Id=Guid.NewGuid(), Name=".NET Framework" };
            Category cat2 = new Category { Id = Guid.NewGuid(), Name = "SQL Server" };
            Category cat3 = new Category { Id = Guid.NewGuid(), Name = "jQuery" };

            context.Categories.Add(cat1);
            context.Categories.Add(cat2);
            context.Categories.Add(cat3);

            context.SaveChanges();

        }
    }
}
</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>EntityFramework_CodeFirst</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>02_RecreatingTheDatabase_Every_Run__BackEnd</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>EntityFramework_CodeFirst</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>02_RecreatingTheDatabase_Every_Run__BackEnd</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>//02_RecreatingTheDatabase_Every_Run__BackEnd




using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Data.Entity;
using System.Data.Entity.ModelConfiguration.Conventions;
using System.ComponentModel.DataAnnotations;
using System.ComponentModel.DataAnnotations.Schema;
using System.Runtime.CompilerServices;


namespace CodeFirstLib
{
    [Table("BlogPosts")]
    public class BlogPost
    {
        [Key]
        public Guid Id { get; set; }
        public string Title { get; set; }
        public string Content { get; set; }
        public DateTime PublishDate { get; set; }

        [ForeignKey("Category")]
        public Guid CategoryId { get; set; }
        
        public virtual Category Category { get; set; }
    }

    [Table("Categories")]
    public class Category
    {
        [Key]
        public Guid Id { get; set; }
        public string Name { get; set; }
        public virtual ICollection&lt;BlogPost&gt; BlogPosts { get; set; }
    }

    public class BlogContext : DbContext
    {

		public BlogContext()
			: base("BlogDb_HOME")
        {
        }

        public DbSet&lt;BlogPost&gt; BlogPosts { get; set; }
        public DbSet&lt;Category&gt; Categories { get; set; }

    }        

	//If you wish to use CreateDatabaseIfNotExists database initializer, 
	//you need not do anything specific since it is the default database initializer. 
	//Of course, you can set it explicitly using SetInitializer() method.
    public class BlogContextInitializer : DropCreateDatabaseAlways&lt;BlogContext&gt;
    {
        protected override void Seed(BlogContext context)
        {
            Category cat1 = new Category { Id=Guid.NewGuid(), Name=".NET Framework" };
            Category cat2 = new Category { Id = Guid.NewGuid(), Name = "SQL Server" };
            Category cat3 = new Category { Id = Guid.NewGuid(), Name = "jQuery" };
			//When you set a database initializer, it won't be called immediately.
			//It will be called when the context (BlogContext) is used for the first time. 
			//In this example, the actual database creation will occur only when you add a new Category and BlogPost 
			//	and not when a new instance of BlogContext is created.
            context.Categories.Add(cat1);
            context.Categories.Add(cat2);
            context.Categories.Add(cat3);

            context.SaveChanges();

        }
    }
}
</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>EntityFramework_CodeFirst</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>about_EntityFramework_CodeFirst</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>EntityFramework_CodeFirst</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>about_EntityFramework_CodeFirst</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>about_EntityFramework_CodeFirst


/*
Code First provides three inbuilt database initializers, viz. 
	CreateDatabaseIfNotExists, 
	DropCreateDatabaseWhenModelChanges  
	DropCreateDatabaseAlways. 
	
You can also create your own database initializer either by inheriting from existing initializers 
	or by implementing the IDatabaseInitializer interface. 
	
The SetInitializer() method allows you to specify a database initializer to use for your application.

In case your application needs to seed data you can override the Seed() method of the initializer class.

from: http://weblogs.asp.net/scottgu/entity-framework-4-code-first-custom-database-schema-mapping
Several people asked me at the end of my first blog post whether there was a way to avoid having EF auto-create the database 
	for you.  I apparently didn’t make it clear enough that the auto-database creation/recreation support 
	is an option you must enable (and doesn’t always happen).  
	You can always explicitly create your database however you want (using code, .sql deployment script, a SQL admin tool, etc) 
	and just point your connection string at it – in which case EF won’t ever modify or create database schema.

EF code-first supports the ability to automatically generate database schema and create databases from model classes – 
	enabling you to avoid having to manually perform these steps.
	
This happens by default if your connection-string points to either a SQL CE or SQL Express database file that does not already exist on disk.  
	You do not need to take any manual steps for this to happen.

 By default, the Entity Framework interprets a property that's named ID or classnameID as the primary key.	
	
	
CONFIGURATION
==============
REF; http://msdn.microsoft.com/en-us/data/jj556606.aspx
Entity Framework allows a number of settings to be specified from the configuration file. 
	In general EF follows a ‘convention over configuration’ principle. 
	All the settings discussed in this post have a default behavior, 
		you only need to worry about changing the setting when the default no longer satisfies your requirements.

Code-Based Configuration (EF6 onwards)
ref: http://msdn.microsoft.com/en-us/data/jj680699</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>EntityFramework_CodeFirst</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>Create_StoredProcedure_and_Views</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>EntityFramework_CodeFirst</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>Create_StoredProcedure_and_Views</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>from: http://stackoverflow.com/questions/7667630/can-you-create-sql-views-stored-procedure-using-entity-framework-4-1-code-firs

It appears to be poorly documented however it appears you can now do some Stored Procedure manipulation using 
	AlterStoredProcedure, (https://msdn.microsoft.com/en-us/library/system.data.entity.migrations.dbmigration.alterstoredprocedure%28v=vs.113%29.aspx
	CreateStoredProcedure, (https://msdn.microsoft.com/en-us/library/system.data.entity.migrations.dbmigration.createstoredprocedure%28v=vs.113%29.aspx
	DropStoredProcedure, 
	MoveStoredProcedure, 
	RenameStoredProcedure in Entity Framework 6. 
	I haven't tried them yet so can't yet give an example of how to use them.


=================Solution # 2==============================================================
/*At first sight I really like the approach of Carl G but it involves a lot of manual interaction. 
In my scenario, I always drop all stored procedures, views... and recreate them whenever there is a change in the database. 
This way we are sure everything is up-to-date with the latest version.

Recreation happens by setting the following Initializer:*/

Database.SetInitializer(new MigrateDatabaseToLatestVersion&lt;MyContext, Configuration&gt;());

//Then our seed method will get called whenever there is a migration ready

protected override void Seed(DeploymentLoggingContext context)
    {
        // Delete all stored procs, views
        foreach (var file in Directory.GetFiles(Path.Combine(AppDomain.CurrentDomain.BaseDirectory, "Sql\\Seed"), "*.sql"))
        {
            context.Database.ExecuteSqlCommand(File.ReadAllText(file), new object[0]);
        }

        // Add Stored Procedures
        foreach (var file in Directory.GetFiles(Path.Combine(AppDomain.CurrentDomain.BaseDirectory, "Sql\\StoredProcs"), "*.sql"))
        {
            context.Database.ExecuteSqlCommand(File.ReadAllText(file), new object[0]);
        }
    }
/*
SQL Statements are stored in *.sql files for easy editing. 
Make sure your files have "Build Action" set to "Content" 
	and "Copy to Output Directory" set to "Copy Always". 
We lookup the folders and execute all scripts inside. 
Don't forget to exclude "GO" statements in your SQL because they cannot be executed with ExecuteSqlCommand().

My current directory layout is as follows:*/

    Project.DAL
    + Migrations
    + Sql
    ++ Seed
    +++ dbo.cleanDb.sql
    ++ StoredProcs
    +++ dbo.sp_GetSomething.sql

//Now you just need to drop extra stored procedures in the folder and everything will get updated appropriately.

/*Comments:
You can leave the "GO" statements in the script if you split the file text on "GO" and execute each string in the array separately. 
I suggest using new Regex("GO", RegexOptions.IgnoreCase) and skip executing empty strings.*/

=================Expansion on Solution # 2==============================================================
/*I'm using his pattern but I also map stored procedures inside of my DbContext class which allows simply calling those context methods 
	instead of using SqlQuery() 
	and calling the procedures directly from my repository.
As things can get a bit hairy when the application grows, 
	I've created a check within my Seed method 
	that makes sure the actual stored procedure parameter count match up to the parameter count on the mapping method. 
I've also updated the DROP loop emp mentioned. Instead of having to maintain a separate folder/file for the drop statements, 
	I simply read the first line of each sql file and replace CREATE with DROP (just make sure the first line is always just CREATE PROCEDURE ProcName). 
	This way all procedures in my StoredProcs folder get dropped and recreated each time Update-Database is ran. 
	The drop is also wrapped in a try-catch block in case the procedure is new. 
For the procedure parameter count to work, 
	you'll need to make sure you wrap a BEGIN/END block around your tsql since each line of the file is read up to BEGIN. 
	Also make sure each sp parameter is on new line. 
*/
 
 
 		// Drop Stored Procs
        foreach (var file in Directory.GetFiles(Path.Combine(AppDomain.CurrentDomain.BaseDirectory, "..\\DataContext\\SiteMigrations\\StoredProcs"), "*.sql"))
        {
            // Try to drop proc if its already created
            // Without this, for new procs, seed method fail on trying to delete
            try
            {
                StreamReader reader = new StreamReader(file);
                // Read first line of file to create drop command (turning CREATE [dbo].[TheProc] into DROP [dbo].[TheProc])
                string dropCommand = reader.ReadLine().Replace("CREATE", "DROP");

                context.Database.ExecuteSqlCommand(dropCommand, new object[0]);
            }
            catch { }

        }

        // Add Stored Procs
        foreach (var file in Directory.GetFiles(Path.Combine(AppDomain.CurrentDomain.BaseDirectory, "..\\DataContext\\SiteMigrations\\StoredProcs"), "*.sql"))
        {
            // File/Proc names must match method mapping names in DbContext
            int lastSlash = file.LastIndexOf('\\');
            string fileName = file.Substring(lastSlash + 1);
            string procName = fileName.Substring(0, fileName.LastIndexOf('.'));

            // First make sure proc mapping in DbContext contain matching parameters.  If not throw exception.
            // Get parameters for matching mapping
            MethodInfo mi = typeof(SiteContext).GetMethod(procName);

            if (mi == null)
            {
                throw new Exception(String.Format("Stored proc mapping for {0} missing in DBContext", procName));
            }

            ParameterInfo[] methodParams = mi.GetParameters();
            // Finished getting parameters

            // Get parameters from stored proc
            int spParamCount = 0;
            using (StreamReader reader = new StreamReader(file))
            {
                string line;                    
                while ((line = reader.ReadLine()) != null) 
                {
                    // If end of parameter section, break out
                    if (line.ToUpper() == "BEGIN")
                    {
                        break;
                    }
                    else
                    {
                        if (line.Contains("@"))
                        {
                            spParamCount++;
                        }
                    }                        
                }
            }
            // Finished get parameters from stored proc

            if (methodParams.Count() != spParamCount)
            {
                string err = String.Format("Stored proc mapping for {0} in DBContext exists but has {1} parameter(s)" +
                    " The stored procedure {0} has {2} parameter(s)", procName, methodParams.Count().ToString(), spParamCount.ToString());
                throw new Exception(err);
            }
            else
            {
                context.Database.ExecuteSqlCommand(File.ReadAllText(file), new object[0]);
            }
        }
=================Solution # 1==============================================================
/* from Carl G.
We support stored procedures in our Entity Framework Code First Migrations. 
Our approach is to create some folder to hold the .sql files (~/Sql/ for example). 
Create .sql files in the folder for both creating and dropping the stored procedure. 
E.g. Create_sp_DoSomething.sql and Drop_sp_DoSomething. 
Because the SQL runs in a batch and CREATE PROCEDURE.. must be the first statement in a batch, 
	make the CREATE PROCEDURE... the first statement in the file. 
Also, don't put GO after the DROP.... 
Add a resources file to your project, if you don't have one already. 
Drag the .sql files from solution explorer into the Files view of the Resources designer. 
Now create an empty migration (Add-Migration SomethingMeaningful_sp_DoSomething) and use:
*/

namespace MyApplication.Migrations
{
    using System;
    using System.Data.Entity.Migrations;

    public partial class SomethingMeaningful_sp_DoSomething : DbMigration
    {
        public override void Up()
        {
            this.Sql(Properties.Resources.Create_sp_DoSomething);
        }

        public override void Down()
        {
            this.Sql(Properties.Resources.Drop_sp_DoSomething);
        }
    }
}

//	~/Sql/Create_sp_DoSomething.sql

CREATE PROCEDURE [dbo].[sp_DoSomething] AS
BEGIN TRANSACTION
-- Your stored procedure here
COMMIT TRANSACTION
GO

//	~/Sql/Drop_sp_DoSomething.sql

DROP PROCEDURE [dbo].[sp_DoSomething]

/* COMMENTS
============
Next time you update the SP, couldnt you just add another sql-file with "Alter SP" in the Up-method? 
Or create a new migration and do the Down method first when doing the Up. 

In our case, if the stored procedure changes, 
	a new migration must occur to change it. Same as if new index required or if column has new definition etc. 

As I see it, if you have three migrations introducing/modifying a procedure you have: 
	(1) Up: Create SP, Down: Drop SP;
	(2) Up: Alter SP, Down: Alter SP to be like in step 1; 
	(3) Up: Alter SP again, Down: Alter SP to be like in step 2.
	
	
	
	
	
	
	
	
	
	</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>EntityFramework_CodeFirst</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>ERRORs_during_Update-Database</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>EntityFramework_CodeFirst</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>ERRORs_during_Update-Database</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>// use the modified SaveChanges overide:
//from: http://w3facility.org/question/ef-code-first-how-do-i-see-entityvalidationerrors-property-from-the-nuget-package-console/

using System.Collections.Generic;
using System.Data.Entity.Validation;
using System.Diagnostics;
using System.Text;
using Microsoft.Ajax.Utilities;
using Test_01.Models;
using globalCommon;

namespace Test_01.Migrations
	{
	using System;
	using System.Data.Entity;
	using System.Data.Entity.Migrations;
	using System.Linq;

	internal sealed class Configuration : DbMigrationsConfiguration&lt;Test_01.Models.Test_01_Db&gt;
		{
		public Configuration()
			{
			AutomaticMigrationsEnabled = true;
			ContextKey = "Test_01.Models.Test_01_Db";
			}

		protected override void Seed(Test_01.Models.Test_01_Db context)
			{

			Random rnd = new Random();

			for (int i = 0; i &lt; 10; i++)
				{
				context.Header_Record.AddOrUpdate(h =&gt; h.Header_PK, new Header_R
														{
															Percent_Done_0_00_to_1_00 =(decimal?)(rnd.NextDouble()),
															TheName = rnd.Random_LowerCaseLetters(5),
															PhoneNumber = rnd.Random_Numbers(10),
															Grade_0_to_5 = (Int16?) rnd.Next(0,5),
															bActive = ((short?)rnd.Next(0, 1) == 1),
															Detail =
																new List&lt;Detail_R&gt;
																{
							
																	new Detail_R {
																					Detail_Category_Integer = rnd.Next(1,6),
																					Detail_Category_nvarchar = rnd.Random_UpperCaseLetters(5),
																				  },
																	new Detail_R {
																					Detail_Category_Integer = rnd.Next(1,6),
																					Detail_Category_nvarchar = rnd.Random_UpperCaseLetters(5),
																				  }
																}
														}
												);
				}

			context.Enum_Record.AddOrUpdate(e =&gt; e.Enum_PK,
				new Enum_R { EnumDescription = "unknown", EnumValue = 0, EnumNameSpace_FK = 1 },
				new Enum_R { EnumDescription = "Red", EnumValue = 1, EnumNameSpace_FK = 1 },
				new Enum_R { EnumDescription = "Blue", EnumValue = 2, EnumNameSpace_FK = 1 },
				new Enum_R { EnumDescription = "Green", EnumValue = 3, EnumNameSpace_FK = 1 },
				new Enum_R { EnumDescription = "Indigo", EnumValue = 4, EnumNameSpace_FK = 1 },
				new Enum_R { EnumDescription = "Violet", EnumValue = 5, EnumNameSpace_FK = 1 }
				);

			SaveChanges(context);	//	&lt;=================For debugging: DbEntityValidationException 
		
			}
		
		/// &lt;summary&gt;
		/// Wrapper for SaveChanges adding the Validation Messages to the generated exception
		/// &lt;/summary&gt;
		/// &lt;param name="context"&gt;The context.&lt;/param&gt;
		private void SaveChanges(DbContext context)
			{
			try
				{
				context.SaveChanges();
				}
			catch (DbEntityValidationException ex)
				{
				StringBuilder sb = new StringBuilder();

				foreach (var failure in ex.EntityValidationErrors)
					{
					sb.AppendFormat("{0} failed validation\n", failure.Entry.Entity.GetType());
					foreach (var error in failure.ValidationErrors)
						{
						sb.AppendFormat("- {0} : {1}", error.PropertyName, error.ErrorMessage);
						sb.AppendLine();
						}
					}

				throw new DbEntityValidationException(
					"Entity Validation Failed - errors follow:\n" +
					sb.ToString(), ex
				); // Add the original exception as the innerException
				}
			}
		}
	}
</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>EntityFramework_CodeFirst</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>Seed_about</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>EntityFramework_CodeFirst</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>Seed_about</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>

/*
During the testing phase you often need to populate database tables with sample data. 
At times you also need to populate some application data at the time of database creation. 
For example, while creating our sample database you may want to populate the Categories table with some predefined categories. 
Such seed data can be added to the database being created by overriding the Seed() method of the database initializer class. 
Consider the following piece of code :
*/

//This could go in the Configuration file.

public class BlogContextSeedInitializer : DropCreateDatabaseAlways&lt;BlogContext&gt;
{
    protected override void Seed(BlogContext context)
    {
        Category cat1 = new Category { Id = Guid.NewGuid(), Name = ".NET Framework" };
        Category cat2 = new Category { Id = Guid.NewGuid(), Name = "SQL Server" };
        Category cat3 = new Category { Id = Guid.NewGuid(), Name = "jQuery" };
        context.Categories.Add(cat1);
        context.Categories.Add(cat2);
        context.Categories.Add(cat3);
        context.SaveChanges();
    }
}

/*
Here, you created a custom database initializer by inheriting DropCreateDatabaseAlways class. 
Further, you need to override the Seed() method. 
The Seed() method receives the context object as a parameter. 
You then create three categories and add them to the context. 
Finally SaveChanges() method saves the data to the database that was created during the initialization process.

To see the Seed() method in action, 
	you need to use BlogContextSeedInitializer in the Main() method. 
The front end would correspond to the Model in MVC where the context is initialized for the Model.
Adding the following line of code will do that job:

*/

Database.SetInitializer(new BlogContextSeedInitializer());</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>EntityFramework_General</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>01_InitializerClass__/DAL/SchoolInitializer.cs</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>EntityFramework_General</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>01_InitializerClass__/DAL/SchoolInitializer.cs</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>/*
The Seed method takes the database context object as an input parameter, and the code in the method uses
that object to add new entities to the database. For each entity type, the code creates a collection of new
 entities, adds them to the appropriate DbSet property, and then saves the changes to the database. It isn't
necessary to call the  SaveChanges method after each group of entities, as is done here, but doing that helps
you locate the source of a problem if an exception occurs while the code is writing to the database.




*/
using System;
using System.Collections.Generic;
using System.Linq;
using System.Web;
using System.Data.Entity;
using ContosoUniversity.Models;

namespace ContosoUniversity.DAL
{
    public class SchoolInitializer : System.Data.Entity. DropCreateDatabaseIfModelChanges&lt;SchoolContext&gt;
    {
        protected override void Seed(SchoolContext context)
        {
            var students = new List&lt;Student&gt;
            {
            new Student{FirstMidName="Carson",LastName="Alexander",EnrollmentDate=DateTime.Parse("2005-09-01")},
            new Student{FirstMidName="Meredith",LastName="Alonso",EnrollmentDate=DateTime.Parse("2002-09-01")},
            new Student{FirstMidName="Arturo",LastName="Anand",EnrollmentDate=DateTime.Parse("2003-09-01")},
            new Student{FirstMidName="Gytis",LastName="Barzdukas",EnrollmentDate=DateTime.Parse("2002-09-01")},
            new Student{FirstMidName="Yan",LastName="Li",EnrollmentDate=DateTime.Parse("2002-09-01")},
            new Student{FirstMidName="Peggy",LastName="Justice",EnrollmentDate=DateTime.Parse("2001-09-01")},
            new Student{FirstMidName="Laura",LastName="Norman",EnrollmentDate=DateTime.Parse("2003-09-01")},
            new Student{FirstMidName="Nino",LastName="Olivetto",EnrollmentDate=DateTime.Parse("2005-09-01")}
            };

            students.ForEach(s =&gt; context.Students.Add(s));
            context.SaveChanges();
            var courses = new List&lt;Course&gt;
            {
            new Course{CourseID=1050,Title="Chemistry",Credits=3,},
            new Course{CourseID=4022,Title="Microeconomics",Credits=3,},
            new Course{CourseID=4041,Title="Macroeconomics",Credits=3,},
            new Course{CourseID=1045,Title="Calculus",Credits=4,},
            new Course{CourseID=3141,Title="Trigonometry",Credits=4,},
            new Course{CourseID=2021,Title="Composition",Credits=3,},
            new Course{CourseID=2042,Title="Literature",Credits=4,}
            };
            courses.ForEach(s =&gt; context.Courses.Add(s));
            context.SaveChanges();
            var enrollments = new List&lt;Enrollment&gt;
            {
            new Enrollment{StudentID=1,CourseID=1050,Grade=Grade.A},
            new Enrollment{StudentID=1,CourseID=4022,Grade=Grade.C},
            new Enrollment{StudentID=1,CourseID=4041,Grade=Grade.B},
            new Enrollment{StudentID=2,CourseID=1045,Grade=Grade.B},
            new Enrollment{StudentID=2,CourseID=3141,Grade=Grade.F},
            new Enrollment{StudentID=2,CourseID=2021,Grade=Grade.F},
            new Enrollment{StudentID=3,CourseID=1050},
            new Enrollment{StudentID=4,CourseID=1050,},
            new Enrollment{StudentID=4,CourseID=4022,Grade=Grade.F},
            new Enrollment{StudentID=5,CourseID=4041,Grade=Grade.C},
            new Enrollment{StudentID=6,CourseID=1045},
            new Enrollment{StudentID=7,CourseID=3141,Grade=Grade.A},
            };
            enrollments.ForEach(s =&gt; context.Enrollments.Add(s));
            context.SaveChanges();
        }
    }
}</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>EntityFramework_General</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>01_InitializerClass__about</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>EntityFramework_General</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>01_InitializerClass__about</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>01_InitializerClass__about

/*
from: http://www.asp.net/mvc/overview/getting-started/getting-started-with-ef-using-mvc/creating-an-entity-framework-data-model-for-an-asp-net-mvc-application

The Entity Framework can automatically create (or drop and re-create) a database for you when the application runs. 
You can specify that this should be done every time your application runs or only when the model is out of sync with the existing database. 
You can also write a Seed method that the Entity Framework automatically calls after creating the database in order to populate it with test data. 

The default behavior is to create a database only if it doesn't exist (and throw an exception if the model has changed 
	and the database already exists). 
In this section you'll specify that the database should be dropped and re-created whenever the model changes. 
Dropping the database causes the loss of all your data. 
This is generally OK during development, 
	because the Seed method will run when the database is re-created and will re-create your test data. 
But in production you generally don't want to lose all your data every time you need to change the database schema. 

1] In the DAL folder, create a new class file named SchoolInitializer.cs and replace the template code with the
following code, which causes a database to be created when needed and loads test data into the new database.

2] To tell Entity Framework to use your initializer class, 
	add an element to the entityFramework element in the application Web.config file (the one in the root project folder), 
	as shown in the attached example:
	
OR

As an alternative to setting the initializer in the Web.config file is to do it in code by adding a Database.SetInitializer 
	statement to the Application_Start method in in the Global.asax.cs file. For more information, 
For more info see: http://www.codeguru.com/csharp/article.php/c19999/Understanding-Database-Initializers-in-Entity-Framework-Code-First.htm	
	
Now you'll create a web page to display data, and the process of requesting the data will automatically trigger
the creation of the database. 
You'll begin by creating a new controller. But before you do that, 
	build the project to make the model and context classes available to MVC controller scaffolding.</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>EntityFramework_General</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>EntityFramework_Migrations__Configuration.cs</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>EntityFramework_General</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>EntityFramework_Migrations__Configuration.cs</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>namespace OdeToFood.Migrations  //Required for Restaurant Class    
{
    using OdeToFood.Models;    
    using System;
    using System.Collections.Generic;
    using System.Data.Entity;
    using System.Data.Entity.Migrations;
    using System.Linq;
    //=============================================================================================================
    //This file was generated by the Package Manager Command: Enable-Migrations -ContextTypeName OdeToFoodDb -Force
    //It can be run either automatically, or from the PackageManager Console: Update-Database -Verbose
    //=============================================================================================================
    internal sealed class Configuration : DbMigrationsConfiguration&lt;OdeToFood.Models.OdeToFoodDb&gt;
    {
        public Configuration()
        {
            AutomaticMigrationsEnabled = true; //default was set to false, but changed it so that it will be able to run automatically
            //                                  Once the DB is mature, you want to be more carefull about making changes so casually.
            ContextKey = "OdeToFood.Models.OdeToFoodDb";    //This was not in the author's version, but may be here b/c I did a -Force
        }

        //The see will run everytime I update the Database, generally we talk about updates when we want to update the schema.
        //You can configure the Application to automatically apply updates to the persisted database,
        //  or through the Package Manager Console:
        protected override void Seed(OdeToFood.Models.OdeToFoodDb context)
        {
            //This walks up to the Restaurants table in the Database, and says to add or update the following records
            // Look up the restaurant up by name, if it finds it, it will update the City and Country fields
            //  if the restaurant does not exist, according to the name field, then it will add the record to the table
            // And for the last restaurant, it will add the review
            context.Restaurants.AddOrUpdate(r =&gt; r.Name,
               new Restaurant { Name = "Sabatino's", City = "Baltimore", Country = "USA" },
               new Restaurant { Name = "Great Lake", City = "Chicago", Country = "USA" },
               new Restaurant
               {
                   Name = "Smaka",
                   City = "Gothenburg",
                   Country = "Sweden",
                   Reviews =
                       new List&lt;RestaurantReview&gt; { 
                       new RestaurantReview { Rating = 9, Body="Great food!", ReviewerName="Scott" }
                   }
               });
        }

        //protected override void Seed(OdeToFood.Models.OdeToFoodDb context)
        //{
        //    //  This method will be called after migrating to the latest version.

        //    //  You can use the DbSet&lt;T&gt;.AddOrUpdate() helper extension method 
        //    //  to avoid creating duplicate seed data. E.g.
        //    //
        //    //    context.People.AddOrUpdate(
        //    //      p =&gt; p.FullName,
        //    //      new Person { FullName = "Andrew Peters" },
        //    //      new Person { FullName = "Brice Lambson" },
        //    //      new Person { FullName = "Rowan Miller" }
        //    //    );
        //    //
        //}
    }
}
</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>EntityFramework_General</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>EntityFramework_Migrations</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>EntityFramework_General</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>EntityFramework_Migrations</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>/* I am using VS 2013 Express for this example

'Migrations' are a feature of EF that allow you to 
a) configure Database Schemas with C# code
b) Seed your database with C# code

Migrations can then track changes you are making in your entity classes,
and it can keep the Database Schema in sync with the changes you make in 
your C# code.


To get to the Migrations:
--------------------------
a) View {menu} =&gt; Other Windows =&gt; Package Manager console
	OR type package in the quick Launch entry box {in upper right of IDE}
b) Then 'Package Manager Console' opens in bottom pane. It is a 'PowerShell' command line
c) enter command:
*/
 Enable-Migrations -ContextTypeName OdeToFoodDb
/*
---------------------------------------------------------------------------------------------------------------
Dealt with Error in this next section:
---------------------------------------------------------------------------------------------------------------
Got error: The term 'Enable-Migrations' is not recognized as the name of a cmdlet, function, script file, or operable program. Check the spelling of the name, or if a path
 was included, verify that the path is correct and try again.

enter command: 
*/
Install-Package EntityFramework -IncludePrerelease
/*
It responded: Installing 'EntityFramework 6.1.2-beta2'....
			  Successfully uninstalled 'EntityFramework 5.0.0'.
			
Then I restarted VS	

For this example it responded like this after I repeated the original command: Enable-Migrations -ContextTypeName OdeToFoodDb
"Migrations have already been enabled in project 'OdeToFood'. To overwrite the existing migrations configuration, use the -Force parameter."
So I submitted this command:
*/
 Enable-Migrations -ContextTypeName OdeToFoodDb -Force
/*
NOTE "-Force" made it overwrite the "Configuration.cs" file that was already there from the author.

It responded:
Checking if the context targets an existing database...
Detected database created with a database initializer. Scaffolded migration '201412051643041_InitialCreate' corresponding to existing database. To use an automatic migration instead, delete the Migrations folder and re-run Enable-Migrations specifying the -EnableAutomaticMigrations parameter.
Code First Migrations enabled for project OdeToFood.
---------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------

Now there is a new root folder in my solution called "Migrations"
The author ended up with two files in the folder, 
	I ended up with three files, but I also have two connections in my web.config, he only had one.
------------------------------
Configuration.cs
201412051643041_InitialCreate.cs		&lt;= This one is from 2014, and is probably mine {This is the Schema Change Script generated in C#}
201210161548264_InitialCreate.cs		&lt;= This one is from 2012, and is probably from the Author


	
The configuration.cs file can be set to run automatically.
It also has a seed method for putting data into the tables, see notes in the snippet: EntityFramework_Migrations__Configuration.cs
It (Configuration.cs) can be run either automatically, or from the PackageManager Console: 
*/
Update-Database -Verbose
/*
---------------------------------------------------------------------------------------------------------------
Dealt with Error in this next section:
---------------------------------------------------------------------------------------------------------------
I got the message: "The project 'OdeToFood' failed to build.", so I deleted the file: "201210161548264_InitialCreate.cs", 
and ran it again, here is the response:
=======================================
PM&gt; Update-Database -Verbose
Using StartUp project 'OdeToFood'.
Using NuGet project 'OdeToFood'.
Specify the '-Verbose' flag to view the SQL statements being applied to the target database.
Target database is: 'OdeToFoodDb' (DataSource: .\SQLEXPRESS, Provider: System.Data.SqlClient, Origin: Configuration).
Upgrading history table.
CREATE TABLE [dbo].[__MigrationHistory2] (
    [MigrationId] [nvarchar](150) NOT NULL,
    [ContextKey] [nvarchar](300) NOT NULL,
    [Model] [varbinary](max) NOT NULL,
    [ProductVersion] [nvarchar](32) NOT NULL,
    CONSTRAINT [PK_dbo.__MigrationHistory2] PRIMARY KEY ([MigrationId], [ContextKey])
)
INSERT INTO [dbo].[__MigrationHistory2]
SELECT LEFT([MigrationId], 150), 'OdeToFood.Models.OdeToFoodDb', [Model], LEFT([ProductVersion], 32) FROM [dbo].[__MigrationHistory]
DROP TABLE [dbo].[__MigrationHistory]
EXECUTE sp_rename @objname = N'dbo.__MigrationHistory2', @newname = N'__MigrationHistory', @objtype = N'OBJECT'
Caution: Changing any part of an object name could break scripts and stored procedures.
No pending explicit migrations.
Running Seed method.
=======================================
Now I can see the records in the table, and I see the new table: __MigrationHistory2
and when I run the MVC application, I can see the records as well.
---------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------


Now if you modify one of the record classes in the Application (e.g.  RestaurantReview) by adding a field, then you could do one of two things:
a) Could tell the EF that I explicitly need a migration script to move the DB schema as it exists to a schema with a new field in it
OR
b) I can update the C# record, and let EF figure it out, b/c it has this setting:
*/
AutomaticMigrationsEnabled = true;
/*

EF tracks the migrations that have been done, and what needs to be done in a hidden table called _MigrationHistory, which is a table on the SQL server.

So if I add a field to the RestaurantReview called: "JUNK":
*/
public  double dblJUNK { get; set; }

/* then in the PackageManager Console run the command to update the schema automatically: */
Update-Database -Verbose
/* It gave me this:
=======================================
PM&gt; Update-Database -Verbose
Using StartUp project 'OdeToFood'.
Using NuGet project 'OdeToFood'.
Specify the '-Verbose' flag to view the SQL statements being applied to the target database.
Target database is: 'OdeToFoodDb' (DataSource: .\SQLEXPRESS, Provider: System.Data.SqlClient, Origin: Configuration).
No pending explicit migrations.
Applying automatic migration: 201412051941558_AutomaticMigration.
ALTER TABLE [dbo].[RestaurantReviews] ADD [dblJUNK] [float] NOT NULL DEFAULT 0
INSERT [dbo].[__MigrationHistory]([MigrationId], [ContextKey], [Model], [ProductVersion])
VALUES (N'201412051941558_AutomaticMigration', N'OdeToFood.Models.OdeToFoodDb',  0x1F8B080000...2DB1F0000 , N'6.1.2-beta2-31111')

Running Seed method.
=======================================
I edited the third parameter that it put into the Migration history for brevity sake (...)
Now there is a new field called Junk, and it was reseeded</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>EntityFramework</Category>
        <Language>TEXT</Language>
        <Public>false</Public>
        <Name>CommonErrors_solutions_and_workarounds</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>EntityFramework</Category>
          <Language>TEXT</Language>
          <Public>false</Public>
          <Name>CommonErrors_solutions_and_workarounds</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>ref: http://www.asp.net/mvc/overview/getting-started/getting-started-with-ef-using-mvc/advanced-entity-framework-scenarios-for-an-mvc-web-application


Common errors, and solutions or workarounds for them 

Cannot create/shadow copy

Error Message:


Cannot create/shadow copy '&lt;filename&gt;' when that file already exists.
=======================================================================

Solution


 Wait a few seconds and refresh the page.

Update-Database not recognized
=======================================================================

Error Message (from the Update-Database command in the PMC): 

The term 'Update-Database' is not recognized as the name of a cmdlet, function, script file, or operable program. Check the spelling of the name, or if a path was included, verify that the path is correct and try again.


Solution

 Exit Visual Studio. Reopen project and try again. 

Validation failed
=======================================================================
Error Message (from the Update-Database command in the PMC): 

Validation failed for one or more entities. See 'EntityValidationErrors' property for more details.


Solution

One cause of this problem is validation errors when the Seed method runs.  See  Seeding and Debugging Entity Framework (EF) DBs for tips on debugging the Seed method.

HTTP 500.19 error
=======================================================================
Error Message: 

HTTP Error 500.19 - Internal Server Error
The requested page cannot be accessed because the related configuration data for the page is invalid.


Solution

One way you can get this error is from having multiple copies of the solution, each of them using the same port number. You can usually solve this problem by exiting all instances of Visual Studio, then restarting the project you're working on. If that doesn't work, try changing the port number. Right click on the project file and then click properties. Select the Web tab and then change the port number in the Project Url text box.

Error locating SQL Server instance
=======================================================================
Error Message: 

A network-related or instance-specific error occurred while establishing a connection to SQL Server. The server was not found or was not accessible. Verify that the instance name is correct and that SQL Server is configured to allow remote connections. (provider: SQL Network Interfaces, error: 26 - Error Locating Server/Instance Specified)


Solution

Check the connection string. If you have manually deleted the database, change the name of the database in the construction string.
</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>EntityFramework</Category>
        <Language>TEXT</Language>
        <Public>false</Public>
        <Name>EF_Migration_Problems_Solutions</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>EntityFramework</Category>
          <Language>TEXT</Language>
          <Public>false</Public>
          <Name>EF_Migration_Problems_Solutions</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>
6] error when running commands in PackageManager:

"cannot be loaded because the execution of scripts is disabled on this system"

A) FROM: http://stackoverflow.com/questions/4037939/powershell-says-execution-of-scripts-is-disabled-on-this-system

Running C:\Windows\SysWOW64\WindowsPowerShell\v1.0\ powershell.exe as Administrator, 
then Set-ExecutionPolicy RemoteSigned helped!

===================================================================================================================
5] Error when right clicking in a controller and creating a view that uses a model:
"has already been added" "There was an error running the selected code generator" "A configuration for type" "use the Entity&lt;T&gt;() or Complex&lt;T&gt;() methods" 

from: http://stackoverflow.com/questions/24974218/scaffolding-controller-doesnt-work-with-visual-studio-2013-update-3-and-4

by changing my DBContext derived class to use IDbSet properties instead of DbSet 
	I could generate the controllers and views just fine. 
However, for me this introduced another issue, IDbSet does not support the async methods.


===================================================================================================================
4] 
When I submitted the command: 
	PM&gt; Enable-Migrations -ContextTypeName Test_01_Db
I got errors like this:
	"Test_01.Models.Detail_forHeader_R: : EntityType 'Detail_forHeader_R' has no key defined. Define the key for this EntityType."

You should add attribute [Key] before property CatId:

		using System.ComponentModel.DataAnnotations;	//required for [Key]

        public partial class Category
        {
            [Key]
            public int CatId { get; set; }
            public string CatName { get; set; }
            public string CatDescription { get; set; }
            public List&lt;Product&gt; Product { get; set; }
        }

The problem is that EF can work only when it knows primary key of table. By default EF recognize as primary key property with name Id. If your table has another primary key, you can mark it with attribute [Key] or set Key with fluent configuration.

===================================================================================================================
1] I got this error when I submitted the command:	Update-Database -Verbose

I was getting this error: The operation failed because an index 
	or statistics with name 'IX_RestaurantId' already exists 
	on table 'dbo.RestaurantReviews'. 
	
This was from a solution from Pluralsight that already had a migration file in it. There was some code in that file that seemed 
	to create the index in question, and when I commented it out, it did stop giving me the error, but did  not run the Seed Method.
	I did not like this solution
	b/c I could not see the index in the auto-generated file from MSSMS, and the Seed data was not there
	
Then I realized that there really was no need for the Migration file b/c the C# classes define all the tables. So I excluded the 
migration file, and ran the command again so that it would build the database directly from the C# class defintions.
It seemed to succeed and the Seed method did run successfully. It still did not have the index that generated the original error message.

===================================================================================================================
2] When I went to execute it gave me this error:
	"Could not load file or assembly 'DotNetOpenAuth.Core, Version=4.0.0.0, Culture=neutral, 
		PublicKeyToken=2780ccd10d57b246' or one of its dependencies. 
		The system cannot find the file specified. "
		
I tried updating the NuGet Packages, then I needed to restart VS
That did it.

===================================================================================================================
4.3.4.13329   -   4.3.0.0

3] Error: "Could not load file or assembly 'DotNetOpenAuth.Core, Version=4.0.0.0, Culture=neutral, 
			PublicKeyToken=2780ccd10d57b246' or one of its dependencies. 
			The system cannot find the file specified. "

ref: http://stackoverflow.com/questions/13942653/could-not-load-file-or-assembly-dotnetopenauth-core

To fix this, delete the _bin_deployableAssemblies folder, 
and your application's bin folder and then clean and rebuild your application and it will work again.

From that same article:
------------------------
"In my case the MVC4 application was running fine in the VS2012 debugger, but I was getting the 
"Could not load file or assembly 'DotNetOpenAuth.Core'" error when I publish the application on the server.
Checking the 
"Delete all existing files prior to publish" checkbox on the settings tab of Publish Web window solved the issue."
as well as the:
	 [x] Exclude files from the App_Data folder
	
From that same article: I solved the problem by installing the package using Nuget
------------------------
&gt; Install-Package Microsoft.AspNet.WebPages.OAuth

From that same article:
-----------------------
for me, it worked like the following, using packager manager console, I've uninstall the packages, by the following order sequence and I re installed it, it solved my issue..

    Uninstalling

Uninstall-Package Microsoft.AspNet.WebPages.OAuth
Uninstall-Package DotNetOpenAuth.AspNet 
Uninstall-Package DotNetOpenAuth.OpenId.RelyingParty
Uninstall-Package DotNetOpenAuth.OpenId.Core 
Uninstall-Package DotNetOpenAuth.OAuth.Consumer
Uninstall-Package DotNetOpenAuth.OAuth.Core
Uninstall-Package DotNetOpenAuth.core

Build, If you are not using the above packages then this should solve the problem, to Install the above again in case you need it, do the following..

    Reinstall

install-Package DotNetOpenAuth.AspNet 
install-Package Microsoft.AspNet.WebPages.OAuth
install-Package DotNetOpenAuth.OpenId.RelyingParty
install-Package DotNetOpenAuth.OpenId.Core 
install-Package DotNetOpenAuth.OAuth.Consumer
install-Package DotNetOpenAuth.OAuth.Core
install-Package DotNetOpenAuth.core

recommanded:

    go to the bin folder of the project, clear it
    when to publish to the IIS, delete the virtual directory and assign the website again to the IIS, since I faced that issue of having cash in the server which was making the problem...

From the same article:
------------------------
By installing the following two packages I resolved this problem on VS2012 Professional Update 4 RC:

    Open the Package Manager Console from the Menu:

    Tools| Library Package Manage | Package Manager Console

    Install-Package DotNetOpenAuth.AspNet
    Install-Package Microsoft.AspNet.WebHelpers
    
        
From: http://www.swiftsoftwaregroup.com/aps-net-mvc-4-could-not-load-file-or-assembly-dotnetopenauth-core-version-4-0-0-0/
-------------------------------------------------------------------------------------------------------------------------------        
 The key pieces of information here are at lines 3 and 7. Basically, Microsoft.Web.WebPages.OAuth needs DotNetOpenAuth.Core 4.0.0.0, but the DotNetOpenAuth.Core I have is version 4.3.0.0.

The solution is to add these lines under the &lt;runtime&gt;/&lt;assemblyBinding&gt; section of the root Web.config:

&lt;dependentAssembly&gt;
	&lt;assemblyIdentity name="DotNetOpenAuth.AspNet" publicKeyToken="2780ccd10d57b246" culture="neutral" /&gt;
	&lt;bindingRedirect oldVersion="0.0.0.0-4.3.0.0" newVersion="4.3.0.0" /&gt;
&lt;/dependentAssembly&gt;
&lt;dependentAssembly&gt;
	&lt;assemblyIdentity name="DotNetOpenAuth.Core" publicKeyToken="2780ccd10d57b246" culture="neutral" /&gt;
	&lt;bindingRedirect oldVersion="0.0.0.0-4.3.0.0" newVersion="4.3.0.0" /&gt;
&lt;/dependentAssembly&gt;   

===================================================================================================================</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>EntityFramework</Category>
        <Language>TEXT</Language>
        <Public>false</Public>
        <Name>EF_Migrations_about</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>EntityFramework</Category>
          <Language>TEXT</Language>
          <Public>false</Public>
          <Name>EF_Migrations_about</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>/* I am using VS 2013 Express for this example

'Migrations' are a feature of EF that allow you to 
a) configure Database Schemas with C# code
b) Seed your database with C# code

Migrations can then track changes you are making in your entity classes,
and it can keep the Database Schema in sync with the changes you make in 
your C# code.


To get to the Migrations:
--------------------------
a) View {menu} =&gt; Other Windows =&gt; Package Manager console
	OR type package in the quick Launch entry box {in upper right of IDE}
b) Then 'Package Manager Console' opens in bottom pane. It is a 'PowerShell' command line
c) enter command: Enable-Migrations -ContextTypeName OdeToFoodDb
---------------------------------------------------------------------------------------------------------------
Dealt with Error in this next section:
---------------------------------------------------------------------------------------------------------------
Got error: The term 'Enable-Migrations' is not recognized as the name of a cmdlet, function, script file, or operable program. Check the spelling of the name, or if a path
 was included, verify that the path is correct and try again.

enter command: 
*/
Install-Package EntityFramework -IncludePrerelease
/*
It responded: Installing 'EntityFramework 6.1.2-beta2'....
			  Successfully uninstalled 'EntityFramework 5.0.0'.
			
Then I restarted VS	

For this example it responded like this after I repeated the original command: Enable-Migrations -ContextTypeName OdeToFoodDb
"Migrations have already been enabled in project 'OdeToFood'. To overwrite the existing migrations configuration, use the -Force parameter."
So I submitted this command:
*/
 Enable-Migrations -ContextTypeName OdeToFoodDb -Force
/*
NOTE "-Force" made it overwrite the "Configuration.cs" file that was already there from the author.

It responded:
Checking if the context targets an existing database...
Detected database created with a database initializer. Scaffolded migration '201412051643041_InitialCreate' corresponding to existing database. To use an automatic migration instead, delete the Migrations folder and re-run Enable-Migrations specifying the -EnableAutomaticMigrations parameter.
Code First Migrations enabled for project OdeToFood.
---------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------


Now there is a new root folder in my solution called "Migrations"
The author ended up with two files in the folder, 
	I ended up with three files, but I also have two connections in my web.config, he only had one.
------------------------------
Configuration.cs
201412051643041_InitialCreate.cs		&lt;= This one is from 2014, and is probably mine {This is the Schema Change Script generated in C#}
201210161548264_InitialCreate.cs		&lt;= This one is from 2012, and is probably from the Author


||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||

The first time you want to create the database from the code-first code, use this command:

&gt;Add-Migration InitialCreate

||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
	
The configuration.cs file can be set to run automatically.
It also has a seed method for putting data into the tables, see notes in the snippet: EntityFramework_Migrations__Configuration.cs
It (Configuration.cs) can be run either automatically, or from the PackageManager Console: 
*/
Update-Database -Verbose
/*
---------------------------------------------------------------------------------------------------------------
Dealt with Error in this next section:
---------------------------------------------------------------------------------------------------------------
I got the message: "The project 'OdeToFood' failed to build.", so I deleted the file: "201210161548264_InitialCreate.cs", 
and ran it again, here is the response:
=======================================
PM&gt; Update-Database -Verbose
Using StartUp project 'OdeToFood'.
Using NuGet project 'OdeToFood'.
Specify the '-Verbose' flag to view the SQL statements being applied to the target database.
Target database is: 'OdeToFoodDb' (DataSource: .\SQLEXPRESS, Provider: System.Data.SqlClient, Origin: Configuration).
Upgrading history table.
CREATE TABLE [dbo].[__MigrationHistory2] (
    [MigrationId] [nvarchar](150) NOT NULL,
    [ContextKey] [nvarchar](300) NOT NULL,
    [Model] [varbinary](max) NOT NULL,
    [ProductVersion] [nvarchar](32) NOT NULL,
    CONSTRAINT [PK_dbo.__MigrationHistory2] PRIMARY KEY ([MigrationId], [ContextKey])
)
INSERT INTO [dbo].[__MigrationHistory2]
SELECT LEFT([MigrationId], 150), 'OdeToFood.Models.OdeToFoodDb', [Model], LEFT([ProductVersion], 32) FROM [dbo].[__MigrationHistory]
DROP TABLE [dbo].[__MigrationHistory]
EXECUTE sp_rename @objname = N'dbo.__MigrationHistory2', @newname = N'__MigrationHistory', @objtype = N'OBJECT'
Caution: Changing any part of an object name could break scripts and stored procedures.
No pending explicit migrations.
Running Seed method.
=======================================
Now I can see the records in the table, and I see the new table: __MigrationHistory2
and when I run the MVC application, I can see the records as well.
---------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------


Now if you modify one of the record classes in the Application (e.g.  RestaurantReview) by adding a field, then you could do one of two things:
a) Could tell the EF that I explicitly need a migration script to move the DB schema as it exists to a schema with a new field in it
OR
b) I can update the C# record, and let EF figure it out, b/c it has this setting:
*/
AutomaticMigrationsEnabled = true;
/*

EF tracks the migrations that have been done, and what needs to be done in a hidden table called _MigrationHistory, which is a table on the SQL server.

So if I add a field to the RestaurantReview called: "JUNK":
*/
public  double dblJUNK { get; set; }

/* then in the PackageManager Console run the command to update the schema automatically: */
Update-Database -Verbose
/* It gave me this:
=======================================
PM&gt; Update-Database -Verbose
Using StartUp project 'OdeToFood'.
Using NuGet project 'OdeToFood'.
Specify the '-Verbose' flag to view the SQL statements being applied to the target database.
Target database is: 'OdeToFoodDb' (DataSource: .\SQLEXPRESS, Provider: System.Data.SqlClient, Origin: Configuration).
No pending explicit migrations.
Applying automatic migration: 201412051941558_AutomaticMigration.
ALTER TABLE [dbo].[RestaurantReviews] ADD [dblJUNK] [float] NOT NULL DEFAULT 0
INSERT [dbo].[__MigrationHistory]([MigrationId], [ContextKey], [Model], [ProductVersion])
VALUES (N'201412051941558_AutomaticMigration', N'OdeToFood.Models.OdeToFoodDb',  0x1F8B080000...2DB1F0000 , N'6.1.2-beta2-31111')

Running Seed method.
=======================================
I edited the third parameter that it put into the Migration history for brevity sake (...)
Now there is a new field called Junk, and it was reseeded</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>FORM</Category>
        <Language>VB.NET</Language>
        <Public>false</Public>
        <Name>sub_New_GetStuffFrom_Excel_SettingsFile</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>FORM</Category>
          <Language>VB.NET</Language>
          <Public>false</Public>
          <Name>sub_New_GetStuffFrom_Excel_SettingsFile</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>'sub_New_GetStuffFrom_Excel_SettingsFile
'==========================================
'1] Call Main when the first form is created
'2] mod: Main
'3] mod_Global
'===========================================


'1] Call Main when the first form is created
    Public Sub New()

        ' This call is required by the Windows Form Designer.
        InitializeComponent()
        Main()
        ' Add any initialization after the InitializeComponent() call.

    End Sub
    
    
'2] mod: Main


Module mod_Main


    Friend Sub Main()

        'If the Debug Log file is still there delete it.
        If c_gigFileManager.fn_b_TheFileExists(g_sAppPath &amp; k_NameOfDebugLogFile) Then
            c_gigFileManager.DeleteFile(g_sAppPath &amp; k_NameOfDebugLogFile)
        End If


        'Get the Settings  
        If c_gigFileManager.fn_b_TheFileExists(g_sAppPath &amp; g_sINIFileName) Then
            'Load Facilities and Jobs from INI File, then into their respective objects

            Instantiate_TheFacilities_FromINI()
            Load_TheFacilitieObjects_IntoThe_FacilityArray()

            Instantiate_TheJobs_FromINI()
            Load_TheJobObjects_IntoThe_JobListArray()

        Else
            'Load Default Facilities and Jobs from Hard-Code

            Instantiate_TheDefaultFacilities()
            Load_TheFacilitieObjects_IntoThe_FacilityArray()

            Instantiate_TheDefaultJobs()
            Load_TheJobObjects_IntoThe_JobListArray()

            MsgBox("The INI Settings File was not located where it should have been!" &amp; VbCrLf &amp; "So the hard-coded default settings were used." &amp; VbCrLf &amp; "You should create the INI file if you need to use custom settings")

        End If

        'Format the UI



    End Sub






    Sub Instantiate_TheJobs_FromINI()
            Dim sPathFile As String = g_sAppPath &amp; g_sINIFileName
            Dim dsX As DataSet = c_gigExcelAutomator.fnReadExcel_PassBackDataSet(sPathFile, k_NameOfSecondTableInSettingsDataSet, k_NameOfSecondSheetInINIFile)
            Dim tX As DataTable = dsX.Tables(k_NameOfSecondTableInSettingsDataSet)
            Dim iRowOfJobTable As Integer = 0

            'c_gigDataSetTools.DebugPrintTableInfoForTheDataSet(dsX)

            With tX.Rows(iRowOfJobTable)
                '   AAA1 "DMC - New Payment"
                g_oJob_DMC_NewPayment.iJob_IDNo = CInt(.Item(k_NameOfCol_1_InJobDataSetTable0).ToString)
                g_oJob_DMC_NewPayment.sJob_Name = .Item(k_NameOfCol_2_InJobDataSetTable0).ToString
                g_oJob_DMC_NewPayment.iJob_Parm_Array_Length = CInt(.Item(k_NameOfCol_3_InJobDataSetTable0).ToString)
                g_oJob_DMC_NewPayment.sJob_ReadMe = .Item(k_NameOfCol_4_InJobDataSetTable0).ToString
                g_oJob_DMC_NewPayment.mInitializeParmArrays(g_oJob_DMC_NewPayment.iJob_Parm_Array_Length)
                LoadTheParmArrayForTheJob(g_oJob_DMC_NewPayment.iJob_Parm_Array_Length, g_oJob_DMC_NewPayment, tX, iRowOfJobTable)
            End With

            iRowOfJobTable = iRowOfJobTable + 1
            With tX.Rows(iRowOfJobTable)
                '   AAA1 "DMC - New Payment" 
                g_oJob_PV_ProcessedComment.iJob_IDNo = CInt(.Item(k_NameOfCol_1_InJobDataSetTable0).ToString)
                g_oJob_PV_ProcessedComment.sJob_Name = .Item(k_NameOfCol_2_InJobDataSetTable0).ToString
                g_oJob_PV_ProcessedComment.iJob_Parm_Array_Length = CInt(.Item(k_NameOfCol_3_InJobDataSetTable0).ToString)
                g_oJob_PV_ProcessedComment.sJob_ReadMe = .Item(k_NameOfCol_4_InJobDataSetTable0).ToString
                g_oJob_PV_ProcessedComment.mInitializeParmArrays(g_oJob_PV_ProcessedComment.iJob_Parm_Array_Length)
                LoadTheParmArrayForTheJob(g_oJob_PV_ProcessedComment.iJob_Parm_Array_Length, g_oJob_PV_ProcessedComment, tX, iRowOfJobTable)
            End With

    End Sub

    Sub LoadTheParmArrayForTheJob(ByVal iJobParmArrayLength As Integer, ByRef oJob As c_gigVistaJobs, ByRef xT As DataTable, ByVal iRow As Integer)
        'Dim arrX(iJobParmArrayLength) As String
        Dim sX As String

        oJob.mInitializeParmArrays(iJobParmArrayLength)
        For iParmCol = 1 To iJobParmArrayLength
            sX = xT.Rows(iRow).Item(iParmCol + 3).ToString
            'arrX(iParmCol) = sX
            oJob.mAssignAValueTo_ParmNameArray(iParmCol, sX)
        Next


    End Sub


    Sub Instantiate_TheFacilities_FromINI()
            Dim sPathFile As String = g_sAppPath &amp; g_sINIFileName
            Dim dsX As DataSet = c_gigExcelAutomator.fnReadExcel_PassBackDataSet(sPathFile, k_NameOfFirstTableInSettingsDataSet, k_NameOfFirstSheetInINIFile)
            Dim tX As DataTable = dsX.Tables(k_NameOfFirstTableInSettingsDataSet)

            'c_gigDataSetTools.DebugPrintTableInfoForTheDataSet(dsX)

            With tX.Rows(0)
                g_oFacility_Asheville.iFacilityIDNo = CInt(.Item(k_NameOfCol_1_InFacilityDataSetTable0).ToString)
                g_oFacility_Asheville.sFacilityName = .Item(k_NameOfCol_2_InFacilityDataSetTable0).ToString
                g_oFacility_Asheville.sVISTA_Host = .Item(k_NameOfCol_3_InFacilityDataSetTable0).ToString
                g_oFacility_Asheville.sFacility_STN_number = .Item(k_NameOfCol_4_InFacilityDataSetTable0).ToString
                g_oFacility_Asheville.sAccessCode = .Item(k_NameOfCol_5_InFacilityDataSetTable0).ToString
                g_oFacility_Asheville.sVerifyCode = .Item(k_NameOfCol_6_InFacilityDataSetTable0).ToString
            End With

            With tX.Rows(1)
                g_oFacility_Atlanta.iFacilityIDNo = CInt(.Item(k_NameOfCol_1_InFacilityDataSetTable0).ToString)
                g_oFacility_Atlanta.sFacilityName = .Item(k_NameOfCol_2_InFacilityDataSetTable0).ToString
                g_oFacility_Atlanta.sVISTA_Host = .Item(k_NameOfCol_3_InFacilityDataSetTable0).ToString
                g_oFacility_Atlanta.sFacility_STN_number = .Item(k_NameOfCol_4_InFacilityDataSetTable0).ToString
                g_oFacility_Atlanta.sAccessCode = .Item(k_NameOfCol_5_InFacilityDataSetTable0).ToString
                g_oFacility_Atlanta.sVerifyCode = .Item(k_NameOfCol_6_InFacilityDataSetTable0).ToString
            End With

            With tX.Rows(2)
                g_oFacility_Augusta.iFacilityIDNo = CInt(.Item(k_NameOfCol_1_InFacilityDataSetTable0).ToString)
                g_oFacility_Augusta.sFacilityName = .Item(k_NameOfCol_2_InFacilityDataSetTable0).ToString
                g_oFacility_Augusta.sVISTA_Host = .Item(k_NameOfCol_3_InFacilityDataSetTable0).ToString
                g_oFacility_Augusta.sFacility_STN_number = .Item(k_NameOfCol_4_InFacilityDataSetTable0).ToString
                g_oFacility_Augusta.sAccessCode = .Item(k_NameOfCol_5_InFacilityDataSetTable0).ToString
                g_oFacility_Augusta.sVerifyCode = .Item(k_NameOfCol_6_InFacilityDataSetTable0).ToString
            End With






    End Sub


    Sub Instantiate_TheDefaultFacilities()


        g_oFacility_Asheville.iFacilityIDNo = 1
        g_oFacility_Asheville.sFacilityName = "Asheville"
        g_oFacility_Asheville.sFacility_STN_number = "637"
        g_oFacility_Asheville.sVISTA_Host = "vista.asheville.med.va.gov"

        g_oFacility_Atlanta.iFacilityIDNo = 2
        g_oFacility_Atlanta.sFacilityName = "Atlanta"
        g_oFacility_Atlanta.sFacility_STN_number = "508"
        g_oFacility_Atlanta.sVISTA_Host = "vista.atlanta.med.va.gov"

        g_oFacility_Augusta.iFacilityIDNo = 3
        g_oFacility_Augusta.sFacilityName = "Augusta"
        g_oFacility_Augusta.sFacility_STN_number = "509"
        g_oFacility_Augusta.sVISTA_Host = "vista.Augusta.med.va.gov"





    End Sub

    Sub Load_TheJobObjects_IntoThe_JobListArray()
        'Note the array is zero based, but I just skipped the 0 position and went to 1
        ' I could use that array position for an array with all the names of the objects in it.
        g_arrJobList(1) = g_oJob_DMC_NewPayment         'AAA1
        g_arrJobList(2) = g_oJob_PV_ProcessedComment    'AAB1
    End Sub

    Sub Instantiate_TheDefaultJobs()
        'AAA1
        g_oJob_DMC_NewPayment.iJob_IDNo = 1
        g_oJob_DMC_NewPayment.sJob_Name = "DMC - New Payment"
        g_oJob_DMC_NewPayment.iJob_Parm_Array_Length = 4
        g_oJob_DMC_NewPayment.sJob_ReadMe = "Job to enter New DMC Payments"
        g_oJob_DMC_NewPayment.mInitializeParmArrays(g_oJob_DMC_NewPayment.iJob_Parm_Array_Length)
        g_oJob_DMC_NewPayment.mAssignAValueTo_ParmNameArray(1, "Veteran Name")
        g_oJob_DMC_NewPayment.mAssignAValueTo_ParmNameArray(2, "SSN (9 integers)")
        g_oJob_DMC_NewPayment.mAssignAValueTo_ParmNameArray(3, "Amount $ (0.00)")
        g_oJob_DMC_NewPayment.mAssignAValueTo_ParmNameArray(4, "UDN Date (mmddyy)")

        'AAB1
        g_oJob_PV_ProcessedComment.iJob_IDNo = 1
        g_oJob_PV_ProcessedComment.sJob_Name = "PV - Proceseed Comment"
        g_oJob_PV_ProcessedComment.iJob_Parm_Array_Length = 4
        g_oJob_PV_ProcessedComment.sJob_ReadMe = "Job to enter Comments for Pubic Vouchers after they are Processed"
        g_oJob_PV_ProcessedComment.mInitializeParmArrays(g_oJob_PV_ProcessedComment.iJob_Parm_Array_Length)
        g_oJob_PV_ProcessedComment.mAssignAValueTo_ParmNameArray(1, "First Party Bill Number")
        g_oJob_PV_ProcessedComment.mAssignAValueTo_ParmNameArray(2, "Public Voucher ID")
        g_oJob_PV_ProcessedComment.mAssignAValueTo_ParmNameArray(3, "User Initials")
        g_oJob_PV_ProcessedComment.mAssignAValueTo_ParmNameArray(4, "Veteran Name")


    End Sub

    Sub Load_TheFacilitieObjects_IntoThe_FacilityArray()
        g_arrFacility(1) = g_oFacility_Asheville
        g_arrFacility(2) = g_oFacility_Atlanta
        g_arrFacility(3) = g_oFacility_Augusta
    End Sub





End Module



'3] mod_Global



Module mod_Global

    'START Constants ==================================

    Friend Const k_DebugModeOn As Boolean = True
    Friend Const k_IfDebugModeIsOn_ThenWriteToTextFile As Boolean = True
    Friend Const k_NameOfDebugLogFile As String = "ADE_DebugLog.txt"
    Friend Const k_DelayInSeconds_WaitingForVISTA As Double = 0.5
    Friend Const k_NumberOfTimesToTryWaitingForStringFromVista As Integer = 100

    Friend Const k_ADE_INI_FileName As String = "MyADESettingsFile"


    '   Facility INI and Table
    Friend Const k_NameOfFirstSheetInINIFile As String = "FacilityInfo"
    Friend Const k_NameOfFirstTableInSettingsDataSet As String = "FacilityInfo_t"
    Friend Const k_NameOfCol_1_InFacilityDataSetTable0 As String = "FacilityIDNo"
    Friend Const k_NameOfCol_2_InFacilityDataSetTable0 As String = "Facility Name"
    Friend Const k_NameOfCol_3_InFacilityDataSetTable0 As String = "VISTA Host Setting"
    Friend Const k_NameOfCol_4_InFacilityDataSetTable0 As String = "STN Number"
    Friend Const k_NameOfCol_5_InFacilityDataSetTable0 As String = "AK"
    Friend Const k_NameOfCol_6_InFacilityDataSetTable0 As String = "VK"


    '   Jobs INI and Table
    Friend Const k_NameOfSecondSheetInINIFile As String = "VISTA_JobList"
    Friend Const k_NameOfSecondTableInSettingsDataSet As String = "VISTA_JobList_t"
    Friend Const k_NameOfCol_1_InJobDataSetTable0 As String = "JobIDNo"
    Friend Const k_NameOfCol_2_InJobDataSetTable0 As String = "Job Name"
    Friend Const k_NameOfCol_3_InJobDataSetTable0 As String = "Job Parm Array Length"
    Friend Const k_NameOfCol_4_InJobDataSetTable0 As String = "Read Me"


    Friend Const k_NumberOfJobsInList As Integer = 2
    Friend Const k_NumberOfFacilitiesInList As Integer = 19

    'END Constants ==================================

    Friend g_bStop_VISTA_Loop As Boolean = False 'False =&gt; keep doing whatever you are doing, True =&gt; STOP waiting for VISTA and leave what you are doing.
    Friend g_bStartingNewDataSet As Boolean = True
    Friend g_iRec As Integer = 0    'Tracks which record of the data the user is on.

    Friend g_sINIFileName As String = "MyADESettingsFile.xlsx"
    Friend g_sAppPath As String = c_gigFileManager.fn_DirectoryOfThisApplication




    Friend g_arrFacility(k_NumberOfFacilitiesInList) As c_gigFacility
    Friend g_iMaxFacility As Integer = UBound(g_arrFacility)

    Friend g_arrJobList(k_NumberOfJobsInList) As c_gigVistaJobs
    Friend g_iMaxJobList As Integer = UBound(g_arrJobList)

    '=====START the Jobs Objects ===========================================================
    Friend g_oJob_DMC_NewPayment As New c_gigVistaJobs
    Friend g_oJob_PV_ProcessedComment As New c_gigVistaJobs




    '=====END the Jobs Objects ===========================================================


    '=====START the Facilities Objects ===========================================================
    Friend g_oFacility_Asheville As New c_gigFacility
    Friend g_oFacility_Atlanta As New c_gigFacility
    Friend g_oFacility_Augusta As New c_gigFacility
    Friend g_oFacility_Baltimore As New c_gigFacility
    Friend g_oFacility_Beckley As New c_gigFacility
    Friend g_oFacility_Birmingham As New c_gigFacility
    Friend g_oFacility_CAVHCS As New c_gigFacility
    Friend g_oFacility_Charleston As New c_gigFacility
    Friend g_oFacility_Columbia As New c_gigFacility
    Friend g_oFacility_Dublin As New c_gigFacility
    Friend g_oFacility_Durham As New c_gigFacility
    Friend g_oFacility_Fayetteville As New c_gigFacility
    Friend g_oFacility_Hampton As New c_gigFacility
    Friend g_oFacility_Martinsburg As New c_gigFacility
    Friend g_oFacility_Richmond As New c_gigFacility
    Friend g_oFacility_Salem As New c_gigFacility
    Friend g_oFacility_Salisbury As New c_gigFacility
    Friend g_oFacility_Tuscaloosa As New c_gigFacility
    Friend g_oFacility_Washington As New c_gigFacility
    '=====END the Facilities Objects ===========================================================



End Module

</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>INSERT</Category>
        <Language>SQLSERVER2K SQL</Language>
        <Public>false</Public>
        <Name>SELECT_INTO_to_create_NEW_Table</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>INSERT</Category>
          <Language>SQLSERVER2K SQL</Language>
          <Public>false</Public>
          <Name>SELECT_INTO_to_create_NEW_Table</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>
https://msdn.microsoft.com/en-us/library/ms188029.aspx

EXAMPLES:
- from a JOIN
- While also creating an Identity Column
- from a remote source


////////Creating a table by specifying columns from multiple sources
/*
The following example creates the table dbo.EmployeeAddresses in the AdventureWorks2012 database 
by selecting seven columns from various employee-related and address-related tables.
*/

SELECT c.FirstName, c.LastName, e.JobTitle, a.AddressLine1, a.City, 
    sp.Name AS [State/Province], a.PostalCode
INTO dbo.EmployeeAddresses
FROM Person.Person AS c
    JOIN HumanResources.Employee AS e 
    ON e.BusinessEntityID = c.BusinessEntityID
    JOIN Person.BusinessEntityAddress AS bea
    ON e.BusinessEntityID = bea.BusinessEntityID
    JOIN Person.Address AS a
    ON bea.AddressID = a.AddressID
    JOIN Person.StateProvince as sp 
    ON sp.StateProvinceID = a.StateProvinceID;
GO



////////////Creating an identity column using the IDENTITY function
/*
The following example uses the IDENTITY function to create an identity column 
in the new table Person.USAddress in the AdventureWorks2012 database. 
This is required because the SELECT statement that defines the table contains a join, 
which causes the IDENTITY property to not transfer to the new table. 
Notice that the seed and increment values specified in the IDENTITY function
 are different from those of the AddressID column in the source table Person.Address.
 */
 
 IF OBJECT_ID ('Person.USAddress') IS NOT NULL
DROP TABLE Person.USAddress;
GO
-- Determine the IDENTITY status of the source column AddressID.
SELECT OBJECT_NAME(object_id) AS TableName, name AS column_name, is_identity, seed_value, increment_value
FROM sys.identity_columns
WHERE name = 'AddressID';

-- Create a new table with columns from the existing table Person.Address. A new IDENTITY
-- column is created by using the IDENTITY function.
SELECT IDENTITY (int, 100, 5) AS AddressID, 
       a.AddressLine1, a.City, b.Name AS State, a.PostalCode
INTO Person.USAddress 
FROM Person.Address AS a
INNER JOIN Person.StateProvince AS b ON a.StateProvinceID = b.StateProvinceID
WHERE b.CountryRegionCode = N'US'; 

-- Verify the IDENTITY status of the AddressID columns in both tables.
SELECT OBJECT_NAME(object_id) AS TableName, name AS column_name, is_identity, seed_value, increment_value
FROM sys.identity_columns
WHERE name = 'AddressID';
 
 
 ///////Creating a table by specifying columns from a remote data source
/*
The following example demonstrates three methods of creating a new table on the local server 
from a remote data source. The example begins by creating a link to the remote data source. 
The linked server name, MyLinkServer,
is then specified in the FROM clause of the first SELECT...INTO statement and in the OPENQUERY function 
of the second SELECT...INTO statement. The third SELECT...INTO statement uses the OPENDATASOURCE function, 
which specifies the remote data source directly instead of using the linked server name.
*/

USE master;
GO
-- Create a link to the remote data source. 
-- Specify a valid server name for @datasrc as 'server_name' or 'server_name\instance_name'.
EXEC sp_addlinkedserver @server = N'MyLinkServer',
    @srvproduct = N' ',
    @provider = N'SQLNCLI', 
    @datasrc = N'server_name',
    @catalog = N'AdventureWorks2012';
GO
USE AdventureWorks2012;
GO
-- Specify the remote data source in the FROM clause using a four-part name 
-- in the form linked_server.catalog.schema.object.
SELECT DepartmentID, Name, GroupName, ModifiedDate
INTO dbo.Departments
FROM MyLinkServer.AdventureWorks2012.HumanResources.Department
GO
-- Use the OPENQUERY function to access the remote data source.
SELECT DepartmentID, Name, GroupName, ModifiedDate
INTO dbo.DepartmentsUsingOpenQuery
FROM OPENQUERY(MyLinkServer, 'SELECT *
               FROM AdventureWorks2012.HumanResources.Department'); 
GO
-- Use the OPENDATASOURCE function to specify the remote data source.
-- Specify a valid server name for Data Source using the format server_name or server_name\instance_name.
SELECT DepartmentID, Name, GroupName, ModifiedDate
INTO dbo.DepartmentsUsingOpenDataSource
FROM OPENDATASOURCE('SQLNCLI',
    'Data Source=server_name;Integrated Security=SSPI')
    .AdventureWorks2012.HumanResources.Department;
GO</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>LINQ_To_SQL</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>01_Binding_WinForm__ObjectSource.cs</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>LINQ_To_SQL</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>01_Binding_WinForm__ObjectSource.cs</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>01_Binding_WinForm__ObjectSource.cs


using System.Collections.Generic;
using System.Linq;

namespace Data
{
    public class ObjectSource : ISource
    {
        private List&lt;Category&gt; _categories;
        private List&lt;Product&gt; _products;

        public ObjectSource()
        {
            _categories = new List&lt;Category&gt;();
            _categories.Add(new Category(1, "Beverages"));
            _categories.Add(new Category(2, "Condiments"));
            _categories.Add(new Category(3, "Confections"));
            _categories.Add(new Category(4, "Dairy Products"));
            _categories.Add(new Category(5, "Grains/Cereals"));
            _categories.Add(new Category(6, "Meat/Poultry"));
            _categories.Add(new Category(7, "Produce"));
            _categories.Add(new Category(8, "Seafood"));

            _products = new List&lt;Product&gt;();
            _products.Add(new Product(1, "Chai", 1, "10 boxes x 20 bags", 18.0m, 39, 0, false));
            _products.Add(new Product(2, "Chang", 1, "24 - 12 oz bottles", 19.0m, 17, 40, false));
            _products.Add(new Product(3, "Aniseed Syrup", 2, "12 - 550 ml bottles", 10.0m, 13, 70, false));
            _products.Add(new Product(4, "Chef Anton's Cajun Seasoning", 2, "48 - 6 oz jars", 22.0m, 53, 0, false));
            _products.Add(new Product(5, "Chef Anton's Gumbo Mix", 2, "36 boxes", 21.35m, 0, 0, true));
            _products.Add(new Product(6, "Grandma's Boysenberry Spread", 2, "12 - 8 oz jars", 25.0m, 120, 0, false));
            _products.Add(new Product(7, "Uncle Bob's Organic Dried Pears", 7, "12 - 1 lb pkgs.", 30.0m, 15, 0, false));
            _products.Add(new Product(8, "Northwoods Cranberry Sauce", 2, "12 - 12 oz jars", 40.0m, 6, 0, false));
            _products.Add(new Product(9, "Mishi Kobe Niku", 6, "18 - 500 g pkgs.", 97.0m, 29, 0, true));
            _products.Add(new Product(10, "Ikura", 8, "12 - 200 ml jars", 31.0m, 31, 0, false));
            _products.Add(new Product(11, "Queso Cabrales", 4, "1 kg pkg.", 21.0m, 22, 30, false));
            _products.Add(new Product(12, "Queso Manchego La Pastora", 4, "10 - 500 g pkgs.", 38.0m, 86, 0, false));
            _products.Add(new Product(13, "Konbu", 8, "2 kg box", 6.0m, 24, 0, false));
            _products.Add(new Product(14, "Tofu", 7, "40 - 100 g pkgs.", 23.25m, 35, 0, false));
            _products.Add(new Product(15, "Genen Shouyu", 2, "24 - 250 ml bottles", 15.5m, 39, 0, false));
            _products.Add(new Product(16, "Pavlova", 3, "32 - 500 g boxes", 17.45m, 29, 0, false));
            _products.Add(new Product(17, "Alice Mutton", 6, "20 - 1 kg tins", 39.0m, 0, 0, true));
            _products.Add(new Product(18, "Carnarvon Tigers", 8, "16 kg pkg.", 62.5m, 42, 0, false));
            _products.Add(new Product(19, "Teatime Chocolate Biscuits", 3, "10 boxes x 12 pieces", 9.2m, 25, 0, false));
            _products.Add(new Product(20, "Sir Rodney's Marmalade", 3, "30 gift boxes", 81.0m, 40, 0, false));
            _products.Add(new Product(21, "Sir Rodney's Scones", 3, "24 pkgs. x 4 pieces", 10.0m, 3, 40, false));
            _products.Add(new Product(22, "Gustaf's Knäckebröd", 5, "24 - 500 g pkgs.", 21.0m, 104, 0, false));
            _products.Add(new Product(23, "Tunnbröd", 5, "12 - 250 g pkgs.", 9.0m, 61, 0, false));
            _products.Add(new Product(24, "Guaraná Fantástica", 1, "12 - 355 ml cans", 4.5m, 20, 0, true));
            _products.Add(new Product(25, "NuNuCa Nuß-Nougat-Creme", 3, "20 - 450 g glasses", 14.0m, 76, 0, false));
            _products.Add(new Product(26, "Gumbär Gummibärchen", 3, "100 - 250 g bags", 31.23m, 15, 0, false));
            _products.Add(new Product(27, "Schoggi Schokolade", 3, "100 - 100 g pieces", 43.9m, 49, 0, false));
            _products.Add(new Product(28, "Rössle Sauerkraut", 7, "25 - 825 g cans", 45.6m, 26, 0, true));
            _products.Add(new Product(29, "Thüringer Rostbratwurst", 6, "50 bags x 30 sausgs.", 123.79m, 0, 0, true));
            _products.Add(new Product(30, "Nord-Ost Matjeshering", 8, "10 - 200 g glasses", 25.89m, 10, 0, false));
            _products.Add(new Product(31, "Gorgonzola Telino", 4, "12 - 100 g pkgs", 12.5m, 0, 70, false));
            _products.Add(new Product(32, "Mascarpone Fabioli", 4, "24 - 200 g pkgs.", 32.0m, 9, 40, false));
            _products.Add(new Product(33, "Geitost", 4, "500 g", 2.5m, 112, 0, false));
            _products.Add(new Product(34, "Sasquatch Ale", 1, "24 - 12 oz bottles", 14.0m, 111, 0, false));
            _products.Add(new Product(35, "Steeleye Stout", 1, "24 - 12 oz bottles", 18.0m, 20, 0, false));
            _products.Add(new Product(36, "Inlagd Sill", 8, "24 - 250 g  jars", 19.0m, 112, 0, false));
            _products.Add(new Product(37, "Gravad lax", 8, "12 - 500 g pkgs.", 26.0m, 11, 50, false));
            _products.Add(new Product(38, "Côte de Blaye", 1, "12 - 75 cl bottles", 263.5m, 17, 0, false));
            _products.Add(new Product(39, "Chartreuse verte", 1, "750 cc per bottle", 18.0m, 69, 0, false));
            _products.Add(new Product(40, "Boston Crab Meat", 8, "24 - 4 oz tins", 18.4m, 123, 0, false));
            _products.Add(new Product(41, "Jack's New England Clam Chowder", 8, "12 - 12 oz cans", 9.65m, 85, 0, false));
            _products.Add(new Product(42, "Singaporean Hokkien Fried Mee", 5, "32 - 1 kg pkgs.", 14.0m, 26, 0, true));
            _products.Add(new Product(43, "Ipoh Coffee", 1, "16 - 500 g tins", 46.0m, 17, 10, false));
            _products.Add(new Product(44, "Gula Malacca", 2, "20 - 2 kg bags", 19.45m, 27, 0, false));
            _products.Add(new Product(45, "Rogede sild", 8, "1k pkg.", 9.5m, 5, 70, false));
            _products.Add(new Product(46, "Spegesild", 8, "4 - 450 g glasses", 12.0m, 95, 0, false));
            _products.Add(new Product(47, "Zaanse koeken", 3, "10 - 4 oz boxes", 9.5m, 36, 0, false));
            _products.Add(new Product(48, "Chocolade", 3, "10 pkgs.", 12.75m, 15, 70, false));
            _products.Add(new Product(49, "Maxilaku", 3, "24 - 50 g pkgs.", 20.0m, 10, 60, false));
            _products.Add(new Product(50, "Valkoinen suklaa", 3, "12 - 100 g bars", 16.25m, 65, 0, false));
            _products.Add(new Product(51, "Manjimup Dried Apples", 7, "50 - 300 g pkgs.", 53.0m, 20, 0, false));
            _products.Add(new Product(52, "Filo Mix", 5, "16 - 2 kg boxes", 7.0m, 38, 0, false));
            _products.Add(new Product(53, "Perth Pasties", 6, "48 pieces", 32.8m, 0, 0, true));
            _products.Add(new Product(54, "Tourtière", 6, "16 pies", 7.45m, 21, 0, false));
            _products.Add(new Product(55, "Pâté chinois", 6, "24 boxes x 2 pies", 24.0m, 115, 0, false));
            _products.Add(new Product(56, "Gnocchi di nonna Alice", 5, "24 - 250 g pkgs.", 38.0m, 21, 10, false));
            _products.Add(new Product(57, "Ravioli Angelo", 5, "24 - 250 g pkgs.", 19.5m, 36, 0, false));
            _products.Add(new Product(58, "Escargots de Bourgogne", 8, "24 pieces", 13.25m, 62, 0, false));
            _products.Add(new Product(59, "Raclette Courdavault", 4, "5 kg pkg.", 55.0m, 79, 0, false));
            _products.Add(new Product(60, "Camembert Pierrot", 4, "15 - 300 g rounds", 34.0m, 19, 0, false));
            _products.Add(new Product(61, "Sirop d'érable", 2, "24 - 500 ml bottles", 28.5m, 113, 0, false));
            _products.Add(new Product(62, "Tarte au sucre", 3, "48 pies", 49.3m, 17, 0, false));
            _products.Add(new Product(63, "Vegie-spread", 2, "15 - 625 g jars", 43.9m, 24, 0, false));
            _products.Add(new Product(64, "Wimmers gute Semmelknödel", 5, "20 bags x 4 pieces", 33.25m, 22, 80, false));
            _products.Add(new Product(65, "Louisiana Fiery Hot Pepper Sauce", 2, "32 - 8 oz bottles", 21.05m, 76, 0, false));
            _products.Add(new Product(66, "Louisiana Hot Spiced Okra", 2, "24 - 8 oz jars", 17.0m, 4, 100, false));
            _products.Add(new Product(67, "Laughing Lumberjack Lager", 1, "24 - 12 oz bottles", 14.0m, 52, 0, false));
            _products.Add(new Product(68, "Scottish Longbreads", 3, "10 boxes x 8 pieces", 12.5m, 6, 10, false));
            _products.Add(new Product(69, "Gudbrandsdalsost", 4, "10 kg pkg.", 36.0m, 26, 0, false));
            _products.Add(new Product(70, "Outback Lager", 1, "24 - 355 ml bottles", 15.0m, 15, 10, false));
            _products.Add(new Product(71, "Flotemysost", 4, "10 - 500 g pkgs.", 21.5m, 26, 0, false));
            _products.Add(new Product(72, "Mozzarella di Giovanni", 4, "24 - 200 g pkgs.", 34.8m, 14, 0, false));
            _products.Add(new Product(73, "Röd Kaviar", 8, "24 - 150 g jars", 15.0m, 101, 0, false));
            _products.Add(new Product(74, "Longlife Tofu", 7, "5 kg pkg.", 10.0m, 4, 20, false));
            _products.Add(new Product(75, "Rhönbräu Klosterbier", 1, "24 - 0.5 l bottles", 7.75m, 125, 0, false));
            _products.Add(new Product(76, "Lakkalikööri", 1, "500 ml", 18.0m, 57, 0, false));
            _products.Add(new Product(77, "Original Frankfurter grüne Soße", 2, "12 boxes", 13.0m, 32, 0, false));
        }

        public object GetCategories()
        {
            return _categories;
        }

        public object GetProducts(int categoryId)
        {
            var result = from p in _products
                         where p.CategoryId == categoryId
                         select p;

            return result.ToList();
        }

        public bool DeleteProduct(int productId)
        {
            var query = from p in _products
                        where p.ProductID == productId
                        select p;
            var product = query.Single();

            _products.Remove(product);

            return true;
        }

        public bool AddProduct(Product product)
        {
            var maxId = (
                from p in _products
                select p).Max(p =&gt; p.ProductID);
            product.ProductID = maxId + 1;

            _products.Add(product);

            return true;
        }
        public void Save()
        {
            //Nothing to do
        }
    }
}
</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>MVC_Aggregation</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>01_Pulling_values_from_related_Entities__/DAL/ProjectInitializer.cs</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>MVC_Aggregation</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>01_Pulling_values_from_related_Entities__/DAL/ProjectInitializer.cs</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>//01_Pulling_values_from_related_Entities__/DAL/ProjectInitializer.cs

using System;
using System.Collections.Generic;
using System.Data.Entity;
using System.Data.Entity.Validation;
using System.Diagnostics;
using System.IO;
using System.Text;
using WA007_MVC_MileageReimbursement.Models;

namespace WA007_MVC_MileageReimbursement.DAL
	{
	/*
	 * To tell Entity Framework to use your initializer class, 
	 *  add an element to the entityFramework element in the application Web.config file (the one in the root project folder), 
	 *  as shown in the following example:
	 * &lt;entityFramework&gt;
			&lt;contexts&gt;
			  &lt;context type="DropDown.DAL.ProjectContext, DropDown"&gt;
				&lt;databaseInitializer type="DropDown.DAL.ProjectInitializer, DropDown" /&gt;
			  &lt;/context&gt;
			&lt;/contexts&gt;
	 * &lt;/entityFramework&gt;
	 * (When you don't want EF to use the initializer, you can set an attribute on the context element: disableDatabaseInitialization="true".)
	 * 
	 * As an alternative to setting the initializer in the Web.config file is to do it in code by adding a Database.SetInitializer statement to the Application_Start method in in the Global.asax.cs file.
	 * 
	 * The application is now set up so that when you access the database for the first time in a given run of the application, 
	 *      the Entity Framework compares the database to the model (ProjectContext and entity classes). 
	 *      If there's a difference, the application drops and re-creates the database.
	*/
		//public class ProjectInitializer : CreateDatabaseIfNotExists&lt;ProjectContext&gt;
	//public class ProjectInitializer : DropCreateDatabaseIfModelChanges&lt;ProjectContext&gt;
	//public class ProjectInitializer : DropCreateDatabaseAlways&lt;ProjectContext&gt;
	public class ProjectInitializer : CreateDatabaseIfNotExists&lt;ProjectContext&gt;
		{

		protected override void Seed(ProjectContext context)
			{
			//base.Seed(context);


			////======================================================================================================================

			// Remove any constraints that might cause problems during the Delete phase
			foreach (var file in Directory.GetFiles(Path.Combine(AppDomain.CurrentDomain.BaseDirectory, "Sql\\Before"), "*.sql"))
				{
				context.Database.ExecuteSqlCommand(File.ReadAllText(file), new object[0]);
				}

			// Delete all stored procs, views
			foreach (var file in Directory.GetFiles(Path.Combine(AppDomain.CurrentDomain.BaseDirectory, "Sql\\Delete"), "*.sql"))
				{
				context.Database.ExecuteSqlCommand(File.ReadAllText(file), new object[0]);
				}

			// Add Stored Procedures, views
			foreach (var file in Directory.GetFiles(Path.Combine(AppDomain.CurrentDomain.BaseDirectory, "Sql\\Create"), "*.sql"))
				{
				context.Database.ExecuteSqlCommand(File.ReadAllText(file), new object[0]);
				}

			// Add any constraints that were deleted during the Before phase
			//foreach (var file in Directory.GetFiles(Path.Combine(AppDomain.CurrentDomain.BaseDirectory, "Sql\\After"), "*.sql"))
			//{
			//    context.Database.ExecuteSqlCommand(File.ReadAllText(file), new object[0]);
			//}

			////======================================================================================================================
			SaveChanges(context);
			var fundingProjects = new List&lt;FundingProject&gt;
				{
					new FundingProject { FundingProjectAcronym = "n/a"},
					new FundingProject { FundingProjectAcronym = "ABCD"},
					new FundingProject { FundingProjectAcronym = "BCMC"},
					new FundingProject { FundingProjectAcronym = "CHIPRA"},
					new FundingProject { FundingProjectAcronym = "CHACC"},
					new FundingProject { FundingProjectAcronym = "CS-HCDPH"},
					new FundingProject { FundingProjectAcronym = "CS-PCMH-Rainbow"},
					new FundingProject { FundingProjectAcronym = "FHC"},
					new FundingProject { FundingProjectAcronym = "HCDPH"},
					new FundingProject { FundingProjectAcronym = "IMPACT"},
					new FundingProject { FundingProjectAcronym = "OFD"},					
					new FundingProject { FundingProjectAcronym = "SMC"},				
				};
			fundingProjects.ForEach(s =&gt; context.FundingProjects.Add(s));
			//context.SaveChanges();	// This is the default
			SaveChanges(context);	// This is to use my method


			}

		/// &lt;summary&gt;
		/// Wrapper for SaveChanges adding the Validation Messages to the generated exception
		/// &lt;/summary&gt;
		/// &lt;param name="context"&gt;The context.&lt;/param&gt;
		/// From: http://stackoverflow.com/questions/10219864/ef-code-first-how-do-i-see-entityvalidationerrors-property-from-the-nuget-pac
		private static void SaveChanges(DbContext context)
			{
			try
				{
				context.SaveChanges();
				}
			catch (DbEntityValidationException ex)
				{
				StringBuilder sb = new StringBuilder();

				foreach (var failure in ex.EntityValidationErrors)
					{
					sb.AppendFormat("{0} failed validation\n", failure.Entry.Entity.GetType());
					foreach (var error in failure.ValidationErrors)
						{
						sb.AppendFormat("- {0} : {1}", error.PropertyName, error.ErrorMessage);
						sb.AppendLine();
						}
					}

				throw new DbEntityValidationException(
					"Entity Validation Failed - errors follow:\n" +
					sb.ToString(), ex
					); // Add the original exception as the innerException
				}
			catch (Exception ex)
				{
				Debug.Print("SaveChanges Exception.Message" + ex.Message);
				Debug.Print("SaveChanges Exception.InnerException" + ex.InnerException);

				}
			}



		}
	}
</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>MVC_EntityFramework</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>9_SeedDatabase__about</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>MVC_EntityFramework</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>9_SeedDatabase__about</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>/*9_SeedDatabase__about


This will use:
-------------
a) Migration 
b) Configuration.cs file to seed the database with 1000 new restaurants:

Then go to the 'Package Manager Console' and submit the following command: 
*/
Update-Database
/*</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>MVC_EntityFramework</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>9_SeedDatabase__Configuration.cs</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>MVC_EntityFramework</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>9_SeedDatabase__Configuration.cs</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>//9_SeedDatabase_Configuration.cs


namespace OdeToFood.Migrations
{
    using System;
    using System.Data.Entity;
    using System.Data.Entity.Migrations;
    using System.Linq;
    using OdeToFood.Models;     //required for the seed data
    using System.Collections.Generic;     //required for the seed data


    internal sealed class Configuration : DbMigrationsConfiguration&lt;OdeToFood.Models.OdeToFoodDb&gt;
    {
        public Configuration()
        {
            AutomaticMigrationsEnabled = false;
        }

        protected override void Seed(OdeToFood.Models.OdeToFoodDb context)
        {
            context.Restaurants.AddOrUpdate(r =&gt; r.Name,
               new Restaurant { Name = "Sabatino's", City = "Baltimore", Country = "USA" },
               new Restaurant { Name = "Great Lake", City = "Chicago", Country = "USA" },
               new Restaurant
               {
                   Name = "Smaka",
                   City = "Gothenburg",
                   Country = "Sweden",
                   Reviews =
                       new List&lt;RestaurantReview&gt; { 
                       new RestaurantReview { Rating = 9, Body="Great food!", ReviewerName="Scott" }
                   }
               });

            for (int i = 0; i &lt; 1000; i++)
            {
                context.Restaurants.AddOrUpdate(r =&gt; r.Name,
                    new Restaurant { Name = i.ToString(), City = "Nowhere", Country = "USA" });
            }
        }
    }
}
</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>MVC_Header-Detail</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>01_MinimalWorkingExample__/DAL/ProjectInitializer</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>MVC_Header-Detail</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>01_MinimalWorkingExample__/DAL/ProjectInitializer</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>//01_MinimalWorkingExample__/DAL/ProjectInitializer

using System;
using System.Collections.Generic;
using System.Data.Entity;
using System.Data.Entity.Migrations;
using System.Data.Entity.Validation;
using System.Text;
using globalCommon;
using prj_0043_subprj_07.Models;

namespace prj_0043_subprj_07.DAL
{


    /*
     * To tell Entity Framework to use your initializer class, 
     *  add an element to the entityFramework element in the application Web.config file (the one in the root project folder), 
     *  as shown in the following example:
     * &lt;entityFramework&gt;
            &lt;contexts&gt;
              &lt;context type="DropDown.DAL.ProjectContext, DropDown"&gt;
                &lt;databaseInitializer type="DropDown.DAL.ProjectInitializer, DropDown" /&gt;
              &lt;/context&gt;
            &lt;/contexts&gt;
     * &lt;/entityFramework&gt;
     * (When you don't want EF to use the initializer, you can set an attribute on the context element: disableDatabaseInitialization="true".)
     * 
     * As an alternative to setting the initializer in the Web.config file is to do it in code by adding a Database.SetInitializer statement to the Application_Start method in in the Global.asax.cs file.
     * 
     * The application is now set up so that when you access the database for the first time in a given run of the application, 
     *      the Entity Framework compares the database to the model (ProjectContext and entity classes). 
     *      If there's a difference, the application drops and re-creates the database.
    */

    public class ProjectInitializer : DropCreateDatabaseIfModelChanges&lt;ProjectContext&gt;
    //public class ProjectInitializer : System.Data.Entity.DropCreateDatabaseAlways&lt;ProjectContext&gt;
    {

        protected override void Seed(ProjectContext context)
        {
  
		}

        /// &lt;summary&gt;
        /// Wrapper for SaveChanges adding the Validation Messages to the generated exception
        /// &lt;/summary&gt;
        /// &lt;param name="context"&gt;The context.&lt;/param&gt;
        private static void SaveChanges(DbContext context)
        {
            try
            {
                context.SaveChanges();
            }
            catch (DbEntityValidationException ex)
            {
                StringBuilder sb = new StringBuilder();

                foreach (var failure in ex.EntityValidationErrors)
                {
                    sb.AppendFormat("{0} failed validation\n", failure.Entry.Entity.GetType());
                    foreach (var error in failure.ValidationErrors)
                    {
                        sb.AppendFormat("- {0} : {1}", error.PropertyName, error.ErrorMessage);
                        sb.AppendLine();
                    }
                }

                throw new DbEntityValidationException(
                    "Entity Validation Failed - errors follow:\n" +
                    sb.ToString(), ex
                ); // Add the original exception as the innerException
            }
        }



    }
}
</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>MVC_Initializer</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>about_MVC_Initializer</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>MVC_Initializer</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>about_MVC_Initializer</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>about_MVC_Initializer_Seed_Application_Start

//base for DataBaseInitializer, which is used by 'SetInitializer' in Global.asax/Application_Start

/*
Regarding the Comment:
======================
My Seed method was not invoked even with proper call to Database.SetInitializer in Application_Start... 
Response

=========
The reason for it was really simple: initializer may not be invoked at all if you 
don't yet have any code that actually uses database context.

	
from:	http://stackoverflow.com/questions/6312336/how-can-i-get-my-database-to-seed-using-entity-framework-codefirst

First, lessons learned:
----------------------
The seed method won't be called until the context is used.
The Global.asax.cs won't hit a breakpoint on first run bc it runs before the debugger is attached. To hit a breakpoint on Global.asax.cs, 
	you have can add some white space to Web.config and hit a page; then it will get hit.
If there are VS connections to the db, the seeding won't happen. The app will throw an error.

So, to avoid the sadness:
----------------------------
Disconnect your VS connection.
Switch the base class DropCreateDatabaseAlways for one go.
Hit a page that uses the context.

Now, the sadness:
------------------
I had my custom Initializer class in my Global.asax.cs file. I had a break point on my Initializer Seed method; I started the application and the method never got hit. :(
I point a break point in my Database.SetInitializer call in Application_Start. That never got hit. :(
I realized that I had no db schema changes, so then I changed DropCreateDatabaseIfModelChanges to DropCreateDatabaseAlways. Still, nothing. :(
I finally went to a page that uses the context, and it worked. :/

notes from others:
- I closed the database connection and then switched the base classto DropCreateDatabaseAlways. It worked well 
-the Global only runs the first time. So close IISExpress. 
	Set your break point in global and run. It will hit it, once, because its IIS application level.
</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>MVC_Initializer</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>ApplicationStart_SetInitializer</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>MVC_Initializer</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>ApplicationStart_SetInitializer</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>One user reported that

located in Global.asax

/*
The following change in the Global.asax file worked for me - to invoke 'Seed':

Old Code:
*/

    protected void Application_Start()
    {
        Database.SetInitializer&lt;mycontextclassname&gt;(new DropCreateDatabaseAlways&lt;mycontextclassname&gt;());             
       ...
    }

//New Code:

    protected void Application_Start()
    {
        Database.SetInitializer&lt;mycontextclassname&gt;(new DropCreateDatabaseAlways&lt;mycontextclassname&gt;()); 
        Database.SetInitializer(new Initializer()); 
        ...
    }


</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>MVC_Initializer</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>Firing_Initializer_fromWithin_DbContext</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>MVC_Initializer</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>Firing_Initializer_fromWithin_DbContext</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>Firing an Entity Framework Database Initializer from within DbContext
ref:	http://weblog.west-wind.com/posts/2013/Mar/26/Firing-an-Entity-Framework-Database-Initializer-from-within-DbContext

/*
Initializers can override the Initialize() and Seed methods(). 

The default EF behavior compares the model to any meta data in the database and if it's not valid, throws a failed meta data exception.

Database initializers initialize the database and are intended to run exactly once per AppDomain. 
	This means they need to be run on application startup - 
	either in a desktop apps startup code or in Application_Start in Web application. 
	
For example to use an initializer in ASP.NET, you use syntax like the following:
*/
protected void Application_Start()
{
    Database.SetInitializer&lt;ClassifiedsContext&gt;(new DropCreateDatabaseIfModelChanges());
}
 
/*
It's unlikely that you'll use one of the default database initializers in a live application, 
	since they are pretty destructive but they can be useful for testing or during development when the database changes a lot. 
The more common scenario like is to use the MIGRATION initializer:
*/											--------

Database.SetInitializer&lt;ClassifiedsContext&gt;(
     new MigrateDatabaseToLatestVersion&lt;ClassifiedsContext,MigrationConfiguration&gt;());
     
			/*
			I tend to subclass this mouthful with my own class to make the initializer a little more approachable.
			*/
			public class ClassifiedsModelMigrateInitializer :
			    MigrateDatabaseToLatestVersion&lt;ClassifiedsContext,ClassifiedsBusiness.Migrations.MigrationConfiguration&gt;        
			{
			} 
			
-----------------------------------------------------------------------------------			
/* Intitializer:

The Initialize method is responsible for checking the context 
	and perform any additional initialization you might have to do. 
The default implementation checks the metadata in the database (if it exists) against the model. 
If you don't want that to happen  you can simply implement an empty initializer like so:
*/
public class QueueMessageManagerContextInitializer : IDatabaseInitializer&lt;QueueMessageManagerContext&gt;
{
    protected void Seed(QueueMessageManagerContext context)
    {
    }

    public void InitializeDatabase(QueueMessageManagerContext context)
    {
        // do nothing
        Seed(context);
    }
}

//which can then be used like this:

Database.SetInitializer&lt;QueueMessageManagerContext&gt;(new QueueMessageManagerContextInitializer());

//Turns out there's actually an even simpler way to have a non-actionable database Initializer - simply pass null:

Database.SetInitializer&lt;QueueMessageManagerContext&gt;(null);

-----------------------------------------------------------------------------------
/*
It seems to me that the initializer invocation is not an application responsibility 
	but the responsibility of the context 
	and that's where I would expect that behavior to live.

So I got to thinking - wouldn't it be nice to actually make the initialization more generic 
	so that it can be called from anywhere and still be guaranteed to always fire just once.

Here's a DbContext utility method that does just that:
*/
public static class DbContextUtils&lt;TContext&gt;
    where TContext : DbContext
{
    static object _InitializeLock = new object();
    static bool _InitializeLoaded = false;

    /// &lt;summary&gt;
    /// Method to allow running a DatabaseInitializer exactly once
    /// &lt;/summary&gt;   
    /// &lt;param name="initializer"&gt;A Database Initializer to run&lt;/param&gt;
    public static void SetInitializer(IDatabaseInitializer&lt;TContext&gt; initializer = null)
            
    {            
        if (_InitializeLoaded)
            return;

        // watch race condition
        lock (_InitializeLock)
        {
            // are we sure?
            if (_InitializeLoaded)                
                return;

            _InitializeLoaded = true;

            // force Initializer to load only once
            System.Data.Entity.Database.SetInitializer&lt;TContext&gt;(initializer);
        }
    }
}

/*
all this code really does is check to see if this code was previously by using a static flag to hold the state. 
If you recall, static properties/fields are global to the AppDomain so this SetInitializer call fires exactly once per AppDomain. 
This code needs to be called before the first full context invocation.

If the goal is to internalize this code as part of the context, 
	it's easy to stick it into the constructor of the DbContext subclass you create. 
Here's an example:
*/
public class QueueMessageManagerContext : DbContext
{
    public QueueMessageManagerContext()
    {            
        // don't validate the schema
        DbContextExtensions&lt;QueueMessageManagerContext&gt;.SetInitializer(null);                        
    }
        
    public DbSet&lt;QueueMessageItem&gt; QueueMessageItems { get; set; }        
}
/*
Now you don't have to worry about when the initializer is called
	because the first access to your context automatically initializes the context using the specified initializer.
This also keeps all behavior relative to the Context in one place, so personally I like this. 
	You can also still use app startup code to call the method directly just like calling SetInitializer directly.

This is a small thing of course, 
	but it's something that's important to me 
	as in my current app I'm working on with a client 
	we have many small self contained components that have micro EF models. 
I can now easily force all of those components 
	to non check the meta data when they start and all can share the same database easily.







</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>MVC_Initializer</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>Pattern_0__/Migrations/Configuration.cs</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>MVC_Initializer</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>Pattern_0__/Migrations/Configuration.cs</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>using System.Collections.Generic;
using System.Data.Entity.Validation;
using System.Diagnostics;
using System.Text;
using Microsoft.Ajax.Utilities;
using Test_01.Models;
using globalCommon;

namespace Test_01.Migrations
	{
	using System;
	using System.Data.Entity;
	using System.Data.Entity.Migrations;
	using System.Linq;

	internal sealed class Configuration : DbMigrationsConfiguration&lt;Test_01.Models.Test_01_DbContext&gt; 
		{
		public Configuration()
			{
			AutomaticMigrationsEnabled = true;
			ContextKey = "Test_01.Models.Test_01_DbContext";
			}

        protected override void Seed(Test_01.Models.Test_01_DbContext context)
        {


            Random rnd = new Random();

            for (int i = 0; i &lt; 10; i++)
            {
                context.Header_Record.AddOrUpdate(h =&gt; h.Header_PK, new Header_R
                                                        {
                                                            Percent_Done_0_00_to_1_00 = (decimal?)(rnd.NextDouble()),
                                                            TheName = rnd.Random_LowerCaseLetters(5),
                                                            PhoneNumber = rnd.Random_Numbers(10),
                                                            Category_0_to_5 = (Int16?)rnd.Next(0, 6),
                                                            bActive = ((short?)rnd.Next(0, 1) == 1),
                                                            Detail =
                                                                new List&lt;Detail_R&gt;
                                                                    {

                                                                        new Detail_R {
                                                                                        Detail_Category_Integer = rnd.Next(1,6),
                                                                                        Detail_Category_nvarchar = rnd.Random_UpperCaseLetters(5),
                                                                                      },
                                                                        new Detail_R {
                                                                                        Detail_Category_Integer = rnd.Next(1,6),
                                                                                        Detail_Category_nvarchar = rnd.Random_UpperCaseLetters(5),
                                                                                      }
                                                                    }
                                                        }
                                                );
            }

            context.Enum_Record.AddOrUpdate(e =&gt; e.Enum_PK,
                new Enum_R { EnumDescription = "unknown", EnumValue = 0, EnumNameSpace_FK = 1 },
                new Enum_R { EnumDescription = "Red", EnumValue = 1, EnumNameSpace_FK = 1 },
                new Enum_R { EnumDescription = "Blue", EnumValue = 2, EnumNameSpace_FK = 1 },
                new Enum_R { EnumDescription = "Green", EnumValue = 3, EnumNameSpace_FK = 1 },
                new Enum_R { EnumDescription = "Indigo", EnumValue = 4, EnumNameSpace_FK = 1 },
                new Enum_R { EnumDescription = "Violet", EnumValue = 5, EnumNameSpace_FK = 1 }
                );

            for (int i = 0; i &lt; 7; i++)
            {
                context.JustHeader_Record.AddOrUpdate(h =&gt; h.JustHeader_PK, new JustHeader_R
                                                        {
                                                            Percent_Done_0_00_to_1_00 = (decimal?)(rnd.NextDouble()),
                                                            TheName = rnd.Random_LowerCaseLetters(5),
                                                            PhoneNumber = rnd.Random_Numbers(10),
                                                            Category_0_to_5 = (Int16?)rnd.Next(0, 6),
                                                            bActive = ((short?)rnd.Next(0, 1) == 1),
                                                        }
                                                );
            }

            SaveChanges(context);	//For debugging: DbEntityValidationException 

        }

        /// &lt;summary&gt;
        /// Wrapper for SaveChanges adding the Validation Messages to the generated exception
        /// &lt;/summary&gt;
        /// &lt;param name="context"&gt;The context.&lt;/param&gt;
        private void SaveChanges(DbContext context)
        {
            try
            {
                context.SaveChanges();
            }
            catch (DbEntityValidationException ex)
            {
                StringBuilder sb = new StringBuilder();

                foreach (var failure in ex.EntityValidationErrors)
                {
                    sb.AppendFormat("{0} failed validation\n", failure.Entry.Entity.GetType());
                    foreach (var error in failure.ValidationErrors)
                    {
                        sb.AppendFormat("- {0} : {1}", error.PropertyName, error.ErrorMessage);
                        sb.AppendLine();
                    }
                }

                throw new DbEntityValidationException(
                    "Entity Validation Failed - errors follow:\n" +
                    sb.ToString(), ex
                ); // Add the original exception as the innerException
            }
        }
		}
	}
</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>MVC_Initializer</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>Pattern_0__Models/myDatabaseInitializer.cs</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>MVC_Initializer</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>Pattern_0__Models/myDatabaseInitializer.cs</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>

using System;
using System.Collections.Generic;
using System.Data.Entity;
using System.Linq;
using System.Web;

namespace Test_01.Models
{
    public class myDatabaseInitializer_CreateDatabaseIfNotExists : CreateDatabaseIfNotExists&lt;Test_01_DbContext&gt;
    {

        protected override void Seed(Test_01_DbContext context)
        {
            context.Seed(context);

            base.Seed(context);
        }
    }

    public class myDatabaseInitializer_DropCreateDatabaseIfModelChanges : DropCreateDatabaseIfModelChanges&lt;Test_01_DbContext&gt;
    {

        protected override void Seed(Test_01_DbContext context)
        {
            context.Seed(context);

            base.Seed(context);
        }
    }

    public class myDatabaseInitializer_DropCreateDatabaseAlways : DropCreateDatabaseAlways&lt;Test_01_DbContext&gt;
    {

        protected override void Seed(Test_01_DbContext context)
        {
            context.Seed(context);

            base.Seed(context);
        }
    }

}</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>MVC_Initializer</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>Pattern_0__Models/Test_01_DbContext.cs</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>MVC_Initializer</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>Pattern_0__Models/Test_01_DbContext.cs</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>//Pattern_0__Models/Test_01_DbContext.cs


using System;
using System.Collections.Generic;
using System.Data.Entity;
using System.Data.Entity.Migrations;
using System.Data.Entity.Validation;
using System.Text;
using globalCommon;
//requred for: DbContext


namespace Test_01.Models
{
    public class Test_01_DbContext : DbContext
    {

        //=======================================================================================
        //Error when right clicking in a controller and creating a view that uses a model:
        //"has already been added" "There was an error running the selected code generator" "A configuration for type" "use the Entity&lt;T&gt;() or Complex&lt;T&gt;() methods" 

        //from: http://stackoverflow.com/questions/24974218/scaffolding-controller-doesnt-work-with-visual-studio-2013-update-3-and-4

        //by changing my DBContext derived class to use IDbSet properties instead of DbSet 
        //I could generate the controllers and views just fine. 
        //However, for me this introduced another issue, IDbSet does not support the async methods.
        //=======================================================================================
        // 'DbSet' type properties are used to represent the entities that you want to query and persist
        public DbSet&lt;JustHeader_R&gt; JustHeader_Record { get; set; }
        public DbSet&lt;Header_R&gt; Header_Record { get; set; }
        public DbSet&lt;Detail_R&gt; Detail_Record { get; set; }
        public DbSet&lt;Enum_R&gt; Enum_Record { get; set; }


        public Test_01_DbContext() //If you don't specify a connection string or the name of one explicitly, Entity Framework assumes that the connection string name is the same as the class name. The default connection string name in this example would then be Test_01_DbContext.
            : base("name=SQLEXPRESS_LOCAL_Connection") 
            
        {
            //Database.SetInitializer(new CreateDatabaseIfNotExists&lt;Test_01_DbContext&gt;());
            Database.SetInitializer(new myDatabaseInitializer_CreateDatabaseIfNotExists());
            //Database.SetInitializer(new DropCreateDatabaseAlways&lt;Test_01_DbContext&gt;());

        }

        public void Seed(Test_01_DbContext context)
        {
            if (Global_BackAndFrontEnd.g_TEST)
            {


            }



            Random rnd = new Random();

            for (int i = 0; i &lt; 10; i++)
            {
                context.Header_Record.AddOrUpdate(h =&gt; h.Header_PK, new Header_R
                {
                    Percent_Done_0_00_to_1_00 = (decimal?)(rnd.NextDouble()),
                    TheName = rnd.Random_LowerCaseLetters(5),
                    PhoneNumber = rnd.Random_Numbers(10),
                    Category_0_to_5 = (Int16?)rnd.Next(0, 6),
                    bActive = ((short?)rnd.Next(0, 1) == 1),
                    Detail =
                        new List&lt;Detail_R&gt;
																{
							
																	new Detail_R {
																					Detail_Category_Integer = rnd.Next(1,6),
																					Detail_Category_nvarchar = rnd.Random_UpperCaseLetters(5),
																				  },
																	new Detail_R {
																					Detail_Category_Integer = rnd.Next(1,6),
																					Detail_Category_nvarchar = rnd.Random_UpperCaseLetters(5),
																				  }
																}
                }
                                                );
            }

            context.Enum_Record.AddOrUpdate(e =&gt; e.Enum_PK,
                new Enum_R { EnumDescription = "unknown", EnumValue = 0, EnumNameSpace_FK = 1 },
                new Enum_R { EnumDescription = "Red", EnumValue = 1, EnumNameSpace_FK = 1 },
                new Enum_R { EnumDescription = "Blue", EnumValue = 2, EnumNameSpace_FK = 1 },
                new Enum_R { EnumDescription = "Green", EnumValue = 3, EnumNameSpace_FK = 1 },
                new Enum_R { EnumDescription = "Indigo", EnumValue = 4, EnumNameSpace_FK = 1 },
                new Enum_R { EnumDescription = "Violet", EnumValue = 5, EnumNameSpace_FK = 1 }
                );

            for (int i = 0; i &lt; 7; i++)
            {
                context.JustHeader_Record.AddOrUpdate(h =&gt; h.JustHeader_PK, new JustHeader_R
                {
                    Percent_Done_0_00_to_1_00 = (decimal?)(rnd.NextDouble()),
                    TheName = rnd.Random_LowerCaseLetters(5),
                    PhoneNumber = rnd.Random_Numbers(10),
                    Category_0_to_5 = (Int16?)rnd.Next(0, 6),
                    bActive = ((short?)rnd.Next(0, 1) == 1),
                }
                                                );
            }

            SaveChanges(context);	//For debugging: DbEntityValidationException 

        }

        /// &lt;summary&gt;
        /// Wrapper for SaveChanges adding the Validation Messages to the generated exception
        /// &lt;/summary&gt;
        /// &lt;param name="context"&gt;The context.&lt;/param&gt;
        private static void SaveChanges(DbContext context)
        {
            try
            {
                context.SaveChanges();
            }
            catch (DbEntityValidationException ex)
            {
                StringBuilder sb = new StringBuilder();

                foreach (var failure in ex.EntityValidationErrors)
                {
                    sb.AppendFormat("{0} failed validation\n", failure.Entry.Entity.GetType());
                    foreach (var error in failure.ValidationErrors)
                    {
                        sb.AppendFormat("- {0} : {1}", error.PropertyName, error.ErrorMessage);
                        sb.AppendLine();
                    }
                }

                throw new DbEntityValidationException(
                    "Entity Validation Failed - errors follow:\n" +
                    sb.ToString(), ex
                ); // Add the original exception as the innerException
            }
        }




        protected override void OnModelCreating(DbModelBuilder modelBuilder)
        {
            modelBuilder.Configurations.Add(new Detail_R_Map());
            modelBuilder.Configurations.Add(new Header_R_Map());
            modelBuilder.Configurations.Add(new JustHeader_R_Map());
            modelBuilder.Configurations.Add(new Enum_R_Map());
        }

    }


}</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>MVC_Initializer</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>Pattern_1</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>MVC_Initializer</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>Pattern_1</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>//from: http://stackoverflow.com/questions/6312336/how-can-i-get-my-database-to-seed-using-entity-framework-codefirst


//Client web.config
--------------------

&lt;connectionStrings&gt;
  &lt;add name="DatabaseContext" connectionString="data source=.\SQLEXPRESS;Database=Database;Integrated Security=SSPI;" providerName="System.Data.SqlClient" /&gt;
&lt;/connectionStrings&gt;

//DatabaseInitializer.cs
--------------------

public class DatabaseInitializer : CreateDatabaseIfNotExists&lt;DatabaseContext&gt;
{
  protected override void Seed(DatabaseContext context)
  {
    // Seed code here
  }
}

//DatabaseContext.cs
--------------------

public class DatabaseContext : DbContext
{
  public DatabaseContext() : base("MyDatabase") { }

  protected override void OnModelCreating(DbModelBuilder mb)
  {
    // Code here
  }

  public DbSet&lt;Entity&gt; Entities { get; set; }
  // Other DbSets
}

//Global.asax.cs - Application_Start()
--------------------

protected void Application_Start()
{
  Database.SetInitializer(new DatabaseInitializer());
  AreaRegistration.RegisterAllAreas();
  RegisterGlobalFilters(GlobalFilters.Filters);
  RegisterRoutes(RouteTable.Routes);
}</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>MVC_Initializer</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>Pattern_2</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>MVC_Initializer</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>Pattern_2</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>//from: http://stackoverflow.com/questions/6312336/how-can-i-get-my-database-to-seed-using-entity-framework-codefirst

//This is what my DbContext classes all look like and they seed just fine:
//This is a great method for doing this because it doesn't require any changes to Application_Start() and therefore re-usable 

public class MyDbContext : DbContext
{
    public DbSet&lt;MyClass&gt; MyClasses { get; set; }

    protected override void OnModelCreating (DbModelBuilder modelBuilder)
    {
        base.OnModelCreating (modelBuilder);
        modelBuilder.Conventions.Remove&lt;System.Data.Entity.ModelConfiguration.Conventions.PluralizingTableNameConvention&gt; ();

        // Add any configuration or mapping stuff here
    }

    public void Seed (MyDbContext Context)
    {
        #if DEBUG
        // Create my debug (testing) objects here
        var TestMyClass = new MyClass () { ... };
        Context.MyClasses.Add (TestMyClass);
        #endif

        // Normal seeding goes here

        Context.SaveChanges ();
    }

    public class DropCreateIfChangeInitializer : DropCreateDatabaseIfModelChanges&lt;MyDbContext&gt;
    {
        protected override void Seed (MyDbContext context)
        {
            context.Seed (context);

            base.Seed (context);
        }
    }

    public class CreateInitializer : CreateDatabaseIfNotExists&lt;MyDbContext&gt;
    {
        protected override void Seed (MyDbContext context)
        {
            context.Seed (context);

            base.Seed (context);
        }
    }

    static MyDbContext ()
    {
        #if DEBUG
        Database.SetInitializer&lt;MyDbContext&gt; (new DropCreateIfChangeInitializer ());
        #else
        Database.SetInitializer&lt;MyDbContext&gt; (new CreateInitializer ());
        #endif
    }
}

//I have used this pattern a few times and it has worked out very well for me.
</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>MVC_Initializer</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>Pattern_3</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>MVC_Initializer</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>Pattern_3</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>http://stackoverflow.com/questions/16542347/database-initialization-strategy-code-first


public class ContextInitializer : CreateDatabaseIfNotExists&lt;Context&gt;
{     
    private static void InitializeWebSecurity()
    {
        if (WebSecurity.Initialized)
            return;

        WebSecurity.InitializeDatabaseConnection(
            connectionStringName: "DefaultConnection",
            userTableName: "User",
            userIdColumn: "Id",
            userNameColumn: "Email",
            autoCreateTables: true);

        Roles.CreateRole("Admin");
        Roles.CreateRole("Customer");
    }       

    protected override void Seed(Context context)
    {
        InitializeWebSecurity();

        // more seeding
        context.SaveChanges();
    }
}

public class MvcApplication : System.Web.HttpApplication
{
    protected void Application_Start()
    {
        AreaRegistration.RegisterAllAreas();
        WebApiConfig.Register(GlobalConfiguration.Configuration);
        FilterConfig.RegisterGlobalFilters(GlobalFilters.Filters);
        RouteConfig.RegisterRoutes(RouteTable.Routes);
        BundleConfig.RegisterBundles(BundleTable.Bundles);
        AuthConfig.RegisterAuth();
        DatabaseConfig.RegisterDatabase();
        AutoMapperConfig.RegisterConfig();

        // Set the initializer here
        Database.SetInitializer(new ContextInitializer());

        // Now initialize it
        using (var context = new Context())
        {
            context.Database.Initialize(false);
        }

        // Double check seeding has initalized, if not
        // We initialize it here to make sure.
        if (!WebSecurity.Initialized)
        {
            WebSecurity.InitializeDatabaseConnection(
                connectionStringName: "DefaultConnection", 
                userTableName: "User", 
                userIdColumn: "Id", 
                userNameColumn: "Email", 
                autoCreateTables: false);
        }
    }
}

//You should be able to create migrations the same way with this setup.
</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>MVC_Initializer</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>Strategies_Overview</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>MVC_Initializer</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>Strategies_Overview</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>//There are four different database Initialization strategies: (from: http://www.entityframeworktutorial.net/code-first/database-initialization-strategy-in-code-first.aspx)

    CreateDatabaseIfNotExists:					/* 
    	This is default initializer. As name suggests, 
    		it will create the database if none exists as per the configuration. 
    	However, if you change the model class and then run the application with this initializer, 
    		then it will throw an exception.	*/
    DropCreateDatabaseIfModelChanges:					/*  
    	This initializer drops an existing database and creates a new database, 
    		if your model classes (entity classes) have been changed. 
    	So you don't have to worry about maintaining your database schema, when your model classes change.	*/
    DropCreateDatabaseAlways:					/* 
    	As the name suggests, 
    		this initializer drops an existing database every time you run the application, 
    		irrespective of whether your model classes have changed or not. 
    	This will be useful, 
    		when you want fresh database, 
    		every time you run the application, 
    		while you are developing the application.	*/
    Custom DB Initializer:					/* 
    	You can also create your own custom initializer, 
    	if any of the above don't satisfy your requirements 
    	or you want to do some other process that initializes the database using above initializer.	*/

//To use one of the above DB initialization strategies, you have to set the DB Initializer using Database class in Context class as following:

     
    public class SchoolDBContext: DbContext 
    {
        
        public SchoolDBContext(): base("SchoolDBConnectionString") 
        {
            Database.SetInitializer&lt;SchoolDBContext&gt;(new CreateDatabaseIfNotExists&lt;SchoolDBContext&gt;());

            //Database.SetInitializer&lt;SchoolDBContext&gt;(new DropCreateDatabaseIfModelChanges&lt;SchoolDBContext&gt;());
            //Database.SetInitializer&lt;SchoolDBContext&gt;(new DropCreateDatabaseAlways&lt;SchoolDBContext&gt;());
            //Database.SetInitializer&lt;SchoolDBContext&gt;(new SchoolDBInitializer());
        }
        public DbSet&lt;Student&gt; Students { get; set; }
        public DbSet&lt;Standard&gt; Standards { get; set; }
    }
        
//You can also create your custom DB initializer, by inheriting one of the intializers as shown below:

    
    public class SchoolDBInitializer :  CreateDatabaseIfNotExists&lt;SchoolDBContext&gt;
    {
        protected override void Seed(SchoolDBContext context)
        {
            base.Seed(context);
        }
    }
  
//Set db initializer in the configuration file:

You can also set db initializer in the configuration file. For example, to set default initializer in app.config:

   
    &lt;?xml version="1.0" encoding="utf-8" ?&gt;
    &lt;configuration&gt;
      &lt;appSettings&gt;
        &lt;add key="DatabaseInitializerForType SchoolDataLayer.SchoolDBContext, SchoolDataLayer"         
            value="System.Data.Entity.DropCreateDatabaseAlways`1[[SchoolDataLayer.SchoolDBContext, SchoolDataLayer]], EntityFramework" /&gt;
      &lt;/appSettings&gt;
    &lt;/configuration&gt;
        

//You can set custom db initializer as following:

    &lt;?xml version="1.0" encoding="utf-8" ?&gt;
    &lt;configuration&gt;
      &lt;appSettings&gt;    
        &lt;add key="DatabaseInitializerForType SchoolDataLayer.SchoolDBContext, SchoolDataLayer"
             value="SchoolDataLayer.SchoolDBInitializer, SchoolDataLayer" /&gt;
      &lt;/appSettings&gt;
    &lt;/configuration&gt;
        
</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>MVC_Migration</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>about_MVC_Migration</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>MVC_Migration</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>about_MVC_Migration</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>about_MVC_Migration

/* Initially, during Development, we have configured the  project by configuring the Entity Framework 
	to automatically drop and re-create the database each time you change the data model. 
When you add, remove, or change entity classes or change your DbContext class, 
	the next time you run the application it automatically deletes your existing database, 
	creates a new one that matches the model, and seeds it with test data.	
*/
   public class ProjectInitializer : System.Data.Entity.DropCreateDatabaseIfModelChanges&lt;ProjectContext&gt;
    {

        protected override void Seed(ProjectContext context)
        {
			...
            SaveChanges(context);	//For debugging: DbEntityValidationException 
        }

        private static void SaveChanges(DbContext context)
        {
            ...
            }
        }
    }
/*
This method of keeping the database in sync with the data model works well 
	until you deploy the application to production. 
When the application is running in production it is usually storing data that you want to keep, 
	and you don't want to lose everything each time you make a change such as adding a new column. 
The Code First Migrations feature solves this problem by enabling Code First to update the database schema 
	instead of dropping and re-creating the database. 

*/


</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>MVC_Migration</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>ConfigureMigration_after_initial_Development_using_DropCreateDatabaseIfModelChanges</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>MVC_Migration</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>ConfigureMigration_after_initial_Development_using_DropCreateDatabaseIfModelChanges</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>


/*
1] Disable the initializer that you set up earlier by commenting out or deleting the contexts element 
	that you added to the application Web.config file.
	Comment out these lines in  Web.config file (where 'ContosoUniversity' is the assembly name,
		and 'SchoolInitializer' is comparable to my 'ProjectInitializer' :
*/
  &lt;contexts&gt;
    &lt;context type="ContosoUniversity.DAL.SchoolContext, ContosoUniversity"&gt;
      &lt;databaseInitializer type="ContosoUniversity.DAL.SchoolInitializer, ContosoUniversity" /&gt;
    &lt;/context&gt;
  &lt;/contexts&gt;
 /*
 2] Also in the application Web.config file, change the name of the database in the connection string to "ContosoUniversity2"
 	where it has been up until now "ContosoUniversity"
 	
 This change sets up the project so that the first migration will create a new database. 
 This isn't required but you'll see later why it's a good idea.
 
 3] From the Tools menu, click Library Package Manager and then Package Manager Console.
 
]4 At the PM&gt; prompt enter the following commands:	*/
	&gt;enable-migrations			//-Force
	&gt;add-migration InitialCreate 
/*
	The enable-migrations command creates a Migrations folder in the ContosoUniversity project, 
		and it puts in that folder a Configuration.cs file that you can edit to configure Migrations. 
	
	(If you missed the step above that directs you to change the database name, 
		Migrations will find the existing database and automatically do the  add-migration command. 
	That's OK, it just means you won't run a test of the migrations code before you deploy the database. 
	Later when you run the  update-database command nothing will happen because the database will already exist.)

	Like the initializer class that you saw earlier, the Configuration class includes a Seed method. */
	
internal sealed class Configuration : DbMigrationsConfiguration&lt;ContosoUniversity.DAL.SchoolContext&gt;
{
    public Configuration()
    {
        AutomaticMigrationsEnabled = false;
    }

    protected override void Seed(ContosoUniversity.DAL.SchoolContext context)
    {
        //  This method will be called after migrating to the latest version.

        //  You can use the DbSet&lt;T&gt;.AddOrUpdate() helper extension method 
        //  to avoid creating duplicate seed data. E.g.
        //
        //    context.People.AddOrUpdate(
        //      p =&gt; p.FullName,
        //      new Person { FullName = "Andrew Peters" },
        //      new Person { FullName = "Brice Lambson" },
        //      new Person { FullName = "Rowan Miller" }
        //    );
        //
    }
}
/*
When you are dropping and re-creating the database for every data model change, 
	you use the initializer class's Seed method to insert test data, 
	because after every model change the database is dropped and all the test data is lost. 
With Code First Migrations, test data is retained after database changes, 
	so including test data in the Seed method is typically not necessary. 
In fact, you don't want the Seed method to insert test data if you'll be using Migrations to deploy the database to production, 
	because the Seed method will run in production. 
In that case you want the Seed method to insert into the database only the data that you need in production. 
For example, you might want the database to include actual department names in the Department table when the application becomes available in production.

For this tutorial, you'll be using Migrations for deployment, 
	but your Seed method will insert test data anyway in order to make it easier to see how application functionality works 
	without having to manually insert a lot of data.

Set up the Seed Method
-----------------------
1] Replace the contents of the Configuration.cs file with the following code, which will load test data into the new database. 
*/
namespace ContosoUniversity.Migrations
{
    using ContosoUniversity.Models;
    using System;
    using System.Collections.Generic;
    using System.Data.Entity;
    using System.Data.Entity.Migrations;
    using System.Linq;

    internal sealed class Configuration : DbMigrationsConfiguration&lt;ContosoUniversity.DAL.SchoolContext&gt;
    {
        public Configuration()
        {
            AutomaticMigrationsEnabled = false;
        }

        protected override void Seed(ContosoUniversity.DAL.SchoolContext context)
        {
            var students = new List&lt;Student&gt;
            {
                new Student { FirstMidName = "Carson",   LastName = "Alexander", 
                    EnrollmentDate = DateTime.Parse("2010-09-01") },
                new Student { FirstMidName = "Meredith", LastName = "Alonso",    
                    EnrollmentDate = DateTime.Parse("2012-09-01") },
                new Student { FirstMidName = "Arturo",   LastName = "Anand",     
                    EnrollmentDate = DateTime.Parse("2013-09-01") },
                new Student { FirstMidName = "Gytis",    LastName = "Barzdukas", 
                    EnrollmentDate = DateTime.Parse("2012-09-01") },
                new Student { FirstMidName = "Yan",      LastName = "Li",        
                    EnrollmentDate = DateTime.Parse("2012-09-01") },
                new Student { FirstMidName = "Peggy",    LastName = "Justice",   
                    EnrollmentDate = DateTime.Parse("2011-09-01") },
                new Student { FirstMidName = "Laura",    LastName = "Norman",    
                    EnrollmentDate = DateTime.Parse("2013-09-01") },
                new Student { FirstMidName = "Nino",     LastName = "Olivetto",  
                    EnrollmentDate = DateTime.Parse("2005-08-11") }
            };
            students.ForEach(s =&gt; context.Students.AddOrUpdate(p =&gt; p.LastName, s));
            context.SaveChanges();

            var courses = new List&lt;Course&gt;
            {
                new Course {CourseID = 1050, Title = "Chemistry",      Credits = 3, },
                new Course {CourseID = 4022, Title = "Microeconomics", Credits = 3, },
                new Course {CourseID = 4041, Title = "Macroeconomics", Credits = 3, },
                new Course {CourseID = 1045, Title = "Calculus",       Credits = 4, },
                new Course {CourseID = 3141, Title = "Trigonometry",   Credits = 4, },
                new Course {CourseID = 2021, Title = "Composition",    Credits = 3, },
                new Course {CourseID = 2042, Title = "Literature",     Credits = 4, }
            };
            courses.ForEach(s =&gt; context.Courses.AddOrUpdate(p =&gt; p.Title, s));
            context.SaveChanges();

            var enrollments = new List&lt;Enrollment&gt;
            {
                new Enrollment { 
                    StudentID = students.Single(s =&gt; s.LastName == "Alexander").ID, 
                    CourseID = courses.Single(c =&gt; c.Title == "Chemistry" ).CourseID, 
                    Grade = Grade.A 
                },
                 new Enrollment { 
                    StudentID = students.Single(s =&gt; s.LastName == "Alexander").ID,
                    CourseID = courses.Single(c =&gt; c.Title == "Microeconomics" ).CourseID, 
                    Grade = Grade.C 
                 },                            
                 new Enrollment { 
                    StudentID = students.Single(s =&gt; s.LastName == "Alexander").ID,
                    CourseID = courses.Single(c =&gt; c.Title == "Macroeconomics" ).CourseID, 
                    Grade = Grade.B
                 },
                 new Enrollment { 
                     StudentID = students.Single(s =&gt; s.LastName == "Alonso").ID,
                    CourseID = courses.Single(c =&gt; c.Title == "Calculus" ).CourseID, 
                    Grade = Grade.B 
                 },
                 new Enrollment { 
                     StudentID = students.Single(s =&gt; s.LastName == "Alonso").ID,
                    CourseID = courses.Single(c =&gt; c.Title == "Trigonometry" ).CourseID, 
                    Grade = Grade.B 
                 },
                 new Enrollment {
                    StudentID = students.Single(s =&gt; s.LastName == "Alonso").ID,
                    CourseID = courses.Single(c =&gt; c.Title == "Composition" ).CourseID, 
                    Grade = Grade.B 
                 },
                 new Enrollment { 
                    StudentID = students.Single(s =&gt; s.LastName == "Anand").ID,
                    CourseID = courses.Single(c =&gt; c.Title == "Chemistry" ).CourseID
                 },
                 new Enrollment { 
                    StudentID = students.Single(s =&gt; s.LastName == "Anand").ID,
                    CourseID = courses.Single(c =&gt; c.Title == "Microeconomics").CourseID,
                    Grade = Grade.B         
                 },
                new Enrollment { 
                    StudentID = students.Single(s =&gt; s.LastName == "Barzdukas").ID,
                    CourseID = courses.Single(c =&gt; c.Title == "Chemistry").CourseID,
                    Grade = Grade.B         
                 },
                 new Enrollment { 
                    StudentID = students.Single(s =&gt; s.LastName == "Li").ID,
                    CourseID = courses.Single(c =&gt; c.Title == "Composition").CourseID,
                    Grade = Grade.B         
                 },
                 new Enrollment { 
                    StudentID = students.Single(s =&gt; s.LastName == "Justice").ID,
                    CourseID = courses.Single(c =&gt; c.Title == "Literature").CourseID,
                    Grade = Grade.B         
                 }
            };

            foreach (Enrollment e in enrollments)
            {
                var enrollmentInDataBase = context.Enrollments.Where(
                    s =&gt;
                         s.Student.ID == e.StudentID &amp;&amp;
                         s.Course.CourseID == e.CourseID).SingleOrDefault();
                if (enrollmentInDataBase == null)
                {
                    context.Enrollments.Add(e);
                }
            }
            context.SaveChanges();
        }
    }
}

/*
The  Seed method takes the database context object as an input parameter, 
	and the code in the method uses that object to add new entities to the database. 
For each entity type, the code creates a collection of new entities, 
	adds them to the appropriate DbSet property, and then saves the changes to the database. 
It isn't necessary to call the SaveChanges method after each group of entities, as is done here, 
	but doing that helps you locate the source of a problem if an exception occurs while the code is writing to the database.

Some of the statements that insert data use the  AddOrUpdate method to perform an "upsert" operation. 
Because the Seed method runs every time you execute the  update-database command, typically after each migration, 
you can't just insert data, because the rows you are trying to add will already be there after the first migration that creates the database. 
The "upsert" operation prevents errors that would happen if you try to insert a row that already exists, 
but it overrides any changes to data that you may have made while testing the application. 
With test data in some tables you might not want that to happen:  
	in some cases when you change data while testing you want your changes to remain after database updates. 
In that case you want to do a conditional insert operation: insert a row only if it doesn't already exist. 
The Seed method uses both approaches.

The first parameter passed to the  AddOrUpdate method specifies the property to use to check if a row already exists. 
For the test student data that you are providing, 
the LastName property can be used for this purpose since each last name in the list is unique: 
*/
context.Students.AddOrUpdate(p =&gt; p.LastName, s)
/*
This code assumes that last names are unique. 
If you manually add a student with a duplicate last name, 
	you'll get the following exception the next time you perform a migration.
*/ 
Sequence contains more than one element
/*
For information about how to handle redundant data such as two students named "Alexander Carson", see 
http://blogs.msdn.com/b/rickandy/archive/2013/02/12/seeding-and-debugging-entity-framework-ef-dbs.aspx


PRIMARY KEY
-------------------
The code that creates Enrollment entities assumes you have the ID value in the entities in the students collection, 
	although you didn't set that property in the code that creates the collection. 
*/	
new Enrollment { 
    StudentID = students.Single(s =&gt; s.LastName == "Alexander").ID, 
    CourseID = courses.Single(c =&gt; c.Title == "Chemistry" ).CourseID, 
    Grade = Grade.A 
},
/*
You can use the ID property here because the ID value is set when you call SaveChanges for the students collection. 
EF automatically gets the primary key value when it inserts an entity into the database, 
	and it updates the ID property of the entity in memory. 

TO PRESERVE CHANGES
--------------------
The code that adds each Enrollment entity to the  Enrollments entity set doesn't use the  AddOrUpdate method. 
It checks if an entity already exists and inserts the entity if it doesn't exist. 
This approach will preserve changes you make to an enrollment grade by using the application UI. 
The code loops through each member of the Enrollment List and if the enrollment is not found in the database, 
	it adds the enrollment to the database. 
The first time you update the database, the database will be empty, so it will add each enrollment. 
*/
foreach (Enrollment e in enrollments)
{
    var enrollmentInDataBase = context.Enrollments.Where(
        s =&gt; s.Student.ID == e.Student.ID &amp;&amp;
             s.Course.CourseID == e.Course.CourseID).SingleOrDefault();
    if (enrollmentInDataBase == null)
    {
        context.Enrollments.Add(e);
    }
}
/*
THEN BUILD THE PROJECT


Execute the First Migration
------------------------------

When you executed the*/ add-migration /*command, 
	Migrations generated the code that would create the database from scratch. 
This code is also in the Migrations folder, in the file named  &lt;timestamp&gt;_InitialCreate.cs. 
The Up method of the InitialCreate class creates the database tables that correspond to the data model entity sets, 
	and the  Down method deletes them
	
Migrations calls the Up method to implement the data model changes for a migration. 
When you enter a command to roll back the update, Migrations calls the Down method.	

This is the initial migration that was created when you entered the add-migration InitialCreate command. 
The parameter (InitialCreate in the example) is used for the file name and can be whatever you want. 
For example, you might name a later migration "AddDepartmentTable".
*/
public partial class InitialCreate : DbMigration
{
    public override void Up()
    {
        CreateTable(
            "dbo.Course",
            c =&gt; new
                {
                    CourseID = c.Int(nullable: false),
                    Title = c.String(),
                    Credits = c.Int(nullable: false),
                })
            .PrimaryKey(t =&gt; t.CourseID);
        
        CreateTable(
            "dbo.Enrollment",
            c =&gt; new
                {
                    EnrollmentID = c.Int(nullable: false, identity: true),
                    CourseID = c.Int(nullable: false),
                    StudentID = c.Int(nullable: false),
                    Grade = c.Int(),
                })
            .PrimaryKey(t =&gt; t.EnrollmentID)
            .ForeignKey("dbo.Course", t =&gt; t.CourseID, cascadeDelete: true)
            .ForeignKey("dbo.Student", t =&gt; t.StudentID, cascadeDelete: true)
            .Index(t =&gt; t.CourseID)
            .Index(t =&gt; t.StudentID);
        
        CreateTable(
            "dbo.Student",
            c =&gt; new
                {
                    ID = c.Int(nullable: false, identity: true),
                    LastName = c.String(),
                    FirstMidName = c.String(),
                    EnrollmentDate = c.DateTime(nullable: false),
                })
            .PrimaryKey(t =&gt; t.ID);
        
    }
    
    public override void Down()
    {
        DropForeignKey("dbo.Enrollment", "StudentID", "dbo.Student");
        DropForeignKey("dbo.Enrollment", "CourseID", "dbo.Course");
        DropIndex("dbo.Enrollment", new[] { "StudentID" });
        DropIndex("dbo.Enrollment", new[] { "CourseID" });
        DropTable("dbo.Student");
        DropTable("dbo.Enrollment");
        DropTable("dbo.Course");
    }
}

/*
If you created the initial migration when the database already exists, 
	the database creation code is generated but it doesn't have to run because the database already matches the data model. 
When you deploy the app to another environment where the database doesn't exist yet, 
	this code will run to create your database, 
	so it's a good idea to test it first.
That's why you changed the name of the database in the connection string earlier -- so that migrations can create a new one from scratch. 
---------------------------------------------------------------------------------------------------------------------------------------
1] In the Package Manager Console window, enter the following command: */
&gt;update-database 
/*
The update-database command runs the Up method to create the database and then it runs the Seed method to populate the database. 
The same process will run automatically in production after you deploy the application, as you'll see in the following section.






</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>MVC_Migration</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>real_life_strategy_for_making_a_Change_and_dealing_with_ERRORs</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>MVC_Migration</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>real_life_strategy_for_making_a_Change_and_dealing_with_ERRORs</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>/*

1] Add the Migration, and name it.
2] Then update the database with the migration's information.

You can edit the migration file from the first step, if the second step fails.
	You may have to add in some custom SQL

You may have to change the target database in the web.config to a new version inorder to build it, thus temporarily loosing the existing data.


Here are the notes from the ContosoUniversity example:
======================================================


Add a Migration and Update the Database
------------------------------------------

enter the command to add a migration	(don't do the update-database command yet):	*/

&gt;add-Migration ComplexDataModel

/*
If you tried to run the update-database command at this point (don't do it yet), you would get the following error:	*/

The ALTER TABLE statement conflicted with the FOREIGN KEY constraint "FK_dbo.Course_dbo.Department_DepartmentID". 
The conflict occurred in database "ContosoUniversity", table "dbo.Department", column 'DepartmentID'.
    
/*
Sometimes when you execute migrations with existing data, 
	you need to insert stub data into the database to satisfy foreign key constraints, 
	and that's what you have to do now. 
The generated code in the ComplexDataModel Up method adds a non-nullable DepartmentID foreign key to the Course table. 
Because there are already rows in the Course table when the code runs, 
	the AddColumn operation will fail because SQL Server doesn't know what value to put in the column that can't be null. 

Therefore have to change the code to give the new column a default value, 
	and create a stub department named "Temp" to act as the default department. 
As a result, existing Course rows will all be related to the "Temp" department after the Up method runs.  
You can relate them to the correct departments in the Seed method.    
    
Edit the &lt;timestamp&gt;_ComplexDataModel.cs file, 
	comment out the line of code that adds the DepartmentID column to the Course table, 
	and add the following highlighted code (the commented line is also highlighted):  
	
After the statement in the configuration file to */

	"CreateTable( "dbo.CourseInstructor","
	
/* add these lines */    

    // Create  a department for course to point to.
    Sql("INSERT INTO dbo.Department (Name, Budget, StartDate) VALUES ('Temp', 0.00, GETDATE())");
    //  default value for FK points to department created above.
    AddColumn("dbo.Course", "DepartmentID", c =&gt; c.Int(nullable: false, defaultValue: 1)); 
    //AddColumn("dbo.Course", "DepartmentID", c =&gt; c.Int(nullable: false));    
    
/*
When the Seed method runs, it will insert rows in the  Department table and it will relate existing Course rows to those new Department rows. 

If you haven't added any courses in the UI, 
	you would then no longer need the "Temp" department or the default value on the Course.DepartmentID column. 
	
To allow for the possibility that someone might have added courses by using the application, 
	you'd also want to update the Seed method code to ensure that all Course rows 
	(not just the ones inserted by earlier runs of the Seed method) 
	have valid DepartmentID values before you remove the default value from the column and delete the "Temp" department.    
    AND HOW THE HELL DO YOU DO THAT?   
    
After you have finished editing the &lt;timestamp&gt;_ComplexDataModel.cs file, enter the  update-database command in the PMC to execute the migration.
*/
&gt;update-database    
    
/*
Handling ERRORs during Migration
----------------------------------

 1] If you get migration errors you can't resolve, 
 	you can either change the database name in the connection string or delete the database.
 	 The simplest approach is to rename the database in  Web.config file.   
   
 	With a new database, there is no data to migrate, and the  update-database command is much more likely to complete without errors.  
   
    
2] If that fails, another thing you can try is re-initialize the database by entering the following command in the PMC:*/

&gt;update-database -TargetMigration:0
    </Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>MVC_Migration</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>Seed_Method_for_Many-to-Many</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>MVC_Migration</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>Seed_Method_for_Many-to-Many</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>/*
Update the Migrations\Configuration.cs file : add the Seed Method
--------------------------------------------

Notice how the Course entity, which has a many-to-many relationship with the Instructor entity, is handled:
*/

var courses = new List&lt;Course&gt;
{
    new Course {CourseID = 1050, Title = "Chemistry",      Credits = 3,
      DepartmentID = departments.Single( s =&gt; s.Name == "Engineering").DepartmentID,
      Instructors = new List&lt;Instructor&gt;() 
    },
    ...
};
courses.ForEach(s =&gt; context.Courses.AddOrUpdate(p =&gt; p.CourseID, s));
context.SaveChanges();
    
/*
When you create a Course object, you initialize the Instructors navigation property as an empty collection using the code */
	Instructors = new List&lt;Instructor&gt;()
/*This makes it possible to add Instructor entities that are related to this Course by using the Instructors.Add method. 
If you didn't create an empty list, 
	you wouldn't be able to add these relationships, 
	because the Instructors property would be null and wouldn't have an Add method. 

NOTE: You could also add the list initialization to the constructor.  

This is for these entities: Instructor  &amp;  Course

The Instructor and Course entities have that kind of many-to-many relationship, 
	and as you can see, there is no entity class between them:    
*/    
    
public class Instructor
{
   public int ID { get; set; }

   public virtual ICollection&lt;Course&gt; Courses { get; set; }

}    
    
   public class Course
   {
      [DatabaseGenerated(DatabaseGeneratedOption.None)]
      [Display(Name = "Number")]
      public int CourseID { get; set; }

      public virtual ICollection&lt;Instructor&gt; Instructors { get; set; }
   }    
    
//A join table is required in the database, however it is handled by EF without requiring any coding:    
    
Instructor		CourseInstructor
==========		===============			Course
ID		&lt;---&gt;	InstructorID			============
LastName		CourseID	&lt;---&gt;		CourseID
FirstMidName							Title
HireDate								Credits
										DepartmentID</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>MVC_Migration</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>When_modelChanges_update_WITHOUT_loosing_data</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>MVC_Migration</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>When_modelChanges_update_WITHOUT_loosing_data</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code> //You get the following error:

The model backing the 'SchoolContext' context has changed since the database was created. 
Consider using Code First Migrations to update the database (http://go.microsoft.com/fwlink/?LinkId=238269). 

/*
The database model  has changed in a way that requires a change in the database schema, 
	and Entity Framework detected that. 
	You'll use migrations to update the schema without losing any data that you added to the database by using the UI. 
	If you changed data that was created by the Seed method, 
	that will be changed back to its original state because of the AddOrUpdate method that you're using in the Seed method.

In the Package Manager Console (PMC), enter the following commands: */

&gt;add-migration MaxLengthOnNames	//where 'MaxLengthOnNames' is just some description of the changes you made
&gt;update-database

/*The add-migration command creates a file named &lt;timeStamp&gt;_MaxLengthOnNames.cs. 
This file contains code in the Up method that will update the database to match the current data model. 
The  update-database command ran that code.

The timestamp prepended to the migrations file name is used by Entity Framework to order the migrations. 
You can create multiple migrations before running the update-database command, 
	and then all of the migrations are applied in the order in which they were created. 
</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>MVC_Misc</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>01_EntityFramework_TEMPLATE__/DAL/ProjectInitializer.cs</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>MVC_Misc</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>01_EntityFramework_TEMPLATE__/DAL/ProjectInitializer.cs</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>//01_EntityFramework_TEMPLATE__/DAL/ProjectInitializer.cs

using System;
using System.Collections.Generic;
using System.Data.Entity;
using System.Data.Entity.Migrations;
using System.Data.Entity.Validation;
using System.Text;
using globalCommon;
using prj_0043_subprj_07.Models;

namespace prj_0043_subprj_07.DAL
{


    /*
     * To tell Entity Framework to use your initializer class, 
     *  add an element to the entityFramework element in the application Web.config file (the one in the root project folder), 
     *  as shown in the following example:
     * &lt;entityFramework&gt;
            &lt;contexts&gt;
              &lt;context type="DropDown.DAL.ProjectContext, DropDown"&gt;
                &lt;databaseInitializer type="DropDown.DAL.ProjectInitializer, DropDown" /&gt;
              &lt;/context&gt;
            &lt;/contexts&gt;
     * &lt;/entityFramework&gt;
     * (When you don't want EF to use the initializer, you can set an attribute on the context element: disableDatabaseInitialization="true".)
     * 
     * As an alternative to setting the initializer in the Web.config file is to do it in code by adding a Database.SetInitializer statement to the Application_Start method in in the Global.asax.cs file.
     * 
     * The application is now set up so that when you access the database for the first time in a given run of the application, 
     *      the Entity Framework compares the database to the model (ProjectContext and entity classes). 
     *      If there's a difference, the application drops and re-creates the database.
    */

    public class ProjectInitializer : DropCreateDatabaseIfModelChanges&lt;ProjectContext&gt;
    //public class ProjectInitializer : System.Data.Entity.DropCreateDatabaseAlways&lt;ProjectContext&gt;
    {

        protected override void Seed(ProjectContext context)
        {
            if (Global_BackAndFrontEnd.g_TEST)
            {


            }

            Random rnd = new Random();


            //======================================================================================================================
            for (int i = 0; i &lt; 10; i++)
            {
                context.HeaderRecords.AddOrUpdate(h =&gt; h.HeaderRecordID, new HeaderRecord
                {
                    Header_Percent_Done_0_00_to_1_00 = (decimal?)(rnd.NextDouble()),
                    Header_Name = rnd.Random_LowerCaseLetters(5),
                    Header_PhoneNumber = rnd.Random_Numbers(10),
                    Header_Category_Value = (Int16?)rnd.Next(0, 6),
                    Header_bActive = ((short?)rnd.Next(0, 1) == 1),
                    Header_Category_Description = rnd.Random_LowerCaseLetters(5),
                    DetailRecords =
                        new List&lt;DetailRecord&gt;
                        {

                            new DetailRecord
                            {
                                Detail_Category_Value =  rnd.Next(1, 6),
                                Detail_Name = rnd.Random_UpperCaseLetters(5),
                            },
                            new DetailRecord
                            {
                                Detail_Category_Value = rnd.Next(1, 6),
                                Detail_Name = rnd.Random_UpperCaseLetters(5),
								
                            }
                        }
                });

            }

            SaveChanges(context);
            //======================================================================================================================
            var enums = new List&lt;enumTable&gt;
	            {	        
                new enumTable { EnumDescription = "unknown", EnumValue = 0, EnumNameSpace_INT = 1 },
                new enumTable { EnumDescription = "Red", EnumValue = 1, EnumNameSpace_INT = 1 },
                new enumTable { EnumDescription = "Blue", EnumValue = 2, EnumNameSpace_INT = 1 },
                new enumTable { EnumDescription = "Green", EnumValue = 3, EnumNameSpace_INT = 1 },
                new enumTable { EnumDescription = "Indigo", EnumValue = 4, EnumNameSpace_INT = 1 },
                new enumTable { EnumDescription = "Violet", EnumValue = 5, EnumNameSpace_INT = 1 }
                };
            enums.ForEach(s =&gt; context.Enums.Add(s));
            SaveChanges(context);	//For debugging: DbEntityValidationException 

            //======================================================================================================================

            for (int i = 0; i &lt; 7; i++)
            {
                context.JustHeaders.AddOrUpdate(j =&gt; j.JustHeaderID, new JustHeader
                {
                    Percent_Done_0_00_to_1_00 = (decimal?)(rnd.NextDouble()),
                    JustHeaderName = rnd.Random_LowerCaseLetters(5),
                    PhoneNumber = rnd.Random_Numbers(10),
                    Category_Value = (Int16?)rnd.Next(0, 6),
                    bActive = ((short?)rnd.Next(0, 1) == 1),
                    Category_Description = rnd.Random_LowerCaseLetters(5)
                }
                                                );
            }

            SaveChanges(context);	//For debugging: DbEntityValidationException 
            //======================================================================================================================

            var principals = new List&lt;cPrincipal&gt;
	            {	        
                new cPrincipal {principalName = "principal_1"},
                new cPrincipal { principalName = "principal_2"},
                new cPrincipal { principalName = "principal_3"},
                new cPrincipal { principalName = "principal_4"},
                new cPrincipal { principalName = "principal_5"},
                new cPrincipal { principalName = "principal_6"},
                };
            principals.ForEach(s =&gt; context.oPrincipals.Add(s));
            SaveChanges(context);	//For debugging: DbEntityValidationException 
            //======================================================================================================================
            var dependents = new List&lt;cDependent&gt;
	            {	        
                new cDependent { dID = 1, DependentName = "dependent_2"},
                new cDependent { dID = 2, DependentName = "dependent_4"},
                new cDependent { dID = 3, DependentName = "dependent_6"},
                new cDependent { dID = 4, DependentName = "dependent_8"},
                };


            dependents.ForEach(s =&gt; context.ocDependents.Add(s));
            SaveChanges(context);	//For debugging: DbEntityValidationException 

            //======================================================================================================================
            var manys = new List&lt;cMany&gt;
	            {	        
                new cMany() {mName = "Many_1"},
                new cMany() {mName = "Many_2"},
                new cMany() {mName = "Many_3"},
                new cMany() {mName = "Many_4"},
                new cMany() {mName = "Many_5"},
                new cMany() {mName = "Many_6"},
                };
            manys.ForEach(s =&gt; context.oManys.Add(s));
            SaveChanges(context);	//For debugging: DbEntityValidationException 
            //======================================================================================================================
            var ones = new List&lt;cOne&gt;
	            {	        
                new cOne() { oneID = 2, oneName = "one_4"},
                new cOne() { oneID = 4, oneName = "one_8"},
                new cOne() { oneID = 6, oneName = "one_12"},
                new cOne() { oneID = 8, oneName = "one_16"},
                };


            ones.ForEach(s =&gt; context.oOnes.Add(s));
            SaveChanges(context);	//For debugging: DbEntityValidationException 

            //======================================================================================================================
        }

        /// &lt;summary&gt;
        /// Wrapper for SaveChanges adding the Validation Messages to the generated exception
        /// &lt;/summary&gt;
        /// &lt;param name="context"&gt;The context.&lt;/param&gt;
        private static void SaveChanges(DbContext context)
        {
            try
            {
                context.SaveChanges();
            }
            catch (DbEntityValidationException ex)
            {
                StringBuilder sb = new StringBuilder();

                foreach (var failure in ex.EntityValidationErrors)
                {
                    sb.AppendFormat("{0} failed validation\n", failure.Entry.Entity.GetType());
                    foreach (var error in failure.ValidationErrors)
                    {
                        sb.AppendFormat("- {0} : {1}", error.PropertyName, error.ErrorMessage);
                        sb.AppendLine();
                    }
                }

                throw new DbEntityValidationException(
                    "Entity Validation Failed - errors follow:\n" +
                    sb.ToString(), ex
                ); // Add the original exception as the innerException
            }
        }



    }
}
</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>MVC_Misc</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>Deployment _Preparation</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>MVC_Misc</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>Deployment _Preparation</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>
/*
1] Set a Baseline Database, and stop letting it do "AutomaticMigrationsEnabled = true;" 
	we need to be more carefull b/c we are working with Live data from now on.
	
	So set:
	
*/	
namespace OdeToFood.Migrations
{
    using OdeToFood.Models;
    using System;
    using System.Collections.Generic;
    using System.Data.Entity;
    using System.Data.Entity.Migrations;
    using System.Linq;
    using System.Web.Security;
    using WebMatrix.WebData;

    internal sealed class Configuration : DbMigrationsConfiguration&lt;OdeToFood.Models.OdeToFoodDb&gt;
    {
        public Configuration()
        {
            AutomaticMigrationsEnabled = false;
        }
        ...
        
        
     }
}

/* We will Re-start the migrations, and start working with live data.
	a) get rid of simulated data
	b) delete or exclude any existing migration scripts, in the Migrations folder
		so the definitions for the schema (tables) will come from the Class definitions
	c) Now create the 'initial' migration, but b/c the "AutomaticMigrationsEnabled = false" we 
		can not use the Update database command
*/	
		&gt;Add-Migration InitialCreate
		-------------------------------
		PM&gt; Add-Migration InitialCreate
		Scaffolding migration 'InitialCreate'.
		The Designer Code for this migration file includes a snapshot of your current Code First model. This snapshot is used to calculate the changes to your model when you scaffold the next migration. If you make additional changes to your model that you want to include in this migration, then you can re-scaffold it by running 'Add-Migration InitialCreate' again.
		-------------------------------
/*		It created this file: "201412261837355_InitialCreate.cs"
	d) Modify the SeedMembership method (Configuration.cs) so that it will not run again:
	Change this */
	
        private void SeedMembership()
        {

            WebSecurity.InitializeDatabaseConnection("DefaultConnection",
                    "UserProfile", "UserId", "UserName", autoCreateTables: true);

			...
        }
//To this, so that it will only run if it is NOT already initialized	
        private void SeedMembership()
        {
            if (!WebSecurity.Initialized)
            {
                WebSecurity.InitializeDatabaseConnection("DefaultConnection",
                    "UserProfile", "UserId", "UserName", autoCreateTables: true);
            }

			...
        }
/* 
	This is b/c Web Migrations will now run when the application runs.
               	
		So change the "Application_Start" method in Global.asax from this:	
*/
		
        protected void Application_Start()
        {
            WebSecurity.InitializeDatabaseConnection("DefaultConnection", "UserProfile", "UserId", "UserName", autoCreateTables: true);                                  
            AreaRegistration.RegisterAllAreas();

            WebApiConfig.Register(GlobalConfiguration.Configuration);
            FilterConfig.RegisterGlobalFilters(GlobalFilters.Filters);
            RouteConfig.RegisterRoutes(RouteTable.Routes);
            BundleConfig.RegisterBundles(BundleTable.Bundles);
            AuthConfig.RegisterAuth();
        }		
//To this:
		using System.Data.Entity.Migrations;	//required for migrator
		using OdeToFood.Migrations;	// required for 'Configuration'
		
		
        protected void Application_Start()
        {

            var migrator = new DbMigrator(new Configuration());
            migrator.Update();	//will run schema changes, seed changes, etc

            //This is probably not needed b/c it will probably run before this, but just to be sure.
            if (!WebSecurity.Initialized)
            {
                WebSecurity.InitializeDatabaseConnection("DefaultConnection", "UserProfile", "UserId", "UserName", autoCreateTables: true);
            }
                                  
            AreaRegistration.RegisterAllAreas();

            WebApiConfig.Register(GlobalConfiguration.Configuration);
            FilterConfig.RegisterGlobalFilters(GlobalFilters.Filters);
            RouteConfig.RegisterRoutes(RouteTable.Routes);
            BundleConfig.RegisterBundles(BundleTable.Bundles);
            AuthConfig.RegisterAuth();
        }
/*  
	This is being done with code, rather than config files, to ensure that it runs BEFORE Web Security tries
		to initialize the Database. This is for Forms Authentication with Web Security
     			
     			
	e) When it deploys I want it to run the migrations automatically.
		You can do it through configuration, but this example will do it through code.
				
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>MVC_Misc</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>SEEDing</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>MVC_Misc</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>SEEDing</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>

//ref: http://blogs.msdn.com/b/rickandy/archive/2013/02/12/seeding-and-debugging-entity-framework-ef-dbs.aspx

var students = new List&lt;Student&gt;
{
    new Student { FirstMidName = "Carson",   LastName = "Alexander", EnrollmentDate = DateTime.Parse("2005-09-01") },
    new Student { FirstMidName = "Meredith", LastName = "Alonso",    EnrollmentDate = DateTime.Parse("2002-09-01") },
    new Student { FirstMidName = "Arturo",   LastName = "Anand",     EnrollmentDate = DateTime.Parse("2003-09-01") },
    new Student { FirstMidName = "Gytis",    LastName = "Barzdukas", EnrollmentDate = DateTime.Parse("2002-09-01") },
    new Student { FirstMidName = "Yan",      LastName = "Li",        EnrollmentDate = DateTime.Parse("2002-09-01") },
    new Student { FirstMidName = "Peggy",    LastName = "Justice",   EnrollmentDate = DateTime.Parse("2001-09-01") },
    new Student { FirstMidName = "Laura",    LastName = "Norman",    EnrollmentDate = DateTime.Parse("2003-09-01") },
    new Student { FirstMidName = "Nino",     LastName = "Olivetto",  EnrollmentDate = DateTime.Parse("2005-09-01") },
};


 students.ForEach(s =&gt; context.Students.AddOrUpdate(p =&gt; p.LastName, s));
 context.SaveChanges();
 
 /*
The first parameter passed to the AddOrUpdate method specifies the property 
	to use to check if a row already exists. 
	For the test student data that we are providing, 
	the LastName property can be used for this purpose since each last name in the list is unique.

Using the code above, you can’t add another Alexander Carson. 
	You can add another Alexander Carson who enrolled on a different date 
	from the first entry with the following code:
*/
foreach (Student e in students)
{
    var studentInDB = context.Students.Where(
        s =&gt; s.FirstMidName == e.FirstMidName &amp;&amp;
        s.LastName == e.LastName  &amp;&amp;
        s.EnrollmentDate == e.EnrollmentDate).SingleOrDefault();
      
    if (studentInDB == null)
    {
        context.Students.Add(e);
    }
}

context.SaveChanges();


/*
While the code above solves the problem of adding another Alexander Carson, 
	it’s not a general solution. 
	You can’t add another Alexander Carson with a duplicate enrolment date. 
	An approach I like to take, 
	especially with larger seed databases, 
	is to add each record one at a time. The following code shows how to do this:
*/
context.Students.Add(new Student { FirstMidName = "Carson", LastName = "Alexander" });
context.SaveChanges();
context.Students.Add(new Student { FirstMidName = "Carson", LastName = "Alexander" });
context.SaveChanges();

/*
You don’t need to call SaveChanges after each Add, but doing so will show you exactly where a problem occurs. 

I couldn’t see how there was any duplicates in my code, 
	so I went into debug mode. 
	Adding System.Diagnostics.Debugger.Break(); 
	to the migrations code caused Visual Studio to shut down and restart. 
	I just happened to see Rowan in the kitchen and he suggested I look at his blog 
	Running &amp; Scripting Migrations From Code. 
	I copy/pasted the following lines from his blog into the about method 
	of my controller and was able to step into and debug the seed method.
*/

public ActionResult About()
{
    var configuration = new Configuration();
    var migrator = new DbMigrator(configuration);
    migrator.Update();
    ViewBag.Message = "Your app description page.";

    return View();
}</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>MVC_MODEL</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>01_ComplexModel_relationships_formatting_validation_datamapping__about</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>MVC_MODEL</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>01_ComplexModel_relationships_formatting_validation_datamapping__about</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>01_ComplexModel_relationships_formatting_validation_datamapping__about

/*
Customize the Data Model by Using Attributes
=============================================

NOTE: the Contoso Example does NOT use mapping files

The DataType Attribute
----------------------

Attributes to make the date show up in the desired format: */
        [DataType(DataType.Date)]
        [DisplayFormat(DataFormatString = "{0:yyyy-MM-dd}", ApplyFormatInEditMode = true)]
        public DateTime EnrollmentDate { get; set; }
 /*
 The  DataType attribute is used to specify a data type that is more specific than the database intrinsic type. 
 In this case we only want to keep track of the date, not the date and time. 
 The   DataType Enumeration provides for many data types, such as
 	 Date, Time, PhoneNumber, Currency, EmailAddress and more. 
The DataType attribute can also enable the application to automatically provide type-specific features. 
For example, a mailto: link can be created for DataType.EmailAddress, 
	and a date selector can be provided for DataType.Date in browsers that support HTML5. 
The  DataType attributes emits HTML 5 data- (pronounced data dash) attributes that HTML 5 browsers can understand. 
The  DataType attributes do not provide any validation. 

DataType.Date does not specify the format of the date that is displayed. 
By default, the data field is displayed according to the default formats based on the server's  CultureInfo.

The ApplyFormatInEditMode setting specifies that the specified formatting should also be applied when the value is displayed in a text box for editing. 
(You might not want that for some fields — for example, for currency values, you might not want the currency symbol in the text box for editing.)

You can use the  DisplayFormat attribute by itself, but the The DataType attribute provides the following benefits that you don't get with DisplayFormat:
	The browser can enable HTML5 features
	By default, the browser will render data using the correct format based on your  locale. 
	The  DataType attribute can enable MVC to choose the right field template to render the data 
		(the  DisplayFormat uses the string template). ref: http://bradwilson.typepad.com/blog/2009/10/aspnet-mvc-2-templates-part-1-introduction.html

CHROME BROWSERS
If you use the DataType attribute with a date field, 
you have to specify the  DisplayFormat attribute also in order to ensure that the field renders correctly in Chrome browsers. 


You can also use attributes to control how your classes and properties are mapped to the database. 
	Suppose you had used the name FirstMidName for the first-name field because the field might also contain a middle name. 
	But you want the database column to be named FirstName, 
		because users who will be writing ad-hoc queries against the database are accustomed to that name. 
	To make this mapping, you can use the Column attribute. */
	
	[Column("FirstName")]
	
/*The Column attribute specifies that when the database is created, 
	the column of the Student table that maps to the FirstMidName property will be named FirstName.

The addition of the Column attribute changes the model backing the SchoolContext, 
	so it won't match the database. Enter the following commands in the PMC to create another migration:*/

&gt;add-migration ColumnFirstName
&gt;update-database

/*


Create the Instructor Entity 
----------------------------
The Courses and OfficeAssignment properties are navigation properties. 
As was explained earlier, they are typically defined as virtual so that they can take advantage of an Entity Framework feature called lazy loading. 
In addition, if a navigation property can hold multiple entities, 
	its type must implement the  ICollection&lt;T&gt; Interface. 
For example IList&lt;T&gt; qualifies but not  IEnumerable&lt;T&gt; because IEnumerable&lt;T&gt; doesn't implement Add.

An instructor can teach any number of courses, so Courses is defined as a collection of Course entities. */

public virtual ICollection&lt;Course&gt; Courses { get; set; }

/*
Our business rules state an instructor can only have at most one office, 
	so OfficeAssignment is defined as a single OfficeAssignment entity (which may be null if no office is assigned).*/
	
public virtual OfficeAssignment OfficeAssignment { get; set; }

/*


Create the OfficeAssignment Entity
----------------------------------



The Key Attribute
------------------

There's a one-to-zero-or-one relationship  between the Instructor and the OfficeAssignment entities. 
An office assignment only exists in relation to the instructor it's assigned to, 
	and therefore its primary key is also its foreign key to the Instructor entity. 
But the Entity Framework can't automatically recognize InstructorID as the primary key of this entity 
	because its name doesn't follow the ID or classnameID naming convention.

That is to say the Table "OfficeAssignment" has a Primary Key called InstructorID, HOWEVER it is NOT an IDENTIY, rather it is
originating from the Instrutor table's Primary Key, with a one-to-one(zero) relationship (so it is unique) and it does
not follow EF convention for naming a PK	
	
OfficeAssignment */
=================
CREATE TABLE [dbo].[OfficeAssignment](
	[InstructorID] [int] NOT NULL,
	[Location] [nvarchar](50) NULL,
 CONSTRAINT [PK_dbo.OfficeAssignment] PRIMARY KEY CLUSTERED 
(
	[InstructorID] ASC
)WITH (PAD_INDEX  = OFF, STATISTICS_NORECOMPUTE  = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS  = ON, ALLOW_PAGE_LOCKS  = ON) ON [PRIMARY]
) ON [PRIMARY]


//Instructor:
===========
CREATE TABLE [dbo].[Instructor](
	[ID] [int] IDENTITY(1,1) NOT NULL,
	[LastName] [nvarchar](50) NOT NULL,
	[FirstName] [nvarchar](50) NOT NULL,
	[HireDate] [datetime] NULL,
 CONSTRAINT [PK_dbo.Instructor] PRIMARY KEY CLUSTERED 
(
	[ID] ASC
)WITH (PAD_INDEX  = OFF, STATISTICS_NORECOMPUTE  = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS  = ON, ALLOW_PAGE_LOCKS  = ON) ON [PRIMARY]
) ON [PRIMARY]
	
/*
	 
Therefore, the Key attribute is used to identify it as the key:
*/
[Key]
[ForeignKey("Instructor")]
public int InstructorID { get; set; }

/*
You can also use the Key attribute if the entity does have its own primary key but you want to name the property something different than classnameID or ID. 
By default EF treats the key as non-database-generated because the column is for an identifying relationship.



The ForeignKey Attribute
-------------------------

When there is a  one-to-zero-or-one relationship or a  one-to-one relationship between two entities 
(such as between OfficeAssignment  and Instructor), 
EF can't work out which end of the relationship is the principal and which end is dependent.  

One-to-one relationships have a reference navigation property in each class to the other class. 
The  ForeignKey Attribute can be applied to the dependent class to establish the relationship. 

If you omit the  ForeignKey Attribute, you get the following error when you try to create the migration: */

Unable to determine the principal end of an association between the types 
'ContosoUniversity.Models.OfficeAssignment' and 'ContosoUniversity.Models.Instructor'. 
The principal end of this association must be explicitly configured using either the relationship fluent API or data annotations.

/*Later in the tutorial you'll see how to configure this relationship with the fluent API. 



The Instructor Navigation Property
------------------------------------

The Instructor entity has a nullable OfficeAssignment navigation property {though I don't see anything to indicate this)
	(because an instructor might not have an office assignment), */
	public class Instructor
	{
		public int ID { get; set; }	
		
					//NOTE: InstructorID is NOT explicitly listed here, but it is in the resulting SQL table, due to the Nav property of the other table
		...
		//Navigation
		public virtual OfficeAssignment OfficeAssignment { get; set; }
	}
	//SQL created by EF
	CREATE TABLE [dbo].[OfficeAssignment](
	[InstructorID] [int] NOT NULL,
	...
 	CONSTRAINT [PK_dbo.OfficeAssignment] PRIMARY KEY CLUSTERED 
		(
			[InstructorID] ASC
		)
	
/*

and the OfficeAssignment entity has a non-nullable Instructor navigation property {though I don't see anything to indicate this)
	(because an office assignment can't exist without an instructor -- InstructorID is non-nullable). 
*/	
    public class OfficeAssignment
    {
		[Key]
        [ForeignKey("Instructor")]					//This, and the Nav property,  creates the [InstructorID] field in the other table!
        public int InstructorID { get; set; }
    	...
    	//Navigation
		public virtual Instructor Instructor { get; set; }
    }
	//SQL created by EF
	CREATE TABLE [dbo].[Instructor](
		[ID] [int] IDENTITY(1,1) NOT NULL,
		...
	 CONSTRAINT [PK_dbo.Instructor] PRIMARY KEY CLUSTERED 
	(
		[ID] ASC
	)

/*	
When an Instructor entity has a related OfficeAssignment entity, each entity will have a reference to the other one in its navigation property.

You could put a  [Required] attribute on the Instructor navigation property to specify that there must be a related instructor, 
	but you don't have to do that because the InstructorID foreign key (which is also the key to this table) is non-nullable.



Modify the Course Entity
-------------------------
The course entity has a foreign key property DepartmentID which points to the related Department entity 
	and it has a Department navigation property. 
*/
   public class Course
   {
      [DatabaseGenerated(DatabaseGeneratedOption.None)]	//specifies that primary key values are provided by the user rather than generated by the database.
      [Display(Name = "Number")]
      public int CourseID { get; set; }

	  ...

      public int DepartmentID { get; set; }

      public virtual Department Department { get; set; }
	  ...
   }
/*
The Entity Framework doesn't require you to add a foreign key property to your data model when you have a navigation property for a related entity.  
EF automatically creates foreign keys in the database wherever they are needed. 
But having the foreign key in the data model can make updates simpler and more efficient. 
For example, when you fetch a course entity to edit, the  Department entity is null if you don't load it, 
	so when you update the course entity, 
	you would have to first fetch the Department entity. 
When the foreign key property DepartmentID is included in the data model, you don't need to fetch the Department entity before you update. 


Foreign Key and Navigation Properties
---------------------------------------
The foreign key properties and navigation properties in the Course entity reflect the following relationships:

A course is assigned to one department, so there's a 
	DepartmentID foreign key 
	and 
	a Department navigation property for the reasons mentioned above. 
	*/
	public int DepartmentID { get; set; }
	public virtual Department Department { get; set; }
	/*
A course can have any number of students enrolled in it, 
	so the Enrollments navigation property is a collection: */
	
	public virtual ICollection&lt;Enrollment&gt; Enrollments { get; set; }
/*
A course may be taught by multiple instructors, 
	so the Instructors navigation property is a collection: */

	public virtual ICollection&lt;Instructor&gt; Instructors { get; set; }

/*

Create the Department Entity
-----------------------------

Foreign Key and Navigation Properties
---------------------------------------

The foreign key and navigation properties reflect the following relationships:
A department may or may not have an administrator, (this is a NAVIGATION property)
	and an administrator is always an instructor. 
Therefore the InstructorID property is included as the foreign key to the Instructor entity, 
	and a question mark is added after the int type designation to mark the property as nullable.
	
The navigation property is named Administrator but holds an Instructor entity: */

   public class Department
   {
      ...
      public int? InstructorID { get; set; }

      public virtual Instructor Administrator { get; set; }		// &lt;==		 Type/Entity = Instructor,   Navigation Property Name = 'Administrator'
      public virtual ICollection&lt;Course&gt; Courses { get; set; }	// A department may have many courses, so there's a Courses navigation property
   }															// 			 Type/Entity = ICollection&lt;Course&gt;,   Navigation Property Name = 'Courses'

/*
NOTE: while I do see the Administrator navigation Property in the EF Model, I  can not find it anywhere in the SQL tables Scripts !!!

By convention, the Entity Framework enables cascade delete for non-nullable foreign keys and for many-to-many relationships. 
This can result in circular cascade delete rules, 
	which will cause an exception when you try to add a migration. 
For example, if you didn't define the Department.InstructorID property as nullable, 
	you'd get the following exception message: "The referential relationship will result in a cyclical reference that's not allowed." 
If your business rules required InstructorID property to be non-nullable,
	you would have to use the following fluent API statement to disable cascade delete on the relationship: 
*/
modelBuilder.Entity().HasRequired(d =&gt; d.Administrator).WithMany().WillCascadeOnDelete(false);

/*
Enrollment Entity
---------------------

Foreign Key and Navigation Properties
--------------------------------------
An enrollment record is for a single course, 
	so there's a CourseID foreign key property and a Course navigation property: */
	public int CourseID { get; set; }
	public virtual Course Course { get; set; }

/*An enrollment record is for a single student, so there's a StudentID foreign key property and a Student navigation property: */

	public int StudentID { get; set; }
	public virtual Student Student { get; set; }

/*
Many-to-Many Relationships
---------------------------

There's a many-to-many relationship between the Student and Course entities, 
	and the Enrollment entity functions as a many-to-many join table with payload in the database. 
This means that the Enrollment table contains additional data besides foreign keys for the joined tables (in this case, 
	a primary key and a Grade property). 
*/

Relationships
.......................
Student (1 - *) Enrollment
Enrollment (* - 1) Course

/*
If the Enrollment table didn't include grade information, it would only need to contain the two foreign keys CourseID and StudentID. 
In that case, it would correspond to a many-to-many join table without payload (or a pure join table) in the database, 
and you wouldn't have to create a model class for it at all.
*/

    public class Student
    {
        public int ID { get; set; }
       ...
        public virtual ICollection&lt;Enrollment&gt; Enrollments { get; set; }
    }
    
    public class Enrollment
    {
        public int EnrollmentID { get; set; }
        public int CourseID { get; set; }
        public int StudentID { get; set; }
        public Grade? Grade { get; set; }

        public virtual Course Course { get; set; }
        public virtual Student Student { get; set; }
    }    
    
   public class Course
   {
      [DatabaseGenerated(DatabaseGeneratedOption.None)]
      [Display(Name = "Number")]
      public int CourseID { get; set; }
      public int DepartmentID { get; set; }

      public virtual ICollection&lt;Enrollment&gt; Enrollments { get; set; }

   }    
    
/*The Instructor and Course entities have that kind of many-to-many relationship, 
	and as you can see, there is no entity class between them:    
*/    
    
public class Instructor
{
   public int ID { get; set; }

   public virtual ICollection&lt;Course&gt; Courses { get; set; }

}    
    
   public class Course
   {
      [DatabaseGenerated(DatabaseGeneratedOption.None)]
      [Display(Name = "Number")]
      public int CourseID { get; set; }

      public virtual ICollection&lt;Instructor&gt; Instructors { get; set; }
   }    
    
//A join table is required in the database, however:    
    
Instructor		CourseInstructor
==========		===============			Course
ID		&lt;---&gt;	InstructorID			============
LastName		CourseID	&lt;---&gt;		CourseID
FirstMidName							Title
HireDate								Credits
										DepartmentID


/*
The Entity Framework automatically creates the CourseInstructor table, 
	and you read and update it indirectly by reading and updating the 
		Instructor.Courses 
		and 
		Course.Instructors 
	navigation properties.    
    
    
Customize the Data Model by adding Code to the Database Context - fluent API
------------------------------------------------------------------    
  
You can also use the fluent API to specify most of the 
	formatting, 
	validation, 
	and mapping rules 
that you can do by using attributes. 
Some attributes such as MinimumLength can't be applied with the fluent API. 
	As mentioned previously, MinimumLength doesn't change the schema, 
		it only applies a client and server side validation rule
    
Some developers prefer to use the fluent API exclusively so that they can keep their entity classes "clean." 
	You can mix attributes and fluent API if you want, 
	and there are a few customizations that can only be done by using fluent API, 
	but in general the recommended practice is to choose one of these two approaches and use that consistently as much as possible.    
    
DAL\SchoolContext.cs - using the Fluent API
--------------------------------------------

The new statement in the OnModelCreating method configures the many-to-many join table:

For the many-to-many relationship between the Instructor and Course entities, 
	the code specifies the table and column names for the join table. 
Code First can configure the many-to-many relationship for you without this code, 
	but if you don't call it, you will get default names such as InstructorInstructorID for the InstructorID column.

*/
modelBuilder.Entity&lt;Course&gt;()
    .HasMany(c =&gt; c.Instructors).WithMany(i =&gt; i.Courses)
    .Map(t =&gt; t.MapLeftKey("CourseID")
        .MapRightKey("InstructorID")
        .ToTable("CourseInstructor"));

/*
The following code provides an example 
of how you could have used fluent API instead of attributes to specify the relationship between the Instructor and OfficeAssignment entities: 
*/

modelBuilder.Entity&lt;Instructor&gt;()
    .HasOptional(p =&gt; p.OfficeAssignment).WithRequired(p =&gt; p.Instructor);

/*
Update the Migrations\Configuration.cs file : add the Seed Method
--------------------------------------------

Notice how the Course entity, which has a many-to-many relationship with the Instructor entity, is handled:
*/

var courses = new List&lt;Course&gt;
{
    new Course {CourseID = 1050, Title = "Chemistry",      Credits = 3,
      DepartmentID = departments.Single( s =&gt; s.Name == "Engineering").DepartmentID,
      Instructors = new List&lt;Instructor&gt;() 
    },
    ...
};
courses.ForEach(s =&gt; context.Courses.AddOrUpdate(p =&gt; p.CourseID, s));
context.SaveChanges();
    
/*
When you create a Course object, you initialize the Instructors navigation property as an empty collection using the code */
	Instructors = new List&lt;Instructor&gt;()
/*This makes it possible to add Instructor entities that are related to this Course by using the Instructors.Add method. 
If you didn't create an empty list, 
	you wouldn't be able to add these relationships, 
	because the Instructors property would be null and wouldn't have an Add method. 
You could also add the list initialization to the constructor.    
    
Add a Migration and Update the Database
------------------------------------------

enter the command to add a migration	(don't do the update-database command yet):	*/

&gt;add-Migration ComplexDataModel

/*
If you tried to run the update-database command at this point (don't do it yet), you would get the following error:	*/

The ALTER TABLE statement conflicted with the FOREIGN KEY constraint "FK_dbo.Course_dbo.Department_DepartmentID". 
The conflict occurred in database "ContosoUniversity", table "dbo.Department", column 'DepartmentID'.
    
/*
Sometimes when you execute migrations with existing data, 
	you need to insert stub data into the database to satisfy foreign key constraints, 
	and that's what you have to do now. 
The generated code in the ComplexDataModel Up method adds a non-nullable DepartmentID foreign key to the Course table. 
Because there are already rows in the Course table when the code runs, 
	the AddColumn operation will fail because SQL Server doesn't know what value to put in the column that can't be null. 

Therefore have to change the code to give the new column a default value, 
	and create a stub department named "Temp" to act as the default department. 
As a result, existing Course rows will all be related to the "Temp" department after the Up method runs.  
You can relate them to the correct departments in the Seed method.    
    
Edit the &lt;timestamp&gt;_ComplexDataModel.cs file, 
	comment out the line of code that adds the DepartmentID column to the Course table, 
	and add the following highlighted code (the commented line is also highlighted):  
	
After the statement in the configuration file to */

	"CreateTable( "dbo.CourseInstructor","
	
/* add these lines */    

    // Create  a department for course to point to.
    Sql("INSERT INTO dbo.Department (Name, Budget, StartDate) VALUES ('Temp', 0.00, GETDATE())");
    //  default value for FK points to department created above.
    AddColumn("dbo.Course", "DepartmentID", c =&gt; c.Int(nullable: false, defaultValue: 1)); 
    //AddColumn("dbo.Course", "DepartmentID", c =&gt; c.Int(nullable: false));    
    
/*
When the Seed method runs, it will insert rows in the  Department table and it will relate existing Course rows to those new Department rows. 

If you haven't added any courses in the UI, 
	you would then no longer need the "Temp" department or the default value on the Course.DepartmentID column. 
	
To allow for the possibility that someone might have added courses by using the application, 
	you'd also want to update the Seed method code to ensure that all Course rows 
	(not just the ones inserted by earlier runs of the Seed method) 
	have valid DepartmentID values before you remove the default value from the column and delete the "Temp" department.    
    AND HOW THE HELL DO YOU DO THAT?   
    
After you have finished editing the &lt;timestamp&gt;_ComplexDataModel.cs file, enter the  update-database command in the PMC to execute the migration.
*/
&gt;update-database    
    
/*
Handling ERRORs during Migration
----------------------------------

 1] If you get migration errors you can't resolve, 
 	you can either change the database name in the connection string or delete the database.
 	 The simplest approach is to rename the database in  Web.config file.   
   
 	With a new database, there is no data to migrate, and the  update-database command is much more likely to complete without errors.  
   
    
2] If that fails, another thing you can try is re-initialize the database by entering the following command in the PMC:*/

&gt;update-database -TargetMigration:0
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    </Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>MVC_MODEL</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>01_ComplexModel_relationships_formatting_validation_datamapping__Migrations\Configuration.cs </Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>MVC_MODEL</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>01_ComplexModel_relationships_formatting_validation_datamapping__Migrations\Configuration.cs </Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>
=================Newer Version =========================\

namespace ContosoUniversity.Migrations
{
    using ContosoUniversity.Models;
    using ContosoUniversity.DAL;
    using System;
    using System.Collections.Generic;
    using System.Data.Entity;
    using System.Data.Entity.Migrations;
    using System.Linq;
    
    internal sealed class Configuration : DbMigrationsConfiguration&lt;SchoolContext&gt;
    {
        public Configuration()
        {
            AutomaticMigrationsEnabled = false;
        }

        protected override void Seed(SchoolContext context)
        {
            var students = new List&lt;Student&gt;
            {
                new Student { FirstMidName = "Carson",   LastName = "Alexander", 
                    EnrollmentDate = DateTime.Parse("2010-09-01") },
                new Student { FirstMidName = "Meredith", LastName = "Alonso",    
                    EnrollmentDate = DateTime.Parse("2012-09-01") },
                new Student { FirstMidName = "Arturo",   LastName = "Anand",     
                    EnrollmentDate = DateTime.Parse("2013-09-01") },
                new Student { FirstMidName = "Gytis",    LastName = "Barzdukas", 
                    EnrollmentDate = DateTime.Parse("2012-09-01") },
                new Student { FirstMidName = "Yan",      LastName = "Li",        
                    EnrollmentDate = DateTime.Parse("2012-09-01") },
                new Student { FirstMidName = "Peggy",    LastName = "Justice",   
                    EnrollmentDate = DateTime.Parse("2011-09-01") },
                new Student { FirstMidName = "Laura",    LastName = "Norman",    
                    EnrollmentDate = DateTime.Parse("2013-09-01") },
                new Student { FirstMidName = "Nino",     LastName = "Olivetto",  
                    EnrollmentDate = DateTime.Parse("2005-09-01") }
            };


            students.ForEach(s =&gt; context.Students.AddOrUpdate(p =&gt; p.LastName, s));
            context.SaveChanges();

            var instructors = new List&lt;Instructor&gt;
            {
                new Instructor { FirstMidName = "Kim",     LastName = "Abercrombie", 
                    HireDate = DateTime.Parse("1995-03-11") },
                new Instructor { FirstMidName = "Fadi",    LastName = "Fakhouri",    
                    HireDate = DateTime.Parse("2002-07-06") },
                new Instructor { FirstMidName = "Roger",   LastName = "Harui",       
                    HireDate = DateTime.Parse("1998-07-01") },
                new Instructor { FirstMidName = "Candace", LastName = "Kapoor",      
                    HireDate = DateTime.Parse("2001-01-15") },
                new Instructor { FirstMidName = "Roger",   LastName = "Zheng",      
                    HireDate = DateTime.Parse("2004-02-12") }
            };
            instructors.ForEach(s =&gt; context.Instructors.AddOrUpdate(p =&gt; p.LastName, s));
            context.SaveChanges();

            var departments = new List&lt;Department&gt;
            {
                new Department { Name = "English",     Budget = 350000, 
                    StartDate = DateTime.Parse("2007-09-01"), 
                    InstructorID  = instructors.Single( i =&gt; i.LastName == "Abercrombie").ID },
                new Department { Name = "Mathematics", Budget = 100000, 
                    StartDate = DateTime.Parse("2007-09-01"), 
                    InstructorID  = instructors.Single( i =&gt; i.LastName == "Fakhouri").ID },
                new Department { Name = "Engineering", Budget = 350000, 
                    StartDate = DateTime.Parse("2007-09-01"), 
                    InstructorID  = instructors.Single( i =&gt; i.LastName == "Harui").ID },
                new Department { Name = "Economics",   Budget = 100000, 
                    StartDate = DateTime.Parse("2007-09-01"), 
                    InstructorID  = instructors.Single( i =&gt; i.LastName == "Kapoor").ID }
            };
            departments.ForEach(s =&gt; context.Departments.AddOrUpdate(p =&gt; p.Name, s));
            context.SaveChanges();

            var courses = new List&lt;Course&gt;
            {
                new Course {CourseID = 1050, Title = "Chemistry",      Credits = 3,
                  DepartmentID = departments.Single( s =&gt; s.Name == "Engineering").DepartmentID,
                  Instructors = new List&lt;Instructor&gt;() 
                },
                new Course {CourseID = 4022, Title = "Microeconomics", Credits = 3,
                  DepartmentID = departments.Single( s =&gt; s.Name == "Economics").DepartmentID,
                  Instructors = new List&lt;Instructor&gt;() 
                },
                new Course {CourseID = 4041, Title = "Macroeconomics", Credits = 3,
                  DepartmentID = departments.Single( s =&gt; s.Name == "Economics").DepartmentID,
                  Instructors = new List&lt;Instructor&gt;() 
                },
                new Course {CourseID = 1045, Title = "Calculus",       Credits = 4,
                  DepartmentID = departments.Single( s =&gt; s.Name == "Mathematics").DepartmentID,
                  Instructors = new List&lt;Instructor&gt;() 
                },
                new Course {CourseID = 3141, Title = "Trigonometry",   Credits = 4,
                  DepartmentID = departments.Single( s =&gt; s.Name == "Mathematics").DepartmentID,
                  Instructors = new List&lt;Instructor&gt;() 
                },
                new Course {CourseID = 2021, Title = "Composition",    Credits = 3,
                  DepartmentID = departments.Single( s =&gt; s.Name == "English").DepartmentID,
                  Instructors = new List&lt;Instructor&gt;() 
                },
                new Course {CourseID = 2042, Title = "Literature",     Credits = 4,
                  DepartmentID = departments.Single( s =&gt; s.Name == "English").DepartmentID,
                  Instructors = new List&lt;Instructor&gt;() 
                },
            };
            courses.ForEach(s =&gt; context.Courses.AddOrUpdate(p =&gt; p.CourseID, s));
            context.SaveChanges();

            var officeAssignments = new List&lt;OfficeAssignment&gt;
            {
                new OfficeAssignment { 
                    InstructorID = instructors.Single( i =&gt; i.LastName == "Fakhouri").ID, 
                    Location = "Smith 17" },
                new OfficeAssignment { 
                    InstructorID = instructors.Single( i =&gt; i.LastName == "Harui").ID, 
                    Location = "Gowan 27" },
                new OfficeAssignment { 
                    InstructorID = instructors.Single( i =&gt; i.LastName == "Kapoor").ID, 
                    Location = "Thompson 304" },
            };
            officeAssignments.ForEach(s =&gt; context.OfficeAssignments.AddOrUpdate(p =&gt; p.InstructorID, s));
            context.SaveChanges();

            AddOrUpdateInstructor(context, "Chemistry", "Kapoor");
            AddOrUpdateInstructor(context, "Chemistry", "Harui");
            AddOrUpdateInstructor(context, "Microeconomics", "Zheng");
            AddOrUpdateInstructor(context, "Macroeconomics", "Zheng");

            AddOrUpdateInstructor(context, "Calculus", "Fakhouri");
            AddOrUpdateInstructor(context, "Trigonometry", "Harui");
            AddOrUpdateInstructor(context, "Composition", "Abercrombie");
            AddOrUpdateInstructor(context, "Literature", "Abercrombie");

            context.SaveChanges();

            var enrollments = new List&lt;Enrollment&gt;
            {
                new Enrollment { 
                    StudentID = students.Single(s =&gt; s.LastName == "Alexander").ID, 
                    CourseID = courses.Single(c =&gt; c.Title == "Chemistry" ).CourseID, 
                    Grade = Grade.A 
                },
                 new Enrollment { 
                    StudentID = students.Single(s =&gt; s.LastName == "Alexander").ID,
                    CourseID = courses.Single(c =&gt; c.Title == "Microeconomics" ).CourseID, 
                    Grade = Grade.C 
                 },                            
                 new Enrollment { 
                    StudentID = students.Single(s =&gt; s.LastName == "Alexander").ID,
                    CourseID = courses.Single(c =&gt; c.Title == "Macroeconomics" ).CourseID, 
                    Grade = Grade.B
                 },
                 new Enrollment { 
                     StudentID = students.Single(s =&gt; s.LastName == "Alonso").ID,
                    CourseID = courses.Single(c =&gt; c.Title == "Calculus" ).CourseID, 
                    Grade = Grade.B 
                 },
                 new Enrollment { 
                     StudentID = students.Single(s =&gt; s.LastName == "Alonso").ID,
                    CourseID = courses.Single(c =&gt; c.Title == "Trigonometry" ).CourseID, 
                    Grade = Grade.B 
                 },
                 new Enrollment {
                    StudentID = students.Single(s =&gt; s.LastName == "Alonso").ID,
                    CourseID = courses.Single(c =&gt; c.Title == "Composition" ).CourseID, 
                    Grade = Grade.B 
                 },
                 new Enrollment { 
                    StudentID = students.Single(s =&gt; s.LastName == "Anand").ID,
                    CourseID = courses.Single(c =&gt; c.Title == "Chemistry" ).CourseID
                 },
                 new Enrollment { 
                    StudentID = students.Single(s =&gt; s.LastName == "Anand").ID,
                    CourseID = courses.Single(c =&gt; c.Title == "Microeconomics").CourseID,
                    Grade = Grade.B         
                 },
                new Enrollment { 
                    StudentID = students.Single(s =&gt; s.LastName == "Barzdukas").ID,
                    CourseID = courses.Single(c =&gt; c.Title == "Chemistry").CourseID,
                    Grade = Grade.B         
                 },
                 new Enrollment { 
                    StudentID = students.Single(s =&gt; s.LastName == "Li").ID,
                    CourseID = courses.Single(c =&gt; c.Title == "Composition").CourseID,
                    Grade = Grade.B         
                 },
                 new Enrollment { 
                    StudentID = students.Single(s =&gt; s.LastName == "Justice").ID,
                    CourseID = courses.Single(c =&gt; c.Title == "Literature").CourseID,
                    Grade = Grade.B         
                 }
            };

            foreach (Enrollment e in enrollments)
            {
                var enrollmentInDataBase = context.Enrollments.Where(
                    s =&gt;
                         s.Student.ID == e.StudentID &amp;&amp;
                         s.Course.CourseID == e.CourseID).SingleOrDefault();
                if (enrollmentInDataBase == null)
                {
                    context.Enrollments.Add(e);
                }
            }
            context.SaveChanges();
        }

        void AddOrUpdateInstructor(SchoolContext context, string courseTitle, string instructorName)
        {
            var crs = context.Courses.SingleOrDefault(c =&gt; c.Title == courseTitle);
            var inst = crs.Instructors.SingleOrDefault(i =&gt; i.LastName == instructorName);
            if (inst == null)
                crs.Instructors.Add(context.Instructors.Single(i =&gt; i.LastName == instructorName));
        }
    }
}

======================ORIGINAL ==================================

using System.Data.Entity;
using System.Data.Entity.SqlServer;

namespace ContosoUniversity.DAL
{
    public class SchoolConfiguration : DbConfiguration
    {
        public SchoolConfiguration()
        {
            SetExecutionStrategy("System.Data.SqlClient", () =&gt; new SqlAzureExecutionStrategy());
        }
    }
}</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>MVC_Parent_DateTime_client-side_server-side</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>AdamChurvs_Instructions</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>MVC_Parent_DateTime_client-side_server-side</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>AdamChurvs_Instructions</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>/*MVC_Parent_DateTime

There are a few differences between this solution and what I did in my Pluralsight course.

First, I'm using Newtonsoft's JSON.Net as the JavaScript serializer rather than the built-in one, mainly because newer versions of MVC are supposed to use it as well, mainly because it puts out ISO 8601 date strings rather than the ugly garbage we get with the current built-in JavaScript serializer.  I'm also using James Newton-King's JsonNetResult code for this.

Second, I'm using jQuery UI's datepicker because it's commonly used and readily available for free.  
	However, this causes a problem using the validation tooltip library I mention in the course, 
	because of an incompatibility between versions of jQuery and jQuery UI 
	that occurs when the minimum necessary version of jQuery is imported to support Bootstrap 3.  
	So no pretty validation balloons :(  I'll have to work around that one day; I just don't have the time right now.

Third, I'm using Moment.js for handling date-related stuff.

Fourth, I've created custom Knockout binding handlers for both the datepicker and the static display of the date.  Pay particular attention to how Moment.js uses format specifications versus how jQuery UI uses them.  Same display format; completely different specifications.

Fifth, I initialize StartDate as Now in the Create controller action so that mistakenly submitting a null date -- which resolves to Jan 1 1901 in the UI -- doesn't throw a data conversion error.

Sixth, though I have specified the default route to go to the list of Work Items that make up this example, you may end up going somewhere else depending upon what code is being edited when you F5, so I've put a link to Work Items in the header of the site.

Seventh, I treat the Delete and Display views as forms as well, rather than wiring-up the Delete button directly to the save() method in the client-side viewmodel.

Eighth, I've created a custom display template for the short date displayed in the Index view inside Views/Shared/DisplayTemplates.

Ninth, clicking Save leaves you on the Edit form because I needed to show that the -1 Day Bug fix was working.  Rewire to suit your own needed.


INSTRUCTIONS:

1. Set the Web project as the StartUp Project.

2. Rebuilt the solution.  I have NuGet Package Restore enabled, so rebuilding the solution will automatically download and configure everything you need that isn't already in there.

   NOTE: If you throw an error saying that you need to enable NuGet Package Restore then right-click the Solution node in Solution Explorer and choose Enable NuGet Package Restore and confirm the prompts, then rebuild again and it should work.  Contact me if it doesn't.  This is a bug in NuGet Package Restore.

3. Debug -&gt; Exceptions, add DateTimeExample.Web.ViewModels.ModelStateException as a CLR exception, de-select both checkboxes, and then click OK:  

4. In the Package Manager Console, choose the DataLayer project and then execute update-database -verbose to build and seed the database.  I always used the -verbose switch to see exactly what's going on.

5. F5, and enjoy! :)

6. Email me directly if you need any help.  

As always, I'm more than happy to help.  Just please don't distribute this to anyone, or write an article and take credit, okay?  I really appreciate it.



Respectfully,

Adam Churvis
President
Productivity Enhancement
</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>MVC_Parent-Child_EF_Knockout_Ajax_Validation</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>01_Parent__DATALAYER__Configuration.cs</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>MVC_Parent-Child_EF_Knockout_Ajax_Validation</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>01_Parent__DATALAYER__Configuration.cs</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>using SolutionName.Model;

//	/Migrations/Configuration.cs

namespace SolutionName.DataLayer.Migrations
	{
	using System;
	using System.Data.Entity;
	using System.Data.Entity.Migrations;
	using System.Linq;

	internal sealed class Configuration : DbMigrationsConfiguration&lt;SolutionName.DataLayer.SalesContext&gt;
		{
		public Configuration()
			{
			AutomaticMigrationsEnabled = true;
			AutomaticMigrationDataLossAllowed = true;	//added this line, since Migration will only be enabled during Development
			}

		protected override void Seed(SolutionName.DataLayer.SalesContext context)
			{
			context.SalesOrders.AddOrUpdate(
				so =&gt; so.CustomerName,
				new SalesOrder { CustomerName = "Adam", PONumber = "9876" },
				new SalesOrder { CustomerName = "Michael" },
				new SalesOrder { CustomerName = "David", PONumber = "Acme 9" }
				);
			}
		}
	}
</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>MVC_Parent-Child_EF_Knockout_Ajax_Validation</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>09_Child__DataLayer/Migrations/Configuration.cs</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>MVC_Parent-Child_EF_Knockout_Ajax_Validation</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>09_Child__DataLayer/Migrations/Configuration.cs</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>//09_Child__DataLayer/Migrations/Configuration.cs

using System.Data.Entity.Migrations;
using SolutionName.Model;

namespace SolutionName.DataLayer.Migrations
	{
		internal sealed class Configuration : DbMigrationsConfiguration&lt;SalesContext&gt;
		{
		public Configuration()
			{
			AutomaticMigrationsEnabled = true;
			AutomaticMigrationDataLossAllowed = true;
			}

		protected override void Seed(SalesContext context)
			{
			context.SalesOrders.AddOrUpdate(
				so =&gt; so.CustomerName,
				new SalesOrder
				{
					CustomerName = "Adam",
					PONumber = "9876",
					SalesOrderItems =
                    {
                        new SalesOrderItem{ProductCode = "ABC123", Quantity = 10, UnitPrice = 1.23m },
                        new SalesOrderItem{ProductCode = "XYZ987", Quantity = 7, UnitPrice = 14.57m },
                        new SalesOrderItem{ProductCode = "SAMPLE", Quantity = 3, UnitPrice = 15.00m }
                    }
				},
				new SalesOrder { CustomerName = "Michael" },
				new SalesOrder { CustomerName = "David", PONumber = "Acme 9" }
				);
			}
		}
	}</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>MVC_Parent-Child_EF_Knockout_Ajax_Validation</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>09_Chilld__about</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>MVC_Parent-Child_EF_Knockout_Ajax_Validation</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>09_Chilld__about</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>//09_Child__about - Adding Children

/* OVERVIEW
=====================================================
'Children' = 'SalesOrderItems'

1] Define and configure the 'SalesOrderItem' model
	- Collection in SalesOrder
	
2] Define the Server-side 'SalesOrderItemViewModel'
	- Collection in SalesOrderViewModel
	
3] Define the Client-side 'SalesOrderItemViewModel'
	- Knockout Mapping
		Configure KO to create one each time it encounters a SalesOrder item in the data

4] Create a Knockout Template (in the Partial Views)
	- to present the salesOrder items to the user
	
5] Identity keys for new SalesOrderItems

=====================================================
Summary:





/*09_Child  - Chapter: Adding Children - Define and Create the SalesOrderItem Model (child)
*/===================================================================================================/*

1] Create class called 'SalesOrderItem' in the MODEL project, that implements 'IObjectWithState'

	NOTE: it has both a FK property (SalesOrderId) and a NAV property (SalesOrder)
		BUT the NAV property is NOT 'virtual'!!!!

2] Create a class called "SalesOrderItemConfiguration" in the DATALAYER to configure the 'SalesOrderItem'
	- it implements: EntityTypeConfiguration&lt;SalesOrderItem&gt;
	In it's constructor:
	- it sets field properties, and Ignores 'ObjectState'

3] Then wire it up in the 'OnModelCreating' method of DATALAYER/SalesContext.cs
*/		modelBuilder.Configurations.Add(new SalesOrderItemConfiguration());
/*
	and add a DbSet into the 'SalesContext' class:					*/
	
	public DbSet&lt;SalesOrderItem&gt; SalesOrderItems { get; set; }
	
/*
Then rebuild.


/*09_Child  - Chapter: Adding Children - Define the Server-side the SalesOrderItemViewModel
*/===================================================================================================/*

1] Create class 'SalesOrderItemViewModel' in WEB/ViewModels that implements 'IObjectWithState'
	- add the property required to implement the interface: ObjectState
	- Get the properties from the MODEL/SalesOrderItem.cs, except the NAV property: SalesOrder
		So: it has a FK field, but it does not need the NAV property

/*09_Child  - Chapter: Adding Children - Define the Client-side  SalesOrderItemViewModel
*/===================================================================================================/*

Define the Client-side ViewModel for the Child in the same file (WEB/Scripts/salesorderviewmodel.js)
Initially stub in the function:			*/

SalesOrderItemViewModel = function (data) {
    var self = this;
    ko.mapping.fromJS(data, {}, self);
};

/*09_Child  - Chapter: Adding Children - Modify and Configure SalesOrder Model
*/===================================================================================================/*

Add a NAV Property to the Parent (MODEL/SalesOrder.cs) to reference the child:			*/

public virtual List&lt;SalesOrderItem&gt; SalesOrderItems { get; set; }
/*
When we maintain a collection, we need to intantiate it in the constructor to ensure that at the time
	that we instantiate the parent, that the collection of children is ready to use.			*/

    public class SalesOrder : IObjectWithState
    {
		public SalesOrder()
			{
			SalesOrderItems = new List&lt;SalesOrderItem&gt;();
			}
		...
    }
/*
/*09_Child  - Chapter: Adding Children - Modify and Reconfigure Server-Side SalesOrderViewModel
*/===================================================================================================/*

Now modify the SalesOrderViewModel to reflect the changes in the SalesOrder
We need a list of strongly typed 'SalesOrderItemViewModel', but will still call it 'SalesOrderItems'
	and it is NOT 'virtual' b/c that is not needed in a ViewModel
*/
 public List&lt;SalesOrderItemViewModel&gt; SalesOrderItems { get; set; }
/* and add code to initialize the collection of children in the constructor	*/

    public class SalesOrderViewModel: IObjectWithState
    {
        public SalesOrderViewModel()
        {
            SalesOrderItems = new List&lt;SalesOrderItemViewModel&gt;();
        }
        ....
    }

/*09_Child  - Chapter: Adding Children - Define the Child-Side Client Mapping
*/===================================================================================================/*

add a mapping to the file: "WEB/Scripts/salesorderviewmodel.js"	 to tell KO how to map the children (Items)	*/

var salesOrderItemMapping = {
    'SalesOrderItems': {                                                            //A) The mapping is for the collection 'SalesOrderItems' inside the SalesOrderViewModel
        key: function (salesOrderItem) {                                            // the The PK (i.e key) For each  Child (i.e. Parm-1 salesOrderItem) 
            return ko.utils.unwrapObservable(salesOrderItem.SalesOrderItemId);      // is 'salesOrderItem.SalesOrderItemId' (i.e. Parm-2)
        },                                                                          //'unwrapObservable' either unwraps an observable, or passes them through if they are a property
        create: function (options) {                                                //B) Tell KO what it has to do for each SalesOrderItem that it needs to create
            return new SalesOrderItemViewModel(options.data);                       //   In this case, it has to create a new 'SalesOrderItemViewModel', using the input data
        }                                                                           //   NOTE: that the 'data' is a property of the 'options' object
    }                                                                               //          The 'options' argument used to call to the Create callback
};                                                                                  //          is a Javascript object containing the members 'data' and 'parent'
/*                                                                                              is the parent object, or array to which the child belongs

Then use that function (salesOrderItemMapping) as the second parm in the call to 'fromJS:		*/

SalesOrderItemViewModel = function (data) {
    var self = this;
    ko.mapping.fromJS(data, salesOrderItemMapping, self);
};

/*
AND you have to use it for the second Parm of the 'SalesOrderViewModel' function that was created and used previously

See attached file: salesorderviewmodel.js

/*09_Child  - Chapter: Adding Children - Create Knockout Template for SalesOrderItem in Partial Views
*/===================================================================================================/*

1] Add a table for the Child into the partial view 'WEB/Views/Shared/_EditSalesOrder.cshtml'

a) Use the "class= "  attribute for formating
b) For the body of the table use a loop:  'foreach:' child 	
	with a 'data-bind' attribute
	NOTE: that since the loop is for each child, I can refer to the child's property directly in the 'data-bind' attribute
	  */

    &lt;tbody data-bind="foreach: SalesOrderItems"&gt;
        &lt;tr&gt;
            &lt;td class="form-group"&gt;&lt;input class="form-control input-sm" data-bind="value: ProductCode" /&gt;&lt;/td&gt;
            &lt;td class="form-group"&gt;&lt;input class="form-control input-sm" data-bind="value: Quantity" /&gt;&lt;/td&gt;
            &lt;td class="form-group"&gt;&lt;input class="form-control input-sm" data-bind="value: UnitPrice" /&gt;&lt;/td&gt;
            &lt;td class="form-group"&gt;Delete&lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
/*
c) for development of this project, add in some seed values for the child into DATALAYER/Migrations/Configuration.cs
d) update the database: 				*/
&gt; update-database -verbose
/*
Now have to get the data back between the models and viewmodels: 2:089 / 4:46

2] 	WEB/ViewModels/Helpers.cs
	-------------------------
	
a) [PARENT] Make 'ObjectState' part of the transfer by adding it in to the 'CreateSalesOrderViewModelFromSalesOrder' method

	BUT: Why would we want to set the ObjectState's value, in the ViewModel to the value of the ObjectState in th Model,
		since the purpose of the Server-side ViewModel that is created after SaveChanges is called is to return
		updated data to the client.
		
		IF every time we do that, we set ObjectState to 'Unchanged', why don't we do that as part of the 
		'CreateSalesOrderViewModelFromSalesOrder' method (in the Helper)

	In addition iterate over al the 'SalesOrderItem' in  the 'salesOrder.SalesOrderItems' collection (in the model)
	Create a 'SalesOrderItemViewModel' for each one
	:
	*/
        public static SalesOrderViewModel CreateSalesOrderViewModelFromSalesOrder(SalesOrder salesOrder)
        {
            ...
            salesOrderViewModel.ObjectState = ObjectState.Unchanged;

            foreach (SalesOrderItem salesOrderItem in salesOrder.SalesOrderItems)
            {
                SalesOrderItemViewModel salesOrderItemViewModel = new SalesOrderItemViewModel();	//Create SalesOrderItemViewModel
                salesOrderItemViewModel.SalesOrderItemId = salesOrderItem.SalesOrderItemId;
                salesOrderItemViewModel.ProductCode = salesOrderItem.ProductCode;
                ...
                salesOrderViewModel.ObjectState = ObjectState.Unchanged;			// &lt;== Set it to unchanged
                salesOrderViewModel.SalesOrderItems.Add(salesOrderItemViewModel);	//Then add it to the collection
            }

            return salesOrderViewModel;
        }
/*
b) [CHILD]

	Do the same as for the parent, but set 'ObjectState' to the value in the originating ViewModel
	*/

        public static SalesOrder CreateSalesOrderFromSalesOrderViewModel(SalesOrderViewModel salesOrderViewModel)
        {
            SalesOrder salesOrder = new SalesOrder();
            salesOrder.SalesOrderId = salesOrderViewModel.SalesOrderId;
            ...
            salesOrder.ObjectState = salesOrderViewModel.ObjectState;	// &lt;= Set equal to originating value

            int temporarySalesOrderItemId = -1; 

            foreach (SalesOrderItemViewModel salesOrderItemViewModel in salesOrderViewModel.SalesOrderItems)
            {
                SalesOrderItem salesOrderItem = new SalesOrderItem();
                salesOrderItem.ProductCode = salesOrderItemViewModel.ProductCode;
                ...
                salesOrderItem.ObjectState = salesOrderItemViewModel.ObjectState;
                ...
                salesOrderItem.SalesOrderId = salesOrderViewModel.SalesOrderId;
                
                salesOrder.SalesOrderItems.Add(salesOrderItem);
            }

            return salesOrder;
        }
/*

3] 	WEB/Controllers/SalesConrroller.cs
	-----------------------------------

	a) Edit {Action}
		remove this line b/c it is taken care of in the Helper:		*/

  			salesOrderViewModel.ObjectState = ObjectState.Unchanged;   
/*
	b) Delete {Action}
		remove this line b/c it is taken care of in the Helper:		*/

  			salesOrderViewModel.ObjectState = ObjectState.Deleted;      
/*
	b) Save {Action}
		remove this line b/c it is taken care of in the Helper:		*/
			salesOrder.ObjectState = salesOrderViewModel.ObjectState;
			//and
  			salesOrderViewModel.ObjectState = ObjectState.Unchanged;   
/*
4] Now adapt the strategy used for '_EditSalesOrder.cshtml' to the '_DisplaySalesOrder.cshtml'

	a) so copy the table for the child records from the '_EditSalesOrder.cshtml' to the '_DisplaySalesOrder.cshtml' 
		partial view, and change the &lt;input&gt; to &lt;span&gt;
	b) remove the Delete button from the child rows, and the column for the Add button, from the header part
		of th table
	c) and change the 'value' attributes to 'text' attributes


/*09_Child  - Chapter: Adding Children - Add a SalesOrderItem  (Child record)
*/===================================================================================================/*

1] change the placeholder text for the Add Button in the 'WEB/Views/Shared/_EditSalesOrder.cshtml' page
	to:			*/
	
	&lt;th&gt;&lt;button class="btn btn-info btn-xs" data-bind="click: addSalesOrderItem"&gt;Add&lt;/button&gt;&lt;/th&gt;
/*
	The function 'addSalesOrderItem' does not exist yet, but will shortly, on the SalesOrderViewModel
	
2] Add the function (addSalesOrderItem)  to WEB/Scripts/salesorderviewmodel.js 				
	Just push a new one, with default values onto the UA (UA=?)
	and make the ObjectState = 'ObjectState.Added'
	
	*/

    self.addSalesOrderItem = function () {
        var salesOrderItem = new SalesOrderItemViewModel({ SalesOrderItemId: 0, ProductCode: "", Quantity: 1, UnitPrice: 0, ObjectState: ObjectState.Added });
        self.SalesOrderItems.push(salesOrderItem);
    };
/*
3]Create a Helper to deal with this issue in the Save Method of the WEB/Controllers/SalesController.cs class:

In the line where we convert from object state to Entity State:			*/
_salesContext.ChangeTracker.Entries&lt;IObjectWithState&gt;().Single().State = DataLayer.Helpers.ConvertState(salesOrder.ObjectState);

/* we see that, That affects the SalesOrders (PARENTS) but not the SalesOrderItems (CHILDREN)

So we need to create a method in the DATALAYER/Helpers.cs file		*/

        public static void ApplyStateChanges(this DbContext context)
        {
            foreach (var entry in context.ChangeTracker.Entries&lt;IObjectWithState&gt;())
            {
                IObjectWithState stateInfo = entry.Entity;
                entry.State = ConvertState(stateInfo.ObjectState);
            }
        }
/*	

It takes a DbContext as a Parm, and iterates over each object that implements  IObjectWithState
	it calls convert state


So back in the Save action of the SalesController, replace the original explicit line for the generalized strategy in 'ApplyStateChanges' */

_salesContext.ApplyStateChanges();

/*
Now since so many properties are changing, after we call savechanges on the model, we need to refresh the 'salesOrderViewModel'
	before sending it back to the client.
So replace this line:						*/

salesOrderViewModel.SalesOrderId = salesOrder.SalesOrderId;

//With this line:

salesOrderViewModel = ViewModels.Helpers.CreateSalesOrderViewModelFromSalesOrder(salesOrder);

//Since the 'salesOrderViewModel' is being refresshed, it would always be the message for 'unchanged', so store the message 
//	in a string, and then add it back to the salesOrderViewModel after refreshing

// SO, replace these two lines:

salesOrderViewModel.MessageToClient = Helpers.GetMessageToClient(salesOrderViewModel.ObjectState, salesOrder.CustomerName);
salesOrderViewModel = ViewModels.Helpers.CreateSalesOrderViewModelFromSalesOrder(salesOrder);

// with this:

string messageToClient = ViewModels.Helpers.GetMessageToClient(salesOrderViewModel.ObjectState, salesOrder.CustomerName);
salesOrderViewModel = ViewModels.Helpers.CreateSalesOrderViewModelFromSalesOrder(salesOrder);
salesOrderViewModel.MessageToClient = messageToClient;


/*09_Child  - Chapter: Adding Children - How to Work with a Collection of New Identity Keys		
*/===================================================================================================/*

Up to this point, we can add a new child record, but if we add two, they would both have the same 
	faux primary key (i.e. they would be Zero) from the default values we set in the Javascript function (self.addSalesOrderItem)
	And multiple values can not have the same PK value (in EF), even if it is temporary and will be ignored on once the records are inserted
	into the database
		
DATALAYER/Helpers.cs
--------------------
The solution is to assign -1 to the first new record's PK (while in method: 'CreateSalesOrderFromSalesOrderViewModel') 
	Then if the ObjectState is NOT Added use the PK that comes with it, but
	if it is one that has just been added, then set equal to the temp value, which is initially -1, and decrement it for each new one
*/

				int temporarySalesOrderItemId = -1;
				...
				if (salesOrderItemViewModel.ObjectState != ObjectState.Added)
					salesOrderItem.SalesOrderItemId = salesOrderItemViewModel.SalesOrderItemId;
				else
					{
					salesOrderItem.SalesOrderItemId = temporarySalesOrderItemId;
					temporarySalesOrderItemId--;
					}
/*
	That way each new record will have a unique temporary value, that is not one of the positive and possibly real PK values.

NOTE: Once we implemented the ability to ADD child entities, we broke the ability to DELETE Parent entities, with related children
That will not be fixed for two more chapters






</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>MVC_Parent-Child_Examples</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>02_SeperateControllers_SeperateViews__/DAL/ProjectInitializer.cs</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>MVC_Parent-Child_Examples</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>02_SeperateControllers_SeperateViews__/DAL/ProjectInitializer.cs</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>//02_SeperateControllers_SeperateViews__/DAL/ProjectInitializer.cs

using System;
using System.Collections.Generic;
using System.Data.Entity;
using System.Data.Entity.Migrations;
using System.Data.Entity.Validation;
using System.Text;
using globalCommon;
using prj_0043_subprj_07.Models;

namespace prj_0043_subprj_07.DAL
{


    /*
     * To tell Entity Framework to use your initializer class, 
     *  add an element to the entityFramework element in the application Web.config file (the one in the root project folder), 
     *  as shown in the following example:
     * &lt;entityFramework&gt;
            &lt;contexts&gt;
              &lt;context type="DropDown.DAL.ProjectContext, DropDown"&gt;
                &lt;databaseInitializer type="DropDown.DAL.ProjectInitializer, DropDown" /&gt;
              &lt;/context&gt;
            &lt;/contexts&gt;
     * &lt;/entityFramework&gt;
     * (When you don't want EF to use the initializer, you can set an attribute on the context element: disableDatabaseInitialization="true".)
     * 
     * As an alternative to setting the initializer in the Web.config file is to do it in code by adding a Database.SetInitializer statement to the Application_Start method in in the Global.asax.cs file.
     * 
     * The application is now set up so that when you access the database for the first time in a given run of the application, 
     *      the Entity Framework compares the database to the model (ProjectContext and entity classes). 
     *      If there's a difference, the application drops and re-creates the database.
    */

    public class ProjectInitializer : DropCreateDatabaseIfModelChanges&lt;ProjectContext&gt;
    //public class ProjectInitializer : System.Data.Entity.DropCreateDatabaseAlways&lt;ProjectContext&gt;
    {

        protected override void Seed(ProjectContext context)
        {
            if (Global_BackAndFrontEnd.g_TEST)
            {


            }

            Random rnd = new Random();


            //======================================================================================================================
			//for (int i = 0; i &lt; 10; i++)
			//{
			//	context.HeaderRecords.AddOrUpdate(h =&gt; h.HeaderRecordID, new HeaderRecord
			//	{
			//		Header_Percent_Done_0_00_to_1_00 = (decimal?)(rnd.NextDouble()),
			//		Header_Name = rnd.Random_LowerCaseLetters(5),
			//		Header_PhoneNumber = rnd.Random_Numbers(10),
			//		Header_Category_Value = (Int16?)rnd.Next(0, 6),
			//		Header_bActive = ((short?)rnd.Next(0, 1) == 1),
			//		Header_Category_Description = rnd.Random_LowerCaseLetters(5),
			//		//DetailRecords =
			//		//	new List&lt;DetailRecord&gt;
			//		//	{

			//		//		new DetailRecord
			//		//		{
			//		//			Detail_Category_Value =  rnd.Next(1, 6),
			//		//			Detail_Name = rnd.Random_UpperCaseLetters(5),
			//		//		},
			//		//		new DetailRecord
			//		//		{
			//		//			Detail_Category_Value = rnd.Next(1, 6),
			//		//			Detail_Name = rnd.Random_UpperCaseLetters(5),
								
			//		//		}
			//		//	}
			//	});

			//}

		//	//SaveChanges(context);
		//	//======================================================================================================================
		//	var enums = new List&lt;enumTable&gt;
		//		{	        
		//		new enumTable { EnumDescription = "unknown", EnumValue = 0, EnumNameSpace_INT = 1 },
		//		new enumTable { EnumDescription = "Red", EnumValue = 1, EnumNameSpace_INT = 1 },
		//		new enumTable { EnumDescription = "Blue", EnumValue = 2, EnumNameSpace_INT = 1 },
		//		new enumTable { EnumDescription = "Green", EnumValue = 3, EnumNameSpace_INT = 1 },
		//		new enumTable { EnumDescription = "Indigo", EnumValue = 4, EnumNameSpace_INT = 1 },
		//		new enumTable { EnumDescription = "Violet", EnumValue = 5, EnumNameSpace_INT = 1 }
		//		};
		//	enums.ForEach(s =&gt; context.Enums.Add(s));
		//	SaveChanges(context);	//For debugging: DbEntityValidationException 

		//	//======================================================================================================================

		//	for (int i = 0; i &lt; 7; i++)
		//	{
		//		context.JustHeaders.AddOrUpdate(j =&gt; j.JustHeaderID, new JustHeader
		//		{
		//			Percent_Done_0_00_to_1_00 = (decimal?)(rnd.NextDouble()),
		//			JustHeaderName = rnd.Random_LowerCaseLetters(5),
		//			PhoneNumber = rnd.Random_Numbers(10),
		//			Category_Value = (Int16?)rnd.Next(0, 6),
		//			bActive = ((short?)rnd.Next(0, 1) == 1),
		//			Category_Description = rnd.Random_LowerCaseLetters(5)
		//		}
		//										);
		//	}

		//	SaveChanges(context);	//For debugging: DbEntityValidationException 
		//	//======================================================================================================================

		//	var principals = new List&lt;cPrincipal&gt;
		//		{	        
		//		new cPrincipal {principalName = "principal_1"},
		//		new cPrincipal { principalName = "principal_2"},
		//		new cPrincipal { principalName = "principal_3"},
		//		new cPrincipal { principalName = "principal_4"},
		//		new cPrincipal { principalName = "principal_5"},
		//		new cPrincipal { principalName = "principal_6"},
		//		};
		//	principals.ForEach(s =&gt; context.oPrincipals.Add(s));
		//	SaveChanges(context);	//For debugging: DbEntityValidationException 
		//	//======================================================================================================================
		//	var dependents = new List&lt;cDependent&gt;
		//		{	        
		//		new cDependent { dID = 1, DependentName = "dependent_2"},
		//		new cDependent { dID = 2, DependentName = "dependent_4"},
		//		new cDependent { dID = 3, DependentName = "dependent_6"},
		//		new cDependent { dID = 4, DependentName = "dependent_8"},
		//		};


		//	dependents.ForEach(s =&gt; context.ocDependents.Add(s));
		//	SaveChanges(context);	//For debugging: DbEntityValidationException 

		//	//======================================================================================================================
		//	var manys = new List&lt;cMany&gt;
		//		{	        
		//		new cMany() {mName = "Many_1"},
		//		new cMany() {mName = "Many_2"},
		//		new cMany() {mName = "Many_3"},
		//		new cMany() {mName = "Many_4"},
		//		new cMany() {mName = "Many_5"},
		//		new cMany() {mName = "Many_6"},
		//		};
		//	manys.ForEach(s =&gt; context.oManys.Add(s));
		//	SaveChanges(context);	//For debugging: DbEntityValidationException 
		//	//======================================================================================================================
		//	var ones = new List&lt;cOne&gt;
		//		{	        
		//		new cOne() { oneID = 2, oneName = "one_4"},
		//		new cOne() { oneID = 4, oneName = "one_8"},
		//		new cOne() { oneID = 6, oneName = "one_12"},
		//		new cOne() { oneID = 8, oneName = "one_16"},
		//		};


		//	ones.ForEach(s =&gt; context.oOnes.Add(s));
		//	SaveChanges(context);	//For debugging: DbEntityValidationException 

		//======================================================================================================================
		}

        /// &lt;summary&gt;
        /// Wrapper for SaveChanges adding the Validation Messages to the generated exception
        /// &lt;/summary&gt;
        /// &lt;param name="context"&gt;The context.&lt;/param&gt;
        private static void SaveChanges(DbContext context)
        {
            try
            {
                context.SaveChanges();
            }
            catch (DbEntityValidationException ex)
            {
                StringBuilder sb = new StringBuilder();

                foreach (var failure in ex.EntityValidationErrors)
                {
                    sb.AppendFormat("{0} failed validation\n", failure.Entry.Entity.GetType());
                    foreach (var error in failure.ValidationErrors)
                    {
                        sb.AppendFormat("- {0} : {1}", error.PropertyName, error.ErrorMessage);
                        sb.AppendLine();
                    }
                }

                throw new DbEntityValidationException(
                    "Entity Validation Failed - errors follow:\n" +
                    sb.ToString(), ex
                ); // Add the original exception as the innerException
            }
        }



    }
}
</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>MVC_Relation</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>01_One-to-OneOrZero_DropDown__/DAL/ProjectInitializer.cs</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>MVC_Relation</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>01_One-to-OneOrZero_DropDown__/DAL/ProjectInitializer.cs</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>//01_One-to-OneOrZero_DropDown__/DAL/ProjectInitializer.cs

using System;
using System.Collections.Generic;
using System.Data.Entity;
using System.Data.Entity.Migrations;
using System.Data.Entity.Validation;
using System.Linq;
using System.Text;
using System.Web;
using globalCommon;
using Microsoft.Ajax.Utilities;
using prj_0043_subprj_07.Models;


namespace prj_0043_subprj_07.DAL
{


    /*
     * To tell Entity Framework to use your initializer class, 
     *  add an element to the entityFramework element in the application Web.config file (the one in the root project folder), 
     *  as shown in the following example:
     * &lt;entityFramework&gt;
            &lt;contexts&gt;
              &lt;context type="DropDown.DAL.ProjectContext, DropDown"&gt;
                &lt;databaseInitializer type="DropDown.DAL.ProjectInitializer, DropDown" /&gt;
              &lt;/context&gt;
            &lt;/contexts&gt;
     * &lt;/entityFramework&gt;
     * (When you don't want EF to use the initializer, you can set an attribute on the context element: disableDatabaseInitialization="true".)
     * 
     * As an alternative to setting the initializer in the Web.config file is to do it in code by adding a Database.SetInitializer statement to the Application_Start method in in the Global.asax.cs file.
     * 
     * The application is now set up so that when you access the database for the first time in a given run of the application, 
     *      the Entity Framework compares the database to the model (ProjectContext and entity classes). 
     *      If there's a difference, the application drops and re-creates the database.
    */

    public class ProjectInitializer : System.Data.Entity.DropCreateDatabaseIfModelChanges&lt;ProjectContext&gt;
    //public class ProjectInitializer : System.Data.Entity.DropCreateDatabaseAlways&lt;ProjectContext&gt;
    {

        protected override void Seed(ProjectContext context)
        {

            Random rnd = new Random();


            //======================================================================================================================

            var principals = new List&lt;Models.cPrincipal&gt;
	            {	        
                new Models.cPrincipal {principalName = "principal_1"},
                new Models.cPrincipal { principalName = "principal_2"},
                new Models.cPrincipal { principalName = "principal_3"},
                new Models.cPrincipal { principalName = "principal_4"},
                new Models.cPrincipal { principalName = "principal_5"},
                new Models.cPrincipal { principalName = "principal_6"},
                };
            principals.ForEach(s =&gt; context.oPrincipals.Add(s));
            SaveChanges(context);	//For debugging: DbEntityValidationException 
            //======================================================================================================================
            var dependents = new List&lt;Models.cDependent&gt;
	            {	        
                new Models.cDependent { dID = 1, DependentName = "dependent_2"},
                new Models.cDependent { dID = 2, DependentName = "dependent_4"},
                new Models.cDependent { dID = 3, DependentName = "dependent_6"},
                new Models.cDependent { dID = 4, DependentName = "dependent_8"},
                };


            dependents.ForEach(s =&gt; context.ocDependents.Add(s));
            SaveChanges(context);	//For debugging: DbEntityValidationException 

            //======================================================================================================================


        }

        /// &lt;summary&gt;
        /// Wrapper for SaveChanges adding the Validation Messages to the generated exception
        /// &lt;/summary&gt;
        /// &lt;param name="context"&gt;The context.&lt;/param&gt;
        private static void SaveChanges(DbContext context)
        {
            try
            {
                context.SaveChanges();
            }
            catch (DbEntityValidationException ex)
            {
                StringBuilder sb = new StringBuilder();

                foreach (var failure in ex.EntityValidationErrors)
                {
                    sb.AppendFormat("{0} failed validation\n", failure.Entry.Entity.GetType());
                    foreach (var error in failure.ValidationErrors)
                    {
                        sb.AppendFormat("- {0} : {1}", error.PropertyName, error.ErrorMessage);
                        sb.AppendLine();
                    }
                }

                throw new DbEntityValidationException(
                    "Entity Validation Failed - errors follow:\n" +
                    sb.ToString(), ex
                ); // Add the original exception as the innerException
            }
        }



    }
}
</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>MVC_Relation</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>01_One-to-OneOrZero_DropDown__about</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>MVC_Relation</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>01_One-to-OneOrZero_DropDown__about</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>01_One-to-OneOrZero_DropDown__about

/*
NOTE: the drop-down is in: Dependent - Create View:

1] Created the entities: cPrincipal and cDependent
2] ProjectContext
	a) add in the DbSet for each entity
	b) add the modelBuilder statement into the OnModelCreating that defines the 
		the principal end of an association between the types
3] Add code to populate the entities in the Seed Method of ProjectInitializer class 
4] Rebuild the project
5] Create the Controllers:

	FIRST
	====
	Right click on the 'Controllers' folder =&gt; Add =&gt; New Scaffolded Item =&gt; MVC 5 Controller with views, using Entity Framework
	
										   Model Class:		cDependent (prj_0043_subprj_07.Models)
		rename the auto-populated Controller Name from: 	cDependentsController
													To: 	DependentController
	SECOND
	======
	Right click on the 'Controllers' folder =&gt; Add =&gt; New Scaffolded Item =&gt; MVC 5 Controller with views, using Entity Framework
	
										   Model Class:		cPrincipal (prj_0043_subprj_07.Models)
		rename the auto-populated Controller Name from: 	cPrincipalsController
													To: 	PrincipalController

When I build and run the application

There is a one-to-One relationship between the Header and the Detail 

Dependent requires a Principal, and gets the foreign key {	[dID] that REFERENCES [dbo].[cPrincipal] ([pID]) } 
Header is the principal and can exist without a Detail, and does not have the foreign key { rather it has a IDENTITY(1,1), clustered Primary Key}
{This relationship is set up in the modelBuilder statement of the 'OnModelCreating' in the 'ProjectContext'
	Or with the [Required] Attribute in the entity model definitions }
*/
======================Views======================================
/*
Principal - Index View:
------------------------
	DependentName principalName 	Edit|Details|Delete
	
Principal - Create View:
------------------------	
	principalName [ textbox ]	[Create]

Principal - Edit View:
------------------------	
	principalName [ textbox ]	[Save]
	
Principal - Details View:
------------------------	
	DependentName dependent_2 	{fieldname, value}
	principalName principal_1  	{fieldname, value}
	
*/
======================Views======================================
/*	
Dependent - Index View:
------------------------
	principalName DependentName  	Edit|Details|Delete				{reversed fieldname order}
	
Dependent - Create View:
------------------------
	dID [&lt; dropdown of list of Principal names &gt;] 	
	DependentName [ textbox ]	[Create]

Dependent - Edit View:
------------------------	
	DependentName [ textbox ]	[Save]
	
Dependent - Details View:
------------------------	
	principalName principal_1  	{fieldname, value}				{reversed fieldname order}	
	DependentName dependent_2 	{fieldname, value}	
	
==================================================================	

I can CREATE a Principal

The CREATE View  for the Dependent:
--------------------------------
	
	If I create a new DependentName, 
		I can assign it to a Principal that does not have something assigned to it, 
		but if I create a DependentName and assign it to a Principal that is already being used then I get a An exception of type 
		
			'{"Violation of PRIMARY KEY constraint 'PK_dbo.cDependent'. 
			Cannot insert duplicate key in object 'dbo.cDependent'. 
			The duplicate key value is (1).\r\nThe statement has been terminated."}
*/			
====================================================================
//FROM Seed Method
//[cDependent]
dID	DependentName
1	dependent_2
2	dependent_4
3	dependent_6
4	dependent_8
//[cPrincipal]
pID	principalName
1	principal_1
2	principal_2
3	principal_3
4	principal_4
5	principal_5
6	principal_6
</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>MVC_Relation</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>02_Many_to_One_DropDown__/DAL/ProjectInitializer.cs</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>MVC_Relation</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>02_Many_to_One_DropDown__/DAL/ProjectInitializer.cs</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>//02_Many_to_One_DropDown__/DAL/ProjectInitializer.cs


using System;
using System.Collections.Generic;
using System.Data.Entity;
using System.Data.Entity.Migrations;
using System.Data.Entity.Validation;
using System.Text;
using globalCommon;
using prj_0043_subprj_07.Models;

namespace prj_0043_subprj_07.DAL
{


    /*
     * To tell Entity Framework to use your initializer class, 
     *  add an element to the entityFramework element in the application Web.config file (the one in the root project folder), 
     *  as shown in the following example:
     * &lt;entityFramework&gt;
            &lt;contexts&gt;
              &lt;context type="DropDown.DAL.ProjectContext, DropDown"&gt;
                &lt;databaseInitializer type="DropDown.DAL.ProjectInitializer, DropDown" /&gt;
              &lt;/context&gt;
            &lt;/contexts&gt;
     * &lt;/entityFramework&gt;
     * (When you don't want EF to use the initializer, you can set an attribute on the context element: disableDatabaseInitialization="true".)
     * 
     * As an alternative to setting the initializer in the Web.config file is to do it in code by adding a Database.SetInitializer statement to the Application_Start method in in the Global.asax.cs file.
     * 
     * The application is now set up so that when you access the database for the first time in a given run of the application, 
     *      the Entity Framework compares the database to the model (ProjectContext and entity classes). 
     *      If there's a difference, the application drops and re-creates the database.
    */

    public class ProjectInitializer : DropCreateDatabaseIfModelChanges&lt;ProjectContext&gt;
    //public class ProjectInitializer : System.Data.Entity.DropCreateDatabaseAlways&lt;ProjectContext&gt;
    {

        protected override void Seed(ProjectContext context)
        {

            Random rnd = new Random();

            //======================================================================================================================
            var manys = new List&lt;cMany&gt;
	            {	        
                new cMany() {mName = "Many_1"},
                new cMany() {mName = "Many_2"},
                new cMany() {mName = "Many_3"},
                new cMany() {mName = "Many_4"},
                new cMany() {mName = "Many_5"},
                new cMany() {mName = "Many_6"},
                };
            manys.ForEach(s =&gt; context.oManys.Add(s));
            SaveChanges(context);	//For debugging: DbEntityValidationException 
            //======================================================================================================================
            var ones = new List&lt;cOne&gt;
	            {	        
                new cOne() { oneID = 2, oneName = "one_4"},
                new cOne() { oneID = 4, oneName = "one_8"},
                new cOne() { oneID = 6, oneName = "one_12"},
                new cOne() { oneID = 8, oneName = "one_16"},
                };


            ones.ForEach(s =&gt; context.oOnes.Add(s));
            SaveChanges(context);	//For debugging: DbEntityValidationException 

            //======================================================================================================================
        }

        /// &lt;summary&gt;
        /// Wrapper for SaveChanges adding the Validation Messages to the generated exception
        /// &lt;/summary&gt;
        /// &lt;param name="context"&gt;The context.&lt;/param&gt;
        private static void SaveChanges(DbContext context)
        {
            try
            {
                context.SaveChanges();
            }
            catch (DbEntityValidationException ex)
            {
                StringBuilder sb = new StringBuilder();

                foreach (var failure in ex.EntityValidationErrors)
                {
                    sb.AppendFormat("{0} failed validation\n", failure.Entry.Entity.GetType());
                    foreach (var error in failure.ValidationErrors)
                    {
                        sb.AppendFormat("- {0} : {1}", error.PropertyName, error.ErrorMessage);
                        sb.AppendLine();
                    }
                }

                throw new DbEntityValidationException(
                    "Entity Validation Failed - errors follow:\n" +
                    sb.ToString(), ex
                ); // Add the original exception as the innerException
            }
        }



    }
}
</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>MVC_Relation</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>02_Many_to_One_DropDown__about</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>MVC_Relation</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>02_Many_to_One_DropDown__about</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>//02_Many_to_One_DropDown__about


/*
Example of where to use this:
	Suppose People Entity has a field called Tie Color, where you want to be able to edit the tie color for people
	People is Many
	The Table of Tie colors is One
========================================

NOTE: the drop-down is in: 
	Many - Create View
	Many - Edit View

1] Created the entities: cMany and cOne
2] ProjectContext
	a) add in the DbSet for each entity
3] Add code to populate the entities in the Seed Method of ProjectInitializer class 
4] Rebuild the project
5] Create the Controllers:

	FIRST
	====
	Right click on the 'Controllers' folder =&gt; Add =&gt; New Scaffolded Item =&gt; MVC 5 Controller with views, using Entity Framework
	
										   Model Class:		cOne (prj_0043_subprj_07.Models)
		rename the auto-populated Controller Name from: 	cOnesController
													To: 	OneController
	SECOND
	======
	Right click on the 'Controllers' folder =&gt; Add =&gt; New Scaffolded Item =&gt; MVC 5 Controller with views, using Entity Framework
	
										   Model Class:		cMany (prj_0043_subprj_07.Models)
		rename the auto-populated Controller Name from: 	cManysController
													To: 	ManyController

When I build and run the application

There is a Many-to-One relationship between the cMany and the cOne entities 

*/		
====================================================================
//FROM Seed Method
//[cOne]
oneID	oneName
2	one_4
4	one_8
6	one_12
8	one_16
//[cMany]
mID	oneId	mName
1	NULL	Many_1
2	NULL	Many_2
3	NULL	Many_3
4	NULL	Many_4
5	NULL	Many_5
6	NULL	Many_6</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>MVC_Security</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>Creating_Records_in_Membership_Table_for_FormsBased_Authentication</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>MVC_Security</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>Creating_Records_in_Membership_Table_for_FormsBased_Authentication</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>Creating_Records_in_Membership_Table_for_FormsBased_Authentication

/*
Using the 'SimpleMembershipProvider' during the Seed method of a Migration

The question is when and where to seed the records, we could do it at Application Startup, but if you are using
Entity Framework, you can use Migration's "Seed" method


1] Modify the Configuration.cs file
add a SeedMembership Method, and invoke it in the Seed Method:

*/
namespace OdeToFood.Migrations
{
    using OdeToFood.Models;
    using System;
    using System.Collections.Generic;
    using System.Data.Entity;
    using System.Data.Entity.Migrations;
    using System.Linq;
    using System.Web.Security;    //required for WebSecurity.InitializeDatabaseConnection(...);
    using WebMatrix.WebData;        //Required for Roles

    internal sealed class Configuration : DbMigrationsConfiguration&lt;OdeToFood.Models.OdeToFoodDb&gt;
    {
        public Configuration()
        {
            AutomaticMigrationsEnabled = true;
        }

        protected override void Seed(OdeToFood.Models.OdeToFoodDb context)
        {
            context.Restaurants.AddOrUpdate(r =&gt; r.Name,
               new Restaurant { Name = "Sabatino's", City = "Baltimore", Country = "USA" },
               new Restaurant { Name = "Great Lake", City = "Chicago", Country = "USA" },
               new Restaurant
               {
                   Name = "Smaka",
                   City = "Gothenburg",
                   Country = "Sweden",
                   Reviews =
                       new List&lt;RestaurantReview&gt; { 
                       new RestaurantReview { Rating = 9, Body="Great food!", ReviewerName="Scott" }
                   }
               });

            for (int i = 0; i &lt; 1000; i++)
            {
                context.Restaurants.AddOrUpdate(r =&gt; r.Name,
                    new Restaurant { Name = i.ToString(), City = "Nowhere", Country = "USA" });
            }

            SeedMembership();

        }

        private void SeedMembership()
        {
            //To implement this submit &gt; Update-Database -Verbose {to the package manager}

            //Make sure the connection is set before proceeding.
            WebSecurity.InitializeDatabaseConnection("DefaultConnection", "UserProfile", "UserId", "UserName", autoCreateTables: true);

            //Get access to the current Role and Membership provider
            var roles = (SimpleRoleProvider)Roles.Provider;
            var membership = (SimpleMembershipProvider)Membership.Provider;

            //create role
            if (!roles.RoleExists("Admin"))
            {
                roles.CreateRole("Admin");
            }
            //Create Membership with username and password
            if (membership.GetUser("sallen", false) == null)
            {
                membership.CreateUserAndAccount("sallen", "imalittleteapot");
            }
            //Add sallen to admin role
            if (!roles.GetRolesForUser("sallen").Contains("Admin"))
            {
                roles.AddUsersToRoles(new[] { "sallen" }, new[] { "admin" });
            } 


        }
    }
}

/*
2] Then modify the root's Web.config file to allow this to work during the Migration, this code would be fine if
run from a web page, but when running it from the Package Manager it needs some modifications:
*/
PM&gt; Update-Database -Verbose
/* Unless the Web.config is modified it will generate this error: */
The Role Manager feature has not been enabled.
/* so in the &lt;system.web&gt; tag you need to add this: */

  &lt;system.web&gt;

	...

    &lt;roleManager enabled="true" defaultProvider="simple"&gt;
      &lt;providers&gt;
        &lt;clear/&gt;
        &lt;add name="simple" type="WebMatrix.WebData.SimpleRoleProvider,  
                             WebMatrix.WebData"/&gt;
      &lt;/providers&gt;
    &lt;/roleManager&gt;
    &lt;membership defaultProvider="simple"&gt;
      &lt;providers&gt;
        &lt;clear/&gt;
        &lt;add name="simple" type="WebMatrix.WebData.SimpleMembershipProvider, 
                             WebMatrix.WebData"/&gt;
      &lt;/providers&gt;
    &lt;/membership&gt;


  &lt;/system.web&gt;

/*
Now when you run the following command it seeds the Membership and roles 
*/
PM&gt; Update-Database -Verbose















</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>MVC_SQL-VIEW</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>01_DropDown_of_Employees_with_Email__/DAL/ProjectInitializer.cs</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>MVC_SQL-VIEW</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>01_DropDown_of_Employees_with_Email__/DAL/ProjectInitializer.cs</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>//01_DropDown_of_Employees_with_Email__/DAL/ProjectInitializer.cs

using System;
using System.Collections.Generic;
using System.Data.Entity;
using System.Data.Entity.Migrations;
using System.Data.Entity.Validation;
using System.IO;
using System.Text;
//using globalCommon;
//using prj_0043_subprj_07.Models;

namespace prj_0043_subprj_08.DAL
{

    /*
     * To tell Entity Framework to use your initializer class, 
     *  add an element to the entityFramework element in the application Web.config file (the one in the root project folder), 
     *  as shown in the following example:
     * &lt;entityFramework&gt;
            &lt;contexts&gt;
              &lt;context type="DropDown.DAL.ProjectContext, DropDown"&gt;
                &lt;databaseInitializer type="DropDown.DAL.ProjectInitializer, DropDown" /&gt;
              &lt;/context&gt;
            &lt;/contexts&gt;
     * &lt;/entityFramework&gt;
     * (When you don't want EF to use the initializer, you can set an attribute on the context element: disableDatabaseInitialization="true".)
     * 
     * As an alternative to setting the initializer in the Web.config file is to do it in code by adding a Database.SetInitializer statement to the Application_Start method in in the Global.asax.cs file.
     * 
     * The application is now set up so that when you access the database for the first time in a given run of the application, 
     *      the Entity Framework compares the database to the model (ProjectContext and entity classes). 
     *      If there's a difference, the application drops and re-creates the database.
    */

    //public class ProjectInitializer : DropCreateDatabaseIfModelChanges&lt;ProjectContext&gt;
    public class ProjectInitializer : System.Data.Entity.DropCreateDatabaseAlways&lt;ProjectContext&gt;
    {

        protected override void Seed(ProjectContext context)
        {
			////======================================================================================================================
            
            // Remove any constraints that might cause problems during the Delete phase
            foreach (var file in Directory.GetFiles(Path.Combine(AppDomain.CurrentDomain.BaseDirectory, "Sql\\Before"), "*.sql"))
            {
                context.Database.ExecuteSqlCommand(File.ReadAllText(file), new object[0]);
            }

            // Delete all stored procs, views
            foreach (var file in Directory.GetFiles(Path.Combine(AppDomain.CurrentDomain.BaseDirectory, "Sql\\Delete"), "*.sql"))
            {
                context.Database.ExecuteSqlCommand(File.ReadAllText(file), new object[0]);
            }

            // Add Stored Procedures, views
            foreach (var file in Directory.GetFiles(Path.Combine(AppDomain.CurrentDomain.BaseDirectory, "Sql\\Create"), "*.sql"))
            {
                context.Database.ExecuteSqlCommand(File.ReadAllText(file), new object[0]);
            }

            // Add any constraints that were deleted during the Before phase
            //foreach (var file in Directory.GetFiles(Path.Combine(AppDomain.CurrentDomain.BaseDirectory, "Sql\\After"), "*.sql"))
            //{
            //    context.Database.ExecuteSqlCommand(File.ReadAllText(file), new object[0]);
            //}

            ////======================================================================================================================
        }

        /// &lt;summary&gt;
        /// Wrapper for SaveChanges adding the Validation Messages to the generated exception
        /// &lt;/summary&gt;
        /// &lt;param name="context"&gt;The context.&lt;/param&gt;
        private static void SaveChanges(DbContext context)
        {
            try
            {
                context.SaveChanges();
            }
            catch (DbEntityValidationException ex)
            {
                StringBuilder sb = new StringBuilder();

                foreach (var failure in ex.EntityValidationErrors)
                {
                    sb.AppendFormat("{0} failed validation\n", failure.Entry.Entity.GetType());
                    foreach (var error in failure.ValidationErrors)
                    {
                        sb.AppendFormat("- {0} : {1}", error.PropertyName, error.ErrorMessage);
                        sb.AppendLine();
                    }
                }

                throw new DbEntityValidationException(
                    "Entity Validation Failed - errors follow:\n" +
                    sb.ToString(), ex
                ); // Add the original exception as the innerException
            }
        }



    }
}</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>MVC_StoredProcedure</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>02_EF6_codeFirst_StoredProcedure__/EntitiesMigrations/Configuration.cs</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>MVC_StoredProcedure</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>02_EF6_codeFirst_StoredProcedure__/EntitiesMigrations/Configuration.cs</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>//02_EF6_codeFirst_StoredProcedure__/EntitiesMigrations/Configuration.cs


namespace CodeFirstStoredProcedure.EntitiesMigrations
{
    using System;
    using System.Data.Entity;
    using System.Data.Entity.Migrations;
    using System.Linq;

    internal sealed class Configuration : DbMigrationsConfiguration&lt;CodeFirstStoredProcedure.EntitiesContext&gt;
    {
        public Configuration()
        {
            AutomaticMigrationsEnabled = false;
            MigrationsDirectory = @"EntitiesMigrations";
        }

        protected override void Seed(CodeFirstStoredProcedure.EntitiesContext context)
        {
            //  This method will be called after migrating to the latest version.

            //  You can use the DbSet&lt;T&gt;.AddOrUpdate() helper extension method 
            //  to avoid creating duplicate seed data. E.g.
            //
            //    context.People.AddOrUpdate(
            //      p =&gt; p.FullName,
            //      new Person { FullName = "Andrew Peters" },
            //      new Person { FullName = "Brice Lambson" },
            //      new Person { FullName = "Rowan Miller" }
            //    );
            //
        }
    }
}
</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>MVC</Category>
        <Language>TEXT</Language>
        <Public>false</Public>
        <Name>Rebuild_And_Reseed_Database</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>MVC</Category>
          <Language>TEXT</Language>
          <Public>false</Public>
          <Name>Rebuild_And_Reseed_Database</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>Rebuild, and reseed the Database
	This assumes that the database is already in place, and that we are making changes during development, such that we want to reseed the db
	Note to confirm that it rebuilt and reseeded correctly check for the existence of the following in the SQL database (MVC_02_MileageReimbursement)
		Views:
			vw_EmployeeEmail
			vw_SupervisorEmail
		Table:
			FundingProject {should be populated}
	1] For the Class: "ProjectInitializer" in ProjectInitializer.cs
		Normally during development the base is: CreateDatabaseIfNotExists
		But change it to: DropCreateDatabaseAlways
	2] Delete the Database from SQL Server
	3] Rebuild the solution in the Package Manager Console
		&gt;     add-migration InitialCreate -Force
	4] Update the database  in the Package Manager Console
		&gt;    update-database -verbose -Force
	{NOTE: at this point the database is rebuilt, but the seed has not run}
		Now start the application in Internet Explorer, and browse to the header index, or the create action view.
		This will run the seed method
	{Note at this point you should see the views and the seeded recorsds}
	Now change the Class: "ProjectInitializer" in ProjectInitializer.cs
		remove: DropCreateDatabaseAlways
		replace it with: CreateDatabaseIfNotExists
	Now if you run the application, it will no longer run the seed method.


Put the Database on a new SQL instance
	This assumes that we want to seed the db
	1] For the Class: "ProjectInitializer" in ProjectInitializer.cs
		Normally during development the base is: CreateDatabaseIfNotExists
		But change it to: DropCreateDatabaseAlways
	2] Delete the Database from SQL Server, if it is already there. In this case most likely it is not there.
	3] Rebuild the solution in the Package Manager Console
		&gt;     add-migration InitialCreate -Force
	4] Update the database  in the Package Manager Console
		&gt;    update-database -verbose -Force
	{NOTE: at this point the database is rebuilt, but the seed has not run}
	Now start the application in Internet Explorer, and browse to the header index, or the create action view.
		This will run the seed method
		{Note at this point you should see the views and the seeded recorsds}
	Now change the Class: "ProjectInitializer" in ProjectInitializer.cs
		remove: DropCreateDatabaseAlways
		replace it with: CreateDatabaseIfNotExists
	Now if you run the application, it will no longer run the seed method.
	Note to confirm that it rebuilt and reseeded correctly check for the existence of the following in the SQL database (MVC_02_MileageReimbursement)
		Views:
			vw_EmployeeEmail
			vw_SupervisorEmail
		Table:
			FundingProject {should be populated}
	NOTE: during debugging you can make 'true' the global variable: "gigDebugOn" which will direct all the emails to 
	</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>SOMEE.COM</Category>
        <Language>TEXT</Language>
        <Public>false</Public>
        <Name>ConnectionString_and_Web.config</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>SOMEE.COM</Category>
          <Language>TEXT</Language>
          <Public>false</Public>
          <Name>ConnectionString_and_Web.config</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>
1] I developed the Web Application and the SQL database locally.

	a) the Local Database was on the DEV Server as Database: "MVC_App_2"
	
	b) the connection string in Web.Config, on my development workstation was:
	
        &lt;add name="Project_Connection"
             connectionString="Data Source=SQL-SERVER\DEV;Initial Catalog=MVC_App_2;Integrated Security=True;Trusted_Connection=Yes;Pooling=False"
             providerName="System.Data.SqlClient" /&gt;
             
   c) I had to use this connection string for the Somee web.config:
   
    &lt;connectionStrings&gt;
         &lt;add name="Project_Connection"
             connectionString="workstation id=DEV1234.mssql.somee.com;packet size=4096;user id=glenngarson_SQLLogin_2;pwd=cgxk7uk72p;data source=DEV1234.mssql.somee.com;persist security info=False;initial catalog=DEV1234"
             providerName="System.Data.SqlClient" /&gt;
    &lt;/connectionStrings&gt;
             	
         

2] I seeded the SQL database (MVC_App_2), then detached it on the local server. 

4] I created an SQL database on the Somee platform called DEV1234

3] In the Somee interface, I browsed to my local MVC_App_2.mdf file and attached it to the DEV1234 remote database 

1] The connections string from Somee.com for my FiveHappiness DEV website, for the DEV1234 SQL database:


Connection string:  workstation id=DEV1234.mssql.somee.com;packet size=4096;user id=glenngarson_SQLLogin_2;pwd=cgxk7uk72p;data source=DEV1234.mssql.somee.com;persist security info=False;initial catalog=DEV1234  


Here is the Web.config for the remote application on Somee:

==============================================================



&lt;?xml version="1.0" encoding="utf-8"?&gt;
&lt;!--
  For more information on how to configure your ASP.NET application, please visit
  http://go.microsoft.com/fwlink/?LinkId=301880
  --&gt;
&lt;configuration&gt;
  &lt;configSections&gt;
    &lt;!-- For more information on Entity Framework configuration, visit http://go.microsoft.com/fwlink/?LinkID=237468 --&gt;
    &lt;section name="entityFramework" type="System.Data.Entity.Internal.ConfigFile.EntityFrameworkSection, EntityFramework, Version=6.0.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089" requirePermission="false" /&gt;
  &lt;/configSections&gt;
    &lt;connectionStrings&gt;
         &lt;add name="Project_Connection"
             connectionString="workstation id=DEV1234.mssql.somee.com;packet size=4096;user id=glenngarson_SQLLogin_2;pwd=cgxk7uk72p;data source=DEV1234.mssql.somee.com;persist security info=False;initial catalog=DEV1234"
             providerName="System.Data.SqlClient" /&gt;
    &lt;/connectionStrings&gt;
    &lt;appSettings&gt;
    &lt;add key="webpages:Version" value="3.0.0.0" /&gt;
    &lt;add key="webpages:Enabled" value="false" /&gt;
    &lt;add key="ClientValidationEnabled" value="true" /&gt;
    &lt;add key="UnobtrusiveJavaScriptEnabled" value="true" /&gt;
  &lt;/appSettings&gt;
  &lt;system.web&gt;
    &lt;compilation debug="true" targetFramework="4.5" /&gt;
    &lt;httpRuntime targetFramework="4.5" /&gt;
    &lt;customErrors mode="Off"/&gt;
  &lt;/system.web&gt;
  &lt;runtime&gt;
    &lt;assemblyBinding xmlns="urn:schemas-microsoft-com:asm.v1"&gt;
      &lt;dependentAssembly&gt;
        &lt;assemblyIdentity name="Newtonsoft.Json" culture="neutral" publicKeyToken="30ad4fe6b2a6aeed" /&gt;
        &lt;bindingRedirect oldVersion="0.0.0.0-6.0.0.0" newVersion="6.0.0.0" /&gt;
      &lt;/dependentAssembly&gt;
      &lt;dependentAssembly&gt;
        &lt;assemblyIdentity name="System.Web.Optimization" publicKeyToken="31bf3856ad364e35" /&gt;
        &lt;bindingRedirect oldVersion="1.0.0.0-1.1.0.0" newVersion="1.1.0.0" /&gt;
      &lt;/dependentAssembly&gt;
      &lt;dependentAssembly&gt;
        &lt;assemblyIdentity name="WebGrease" publicKeyToken="31bf3856ad364e35" /&gt;
        &lt;bindingRedirect oldVersion="0.0.0.0-1.5.2.14234" newVersion="1.5.2.14234" /&gt;
      &lt;/dependentAssembly&gt;
      &lt;dependentAssembly&gt;
        &lt;assemblyIdentity name="System.Web.Helpers" publicKeyToken="31bf3856ad364e35" /&gt;
        &lt;bindingRedirect oldVersion="1.0.0.0-3.0.0.0" newVersion="3.0.0.0" /&gt;
      &lt;/dependentAssembly&gt;
      &lt;dependentAssembly&gt;
        &lt;assemblyIdentity name="System.Web.WebPages" publicKeyToken="31bf3856ad364e35" /&gt;
        &lt;bindingRedirect oldVersion="1.0.0.0-3.0.0.0" newVersion="3.0.0.0" /&gt;
      &lt;/dependentAssembly&gt;
      &lt;dependentAssembly&gt;
        &lt;assemblyIdentity name="System.Web.Mvc" publicKeyToken="31bf3856ad364e35" /&gt;
        &lt;bindingRedirect oldVersion="1.0.0.0-5.2.2.0" newVersion="5.2.2.0" /&gt;
      &lt;/dependentAssembly&gt;
    &lt;/assemblyBinding&gt;
  &lt;/runtime&gt;
  &lt;entityFramework&gt;
      
      &lt;contexts&gt;
          &lt;context type="MVC_App_2.DAL.ProjectContext, MVC_App_2"&gt;
              &lt;databaseInitializer type="MVC_App_2.DAL.ProjectInitializer, MVC_App_2" /&gt;
          &lt;/context&gt;
      &lt;/contexts&gt;
      &lt;defaultConnectionFactory type="System.Data.Entity.Infrastructure.SqlConnectionFactory, EntityFramework" /&gt;
      &lt;providers&gt;
          &lt;provider invariantName="System.Data.SqlClient"
                    type="System.Data.Entity.SqlServer.SqlProviderServices, EntityFramework.SqlServer" /&gt;
      &lt;/providers&gt;     
      &lt;!--
    &lt;defaultConnectionFactory type="System.Data.Entity.Infrastructure.LocalDbConnectionFactory, EntityFramework"&gt;
      &lt;parameters&gt;
        &lt;parameter value="mssqllocaldb" /&gt;
      &lt;/parameters&gt;
    &lt;/defaultConnectionFactory&gt;
    &lt;providers&gt;
      &lt;provider invariantName="System.Data.SqlClient" type="System.Data.Entity.SqlServer.SqlProviderServices, EntityFramework.SqlServer" /&gt;
    &lt;/providers&gt;
    --&gt;
    
  &lt;/entityFramework&gt;
&lt;/configuration&gt;</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>SqlDataAdapter</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>01_Binding_to_Object_or_DataSet_using_SELECT__ObjectSource.cs</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>SqlDataAdapter</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>01_Binding_to_Object_or_DataSet_using_SELECT__ObjectSource.cs</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>using System.Collections.Generic;
using System.Linq;
using System.Windows.Forms;

namespace Data
{
    public class ObjectSource : ISource
    {
        private List&lt;Category&gt; _categories;
        private List&lt;Product&gt; _products;

        public ObjectSource()
        {
            _categories = new List&lt;Category&gt;();
            _categories.Add(new Category(1, "Beverages"));
            _categories.Add(new Category(2, "Condiments"));
            _categories.Add(new Category(3, "Confections"));
            _categories.Add(new Category(4, "Dairy Products"));
            _categories.Add(new Category(5, "Grains/Cereals"));
            _categories.Add(new Category(6, "Meat/Poultry"));
            _categories.Add(new Category(7, "Produce"));
            _categories.Add(new Category(8, "Seafood"));

            _products = new List&lt;Product&gt;();
            _products.Add(new Product(1, "Chai", 1, "10 boxes x 20 bags", 18.0m, 39, 0, false));
            _products.Add(new Product(2, "Chang", 1, "24 - 12 oz bottles", 19.0m, 17, 40, false));
            _products.Add(new Product(3, "Aniseed Syrup", 2, "12 - 550 ml bottles", 10.0m, 13, 70, false));
            _products.Add(new Product(4, "Chef Anton's Cajun Seasoning", 2, "48 - 6 oz jars", 22.0m, 53, 0, false));
            _products.Add(new Product(5, "Chef Anton's Gumbo Mix", 2, "36 boxes", 21.35m, 0, 0, true));
            _products.Add(new Product(6, "Grandma's Boysenberry Spread", 2, "12 - 8 oz jars", 25.0m, 120, 0, false));
            _products.Add(new Product(7, "Uncle Bob's Organic Dried Pears", 7, "12 - 1 lb pkgs.", 30.0m, 15, 0, false));
            _products.Add(new Product(8, "Northwoods Cranberry Sauce", 2, "12 - 12 oz jars", 40.0m, 6, 0, false));
            _products.Add(new Product(9, "Mishi Kobe Niku", 6, "18 - 500 g pkgs.", 97.0m, 29, 0, true));
            _products.Add(new Product(10, "Ikura", 8, "12 - 200 ml jars", 31.0m, 31, 0, false));
            _products.Add(new Product(11, "Queso Cabrales", 4, "1 kg pkg.", 21.0m, 22, 30, false));
            _products.Add(new Product(12, "Queso Manchego La Pastora", 4, "10 - 500 g pkgs.", 38.0m, 86, 0, false));
            _products.Add(new Product(13, "Konbu", 8, "2 kg box", 6.0m, 24, 0, false));
            _products.Add(new Product(14, "Tofu", 7, "40 - 100 g pkgs.", 23.25m, 35, 0, false));
            _products.Add(new Product(15, "Genen Shouyu", 2, "24 - 250 ml bottles", 15.5m, 39, 0, false));
            _products.Add(new Product(16, "Pavlova", 3, "32 - 500 g boxes", 17.45m, 29, 0, false));
            _products.Add(new Product(17, "Alice Mutton", 6, "20 - 1 kg tins", 39.0m, 0, 0, true));
            _products.Add(new Product(18, "Carnarvon Tigers", 8, "16 kg pkg.", 62.5m, 42, 0, false));
            _products.Add(new Product(19, "Teatime Chocolate Biscuits", 3, "10 boxes x 12 pieces", 9.2m, 25, 0, false));
            _products.Add(new Product(20, "Sir Rodney's Marmalade", 3, "30 gift boxes", 81.0m, 40, 0, false));
            _products.Add(new Product(21, "Sir Rodney's Scones", 3, "24 pkgs. x 4 pieces", 10.0m, 3, 40, false));
            _products.Add(new Product(22, "Gustaf's Knäckebröd", 5, "24 - 500 g pkgs.", 21.0m, 104, 0, false));
            _products.Add(new Product(23, "Tunnbröd", 5, "12 - 250 g pkgs.", 9.0m, 61, 0, false));
            _products.Add(new Product(24, "Guaraná Fantástica", 1, "12 - 355 ml cans", 4.5m, 20, 0, true));
            _products.Add(new Product(25, "NuNuCa Nuß-Nougat-Creme", 3, "20 - 450 g glasses", 14.0m, 76, 0, false));
            _products.Add(new Product(26, "Gumbär Gummibärchen", 3, "100 - 250 g bags", 31.23m, 15, 0, false));
            _products.Add(new Product(27, "Schoggi Schokolade", 3, "100 - 100 g pieces", 43.9m, 49, 0, false));
            _products.Add(new Product(28, "Rössle Sauerkraut", 7, "25 - 825 g cans", 45.6m, 26, 0, true));
            _products.Add(new Product(29, "Thüringer Rostbratwurst", 6, "50 bags x 30 sausgs.", 123.79m, 0, 0, true));
            _products.Add(new Product(30, "Nord-Ost Matjeshering", 8, "10 - 200 g glasses", 25.89m, 10, 0, false));
            _products.Add(new Product(31, "Gorgonzola Telino", 4, "12 - 100 g pkgs", 12.5m, 0, 70, false));
            _products.Add(new Product(32, "Mascarpone Fabioli", 4, "24 - 200 g pkgs.", 32.0m, 9, 40, false));
            _products.Add(new Product(33, "Geitost", 4, "500 g", 2.5m, 112, 0, false));
            _products.Add(new Product(34, "Sasquatch Ale", 1, "24 - 12 oz bottles", 14.0m, 111, 0, false));
            _products.Add(new Product(35, "Steeleye Stout", 1, "24 - 12 oz bottles", 18.0m, 20, 0, false));
            _products.Add(new Product(36, "Inlagd Sill", 8, "24 - 250 g  jars", 19.0m, 112, 0, false));
            _products.Add(new Product(37, "Gravad lax", 8, "12 - 500 g pkgs.", 26.0m, 11, 50, false));
            _products.Add(new Product(38, "Côte de Blaye", 1, "12 - 75 cl bottles", 263.5m, 17, 0, false));
            _products.Add(new Product(39, "Chartreuse verte", 1, "750 cc per bottle", 18.0m, 69, 0, false));
            _products.Add(new Product(40, "Boston Crab Meat", 8, "24 - 4 oz tins", 18.4m, 123, 0, false));
            _products.Add(new Product(41, "Jack's New England Clam Chowder", 8, "12 - 12 oz cans", 9.65m, 85, 0, false));
            _products.Add(new Product(42, "Singaporean Hokkien Fried Mee", 5, "32 - 1 kg pkgs.", 14.0m, 26, 0, true));
            _products.Add(new Product(43, "Ipoh Coffee", 1, "16 - 500 g tins", 46.0m, 17, 10, false));
            _products.Add(new Product(44, "Gula Malacca", 2, "20 - 2 kg bags", 19.45m, 27, 0, false));
            _products.Add(new Product(45, "Rogede sild", 8, "1k pkg.", 9.5m, 5, 70, false));
            _products.Add(new Product(46, "Spegesild", 8, "4 - 450 g glasses", 12.0m, 95, 0, false));
            _products.Add(new Product(47, "Zaanse koeken", 3, "10 - 4 oz boxes", 9.5m, 36, 0, false));
            _products.Add(new Product(48, "Chocolade", 3, "10 pkgs.", 12.75m, 15, 70, false));
            _products.Add(new Product(49, "Maxilaku", 3, "24 - 50 g pkgs.", 20.0m, 10, 60, false));
            _products.Add(new Product(50, "Valkoinen suklaa", 3, "12 - 100 g bars", 16.25m, 65, 0, false));
            _products.Add(new Product(51, "Manjimup Dried Apples", 7, "50 - 300 g pkgs.", 53.0m, 20, 0, false));
            _products.Add(new Product(52, "Filo Mix", 5, "16 - 2 kg boxes", 7.0m, 38, 0, false));
            _products.Add(new Product(53, "Perth Pasties", 6, "48 pieces", 32.8m, 0, 0, true));
            _products.Add(new Product(54, "Tourtière", 6, "16 pies", 7.45m, 21, 0, false));
            _products.Add(new Product(55, "Pâté chinois", 6, "24 boxes x 2 pies", 24.0m, 115, 0, false));
            _products.Add(new Product(56, "Gnocchi di nonna Alice", 5, "24 - 250 g pkgs.", 38.0m, 21, 10, false));
            _products.Add(new Product(57, "Ravioli Angelo", 5, "24 - 250 g pkgs.", 19.5m, 36, 0, false));
            _products.Add(new Product(58, "Escargots de Bourgogne", 8, "24 pieces", 13.25m, 62, 0, false));
            _products.Add(new Product(59, "Raclette Courdavault", 4, "5 kg pkg.", 55.0m, 79, 0, false));
            _products.Add(new Product(60, "Camembert Pierrot", 4, "15 - 300 g rounds", 34.0m, 19, 0, false));
            _products.Add(new Product(61, "Sirop d'érable", 2, "24 - 500 ml bottles", 28.5m, 113, 0, false));
            _products.Add(new Product(62, "Tarte au sucre", 3, "48 pies", 49.3m, 17, 0, false));
            _products.Add(new Product(63, "Vegie-spread", 2, "15 - 625 g jars", 43.9m, 24, 0, false));
            _products.Add(new Product(64, "Wimmers gute Semmelknödel", 5, "20 bags x 4 pieces", 33.25m, 22, 80, false));
            _products.Add(new Product(65, "Louisiana Fiery Hot Pepper Sauce", 2, "32 - 8 oz bottles", 21.05m, 76, 0, false));
            _products.Add(new Product(66, "Louisiana Hot Spiced Okra", 2, "24 - 8 oz jars", 17.0m, 4, 100, false));
            _products.Add(new Product(67, "Laughing Lumberjack Lager", 1, "24 - 12 oz bottles", 14.0m, 52, 0, false));
            _products.Add(new Product(68, "Scottish Longbreads", 3, "10 boxes x 8 pieces", 12.5m, 6, 10, false));
            _products.Add(new Product(69, "Gudbrandsdalsost", 4, "10 kg pkg.", 36.0m, 26, 0, false));
            _products.Add(new Product(70, "Outback Lager", 1, "24 - 355 ml bottles", 15.0m, 15, 10, false));
            _products.Add(new Product(71, "Flotemysost", 4, "10 - 500 g pkgs.", 21.5m, 26, 0, false));
            _products.Add(new Product(72, "Mozzarella di Giovanni", 4, "24 - 200 g pkgs.", 34.8m, 14, 0, false));
            _products.Add(new Product(73, "Röd Kaviar", 8, "24 - 150 g jars", 15.0m, 101, 0, false));
            _products.Add(new Product(74, "Longlife Tofu", 7, "5 kg pkg.", 10.0m, 4, 20, false));
            _products.Add(new Product(75, "Rhönbräu Klosterbier", 1, "24 - 0.5 l bottles", 7.75m, 125, 0, false));
            _products.Add(new Product(76, "Lakkalikööri", 1, "500 ml", 18.0m, 57, 0, false));
            _products.Add(new Product(77, "Original Frankfurter grüne Soße", 2, "12 boxes", 13.0m, 32, 0, false));
        }

        public object GetCategories()
        {
            return _categories;
        }

        public object GetProducts(int categoryId)
        {
            var result = from p in _products
                         where p.CategoryId == categoryId
                         select p;

            return result.ToList();
        }

        public void DeleteProduct(BindingSource bindingSource, int productId)
        {
            var query = from p in _products
                        where p.ProductID == productId
                        select p;
            var product = query.Single();

            bindingSource.Remove(product);
        }

        public void AddProduct(BindingSource bindingSource, Product product)
        {
            var maxId = (
                from p in _products
                select p).Max(p =&gt; p.ProductID);
            product.ProductID = maxId + 1;

            bindingSource.Add(product);
        }

        public void Save()
        {
            //Nothing to do
        }
    }
}
</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>STRING_custom_Functions</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>RandomStrings</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>STRING_custom_Functions</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>RandomStrings</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>//from: http://stackoverflow.com/questions/1122483/random-string-generator-returning-same-string

/*
You're instantiating the Random object inside your method.

The Random object is seeded from the system clock, 
which means that if you call your method several times in quick succession it'll use the same seed each time, 
which means that it'll generate the same sequence of random numbers, which means that you'll get the same string.

To solve the problem, 
move your Random instance outside of the method itself (Or make it static and internal to the class)
	(and while you're at it you could get rid of that crazy sequence of calls to Convert and Floor and NextDouble):
*/

private readonly Random _rng = new Random();
private const string _chars = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789";

private string RandomString(int size)
{
    char[] buffer = new char[size];

    for (int i = 0; i &lt; size; i++)
    {
        buffer[i] = _chars[_rng.Next(_chars.Length)];
    }
    return new string(buffer);
}

==========================================================================
//To get 4 random characters with minimum of 0 special characters-

Membership.GeneratePassword(4, 0)

//Note that in 4.0 the second integer parameter denotes the minimum number of nonAlphaNumericCharacters to use.
//So Membership.GeneratePassword(10, 0);
// won't work quite the way you think, it still puts in loads of non-alphanumeric characters, eg: z9sge)?pmV

==========================================================================================================

int yourRandomStringLength = 12; //maximum: 32
Guid.NewGuid().ToString("N").Substring(0, yourRandomStringLength);

//PS: Please keep in mind that yourRandomStringLength cannot exceed 32 as Guid has max length of 32.
//this is not random, but rather unique
===========================================================================================================

public System.String GetRandomString(System.Int32 length)
{
    System.Byte[] seedBuffer = new System.Byte[4];
    using (var rngCryptoServiceProvider = new System.Security.Cryptography.RNGCryptoServiceProvider())
    {
        rngCryptoServiceProvider.GetBytes(seedBuffer);
        System.String chars = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789";
        System.Random random = new System.Random(System.BitConverter.ToInt32(seedBuffer, 0));
        return new System.String(Enumerable.Repeat(chars, length).Select(s =&gt; s[random.Next(s.Length)]).ToArray());
    }
}
=============================================================================================================
public string GenRandString(int length)
{
  byte[] randBuffer = new byte[length];
  RandomNumberGenerator.Create().GetBytes(randBuffer);
  return System.Convert.ToBase64String(randBuffer).Remove(length);
}
=============================================================================================================

private static readonly Random _rand = new Random();

/// &lt;summary&gt;
/// Generate a random string.
/// &lt;/summary&gt;
/// &lt;param name="length"&gt;The length of random string. The minimum length is 3.&lt;/param&gt;
/// &lt;returns&gt;The random string.&lt;/returns&gt;
public string RandomString(int length)
{
    length = Math.Max(length, 3);

    byte[] bytes = new byte[length];
    _rand.NextBytes(bytes);
    return Convert.ToBase64String(bytes).Substring(0, length);
}

=================================================================================================================
//A LINQ one-liner for good measure (assuming a private static Random Random)...

public static string RandomString(int length)
{
    return new string(Enumerable.Range(0, length).Select(_ =&gt; (char)Random.Next('a', 'z')).ToArray());
}


=============================================================================================================
using System.IO;   
public static string RandomStr()

{
    string rStr = Path.GetRandomFileName();
    rStr = rStr.Replace(".", ""); // For Removing the .
    return rStr;
}

/*
This creates files on the disk. From MSDN: 
The GetTempFileName method will raise an IOException if it is used to create more than 65535 files without deleting previous temporary files. 
The GetTempFileName method will raise an IOException if no unique temporary file name is available. 
To resolve this error, delete all unneeded temporary files. 

"The GetRandomFileName method returns a cryptographically strong, 
random string that can be used as either a folder name or a file name.
 Unlike GetTempFileName, GetRandomFileName does not create a file.
  When the security of your file system is paramount, 
  this method should be used instead of GetTempFileName." 
  We are talking about GetRandomFileName() not GetTempFileName().

*/
-----------------------------------------------------------------------------------
//Also, you can get a random uppercase letter by using 
ch = (char)random.Next('A','Z'); 
//a lot simpler than the unreadable line 
ch = Convert.ToChar(Convert.ToInt32(Math.Floor(26 * random.NextDouble() + 65))); 
//from the original post. Then if you want to switch it to lowercase, you can easily switch to 
(char)random.Next('a','z'); 
</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>Table</Category>
        <Language>SQLSERVER2K SQL</Language>
        <Public>false</Public>
        <Name>DELETE_CREATE_with_ForeignKeys</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>Table</Category>
          <Language>SQLSERVER2K SQL</Language>
          <Public>false</Public>
          <Name>DELETE_CREATE_with_ForeignKeys</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>--prj_8011_CreateTable_LNK06_T1_T12.sql

USE [DB2_CRM]
GO


SET ANSI_NULLS ON
GO

SET QUOTED_IDENTIFIER ON
GO

IF OBJECT_ID('dbo.LNK06_T1_T12') IS NOT NULL
    DROP TABLE dbo.LNK06_T1_T12

go

CREATE TABLE [dbo].[LNK06_T1_T12]
(
		[LNK06_PK] [int] IDENTITY(1,1) NOT NULL,		--You must specify both the seed and increment or neither. If neither is specified, the default is (1,1).
		[T1_FK] [int] NULL,
		[T12_FK] [int] NULL,
		[LNK06_T1_T12_RowVersion] [timestamp] NULL
	,CONSTRAINT [LNK06_T1_T12$LNK06_PK] PRIMARY KEY NONCLUSTERED 
		(
			[LNK06_PK] ASC
		)
		WITH 
				(
					PAD_INDEX  = OFF, 
					STATISTICS_NORECOMPUTE  = OFF, 
					IGNORE_DUP_KEY = OFF, 
					ALLOW_ROW_LOCKS  = ON, 
					ALLOW_PAGE_LOCKS  = ON
				) ON [PRIMARY]
	--,CONSTRAINT LNK06_T1_T12$T1_FK FOREIGN KEY (T1_FK) 
	--	REFERENCES  [dbo].[T1_Practice] ([T1_PK])
	--	ON DELETE CASCADE
	--	ON UPDATE CASCADE
)

GO
ALTER TABLE [dbo].[LNK06_T1_T12] 
ADD CONSTRAINT LNK06_T1_T12$T1_FK FOREIGN KEY (T1_FK) 
    REFERENCES [dbo].[T1_Practice] ([T1_PK])
    ON DELETE CASCADE
    ON UPDATE CASCADE
;
GO
ALTER TABLE [dbo].[LNK06_T1_T12] 
ADD CONSTRAINT LNK06_T1_T12$T12_FK FOREIGN KEY (T12_FK) 
    REFERENCES [dbo].[T12_NotEmployees] ([T12_PK])
    ON DELETE CASCADE
    ON UPDATE CASCADE
;
GO
</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>TroubleShooting</Category>
        <Language>SQLSERVER2K SQL</Language>
        <Public>false</Public>
        <Name>BlitzIndex.sql</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>TroubleShooting</Category>
          <Language>SQLSERVER2K SQL</Language>
          <Public>false</Public>
          <Name>BlitzIndex.sql</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>--prj_000_303_BlitzIndex.sql

SET ANSI_NULLS ON;
SET ANSI_PADDING ON;
SET ANSI_WARNINGS ON;
SET ARITHABORT ON;
SET CONCAT_NULL_YIELDS_NULL ON;
SET QUOTED_IDENTIFIER ON;
SET STATISTICS IO OFF;
SET STATISTICS TIME OFF;
GO

USE master;
GO

IF OBJECT_ID('dbo.sp_BlitzIndex') IS NOT NULL 
	DROP PROCEDURE dbo.sp_BlitzIndex;
GO

CREATE PROCEDURE dbo.sp_BlitzIndex
	@DatabaseName NVARCHAR(128) = null, /*Defaults to current DB if not specified*/
	@Mode tinyint=0, /*0=diagnose, 1=Summarize, 2=Index Usage Detail, 3=Missing Index Detail*/
	@SchemaName NVARCHAR(128) = NULL, /*Requires table_name as well.*/
	@TableName NVARCHAR(128) = NULL,  /*Requires schema_name as well.*/
		/*Note:@Mode doesn't matter if you're specifying schema_name and @TableName.*/
	@Filter tinyint = 0 /* 0=no filter (default). 1=No low-usage warnings for objects with 0 reads. 2=Only warn for objects &gt;= 500MB */
		/*Note:@Filter doesn't do anything unless @Mode=0*/
/*
sp_BlitzIndex(TM) v2.02 - Jan 30, 2014

(C) 2014, Brent Ozar Unlimited(TM). 
See http://BrentOzar.com/go/eula for the End User Licensing Agreement.

For help and how-to info, visit http://www.BrentOzar.com/BlitzIndex

How to use:
--	Diagnose:
		EXEC dbo.sp_BlitzIndex @DatabaseName='AdventureWorks';
--	Return detail for a specific table:
		EXEC dbo.sp_BlitzIndex @DatabaseName='AdventureWorks', @SchemaName='Person', @TableName='Person';

Known limitations of this version:
 - Does not include FULLTEXT indexes. (A possibility in the future, let us know if you're interested.)
 - Index create statements are just to give you a rough idea of the syntax. It includes filters and fillfactor.
 --		Example 1: index creates use ONLINE=? instead of ONLINE=ON / ONLINE=OFF. This is because it's important for the user to understand if it's going to be offline and not just run a script.
 --		Example 2: they do not include all the options the index may have been created with (padding, compression filegroup/partition scheme etc.)
 --		(The compression and filegroup index create syntax isn't trivial because it's set at the partition level and isn't trivial to code. Two people have voted for wanting it so far.)
 - Doesn't advise you about data modeling for clustered indexes and primary keys (primarily looks for signs of insanity.)
 - Found something? Let us know at help@brentozar.com.

 Thanks for using sp_BlitzIndex(TM)!
 Sincerely,
 The Humans of Brent Ozar Unlimited(TM)

CHANGE LOG (last five versions):
	Jan 30, 2014 (v2.02)
		Standardized calling parameters with sp_AskBrent(TM) and sp_BlitzIndex(TM). (@DatabaseName instead of @database_name, etc)
		Added check_id 80 and 81-- what appear to be the most frequently used indexes (workaholics)
		Added index_operational_stats info to table level output -- recent scans vs lookups
		Broke index_usage_stats output into two categories, scans and lookups (also in table level output)
		Changed db name, table name, index name to 128 length
		Fixed findings_group column length in #BlitzIndexResults (fixed issues for users w/ longer db names)
		Fixed issue where identities nearing end of range were only detected if the check was run with a specific db context
			Fixed extra tab in @SchemaName= that made pasting into Excel awkward/wrong
		Added abnormal psychology check for clustered columnstore indexes (and general support for detecting them)
		Standardized underscores in create TSQL for missing indexes
		Better error message when running in table mode and the table isn't found.
		Added current timestamp to the header based on user request. (Didn't add startup time-- sorry! Too many things reset usage info, don't want to mislead anyone.)
		Added fillfactor to index create statements.
		Changed all index create statements to ONLINE=?, SORT_IN_TEMPDB=?. The user should decide at index create time what's right for them.
	May 26, 2013 (v2.01)
		Added check_id 28: Non-unqiue clustered indexes. (This should have been checked in for an earlier version, it slipped by).
	May 14, 2013 (v2.0) - Added data types and max length to all columns (keys, includes, secret columns)
		Set sp_blitz to default to current DB if database_name is not specified when called
		Added @Filter:  
			0=no filter (default)
			1=Don't throw low-usage warnings for objects with 0 reads (helpful for dev/non-production environments)
			2=Only report on objects &gt;= 250MB (helps focus on larger indexes). Still runs a few database-wide checks as well.
		Added list of all columns and types in table for runs using: @DatabaseName, @SchemaName, @TableName
		Added count of total number of indexes a column is part of.
		Added check_id 25: Addicted to nullable columns. (All or all but one column is nullable.)
		Added check_id 66 and 67 to flag tables/indexes created within 1 week or modified within 48 hours.
		Added check_id 26: Wide tables (35+ cols or &gt; 2000 non-LOB bytes).
		Added check_id 27: Addicted to strings. Looks for tables with 4 or more columns, of which all or all but one are string or LOB types.
		Added check_id 68: Identity columns within 30% of the end of range (tinyint, smallint, int) AND
			Negative identity seeds or identity increments &lt;&gt; 1
		Added check_id 69: Column collation does not match database collation
		Added check_id 70: Replicated columns. This identifies which columns are in at least one replication publication.
		Added check_id 71: Cascading updates or cascading deletes.
		Split check_id 40 into two checks: fillfactor on nonclustered indexes &lt; 80%, fillfactor on clustered indexes &lt; 90%
		Added check_id 33: Potential filtered indexes based on column names.
		Fixed bug where you couldn't see detailed view for indexed views. 
			(Ex: EXEC dbo.sp_BlitzIndex @DatabaseName='AdventureWorks', @SchemaName='Production', @TableName='vProductAndDescription';)
		Added four index usage columns to table detail output: last_user_seek, last_user_scan, last_user_lookup, last_user_update
		Modified check_id 24. This now looks for wide clustered indexes (&gt; 3 columns OR &gt; 16 bytes).
			Previously just simplistically looked for multiple column CX.
		Removed extra spacing (non-breaking) in more_info column.
		Fixed bug where create t-sql didn't include filter (for filtered indexes)
		Fixed formatting bug where "magic number" in table detail view didn't have commas
		Neatened up column names in result sets.
	April 8, 2013 (v1.5) - Fixed breaking bug for partitioned tables with &gt; 10(ish) partitions
		Added schema_name to suggested create statement for PKs
		Handled "magic_benefit_number" values for missing indexes &gt;= 922,337,203,685,477
		Added count of NC indexes to Index Hoarder: Multi-column clustered index finding
		Added link to EULA
		Simplified aggressive index checks (blocking). Multiple checks confused people more than it helped.
			Left only "Total lock wait time &gt; 5 minutes (row + page)".
		Added CheckId 25 for non-unique clustered indexes. 
		The "Create TSQL" column now shows a commented out drop command for disabled non-clustered indexes
		Updated query which joins to sys.dm_operational_stats DMV when running against 2012 for performance reasons
	December 20, 2012 (v1.4) - Fixed bugs for instances using a case-sensitive collation
		Added support to identify compressed indexes
		Added basic support for columnstore, XML, and spatial indexes
		Added "Abnormal Psychology" diagnosis to alert you to special index types in a database
		Removed hypothetical indexes and disabled indexes from "multiple personality disorders"
		Fixed bug where hypothetical indexes weren't showing up in "self-loathing indexes"
		Fixed bug where the partitioning key column was displayed in the key of aligned nonclustered indexes on partitioned tables
		Added set options to the script so procedure is created with required settings for its use of computed columns

*/
AS 

SET NOCOUNT ON;
SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;


DECLARE	@DatabaseID INT;
DECLARE @ObjectID INT;
DECLARE	@dsql NVARCHAR(MAX);
DECLARE @params NVARCHAR(MAX);
DECLARE	@msg NVARCHAR(4000);
DECLARE	@ErrorSeverity INT;
DECLARE	@ErrorState INT;
DECLARE	@Rowcount BIGINT;
DECLARE @SQLServerProductVersion NVARCHAR(128);
DECLARE @SQLServerEdition INT;
DECLARE @FilterMB INT;
DECLARE @collation NVARCHAR(256);


SELECT @SQLServerProductVersion = CAST(SERVERPROPERTY('ProductVersion') AS NVARCHAR(128));
SELECT @SQLServerEdition =CAST(SERVERPROPERTY('EngineEdition') AS INT); /* We default to online index creates where EngineEdition=3*/
SET @FilterMB=250;

IF @DatabaseName is null 
	SET @DatabaseName=DB_NAME();

SELECT	@DatabaseID = database_id
FROM	sys.databases
WHERE	[name] = @DatabaseName
	AND user_access_desc='MULTI_USER'
	AND state_desc = 'ONLINE';

----------------------------------------
--STEP 1: OBSERVE THE PATIENT
--This step puts index information into temp tables.
----------------------------------------
BEGIN TRY
	BEGIN

		--Validate SQL Server Verson

		IF (SELECT LEFT(@SQLServerProductVersion,
			  CHARINDEX('.',@SQLServerProductVersion,0)-1
			  )) &lt;= 8
		BEGIN
			SET @msg=N'sp_BlitzIndex is only supported on SQL Server 2005 and higher. The version of this instance is: ' + @SQLServerProductVersion;
			RAISERROR(@msg,16,1);
		END

		--Short circuit here if database name does not exist.
		IF @DatabaseName IS NULL OR @DatabaseID IS NULL
		BEGIN
			SET @msg='Database does not exist or is not online/multi-user: cannot proceed.'
			RAISERROR(@msg,16,1);
		END    

		--Validate parameters.
		IF (@Mode NOT IN (0,1,2,3))
		BEGIN
			SET @msg=N'Invalid @Mode parameter. 0=diagnose, 1=summarize, 2=index detail, 3=missing index detail';
			RAISERROR(@msg,16,1);
		END

		IF (@Mode &lt;&gt; 0 AND @TableName IS NOT NULL)
		BEGIN
			SET @msg=N'Setting the @Mode doesn''t change behavior if you supply @TableName. Use default @Mode=0 to see table detail.';
			RAISERROR(@msg,16,1);
		END

		IF ((@Mode &lt;&gt; 0 OR @TableName IS NOT NULL) and @Filter &lt;&gt; 0)
		BEGIN
			SET @msg=N'@Filter only appies when @Mode=0 and @TableName is not specified. Please try again.';
			RAISERROR(@msg,16,1);
		END

		IF (@SchemaName IS NOT NULL AND @TableName IS NULL) 
		BEGIN
			SET @msg='We can''t run against a whole schema! Specify a @TableName, or leave both NULL for diagnosis.'
			RAISERROR(@msg,16,1);
		END


		IF  (@TableName IS NOT NULL AND @SchemaName IS NULL)
		BEGIN
			SET @SchemaName=N'dbo'
			SET @msg='@SchemaName wasn''t specified-- assuming schema=dbo.'
			RAISERROR(@msg,1,1) WITH NOWAIT;
		END

		--If a table is specified, grab the object id.
		--Short circuit if it doesn't exist.
		IF @TableName IS NOT NULL
		BEGIN
			SET @dsql = N'
					SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;
					SELECT	@ObjectID= OBJECT_ID
					FROM	' + QUOTENAME(@DatabaseName) + N'.sys.objects AS so
					JOIN	' + QUOTENAME(@DatabaseName) + N'.sys.schemas AS sc on 
						so.schema_id=sc.schema_id
					where so.type in (''U'', ''V'')
					and so.name=' + QUOTENAME(@TableName,'''')+ N'
					and sc.name=' + QUOTENAME(@SchemaName,'''')+ N'
					/*Has a row in sys.indexes. This lets us get indexed views.*/
					and exists (
						SELECT si.name
						FROM ' + QUOTENAME(@DatabaseName) + '.sys.indexes AS si 
						WHERE so.object_id=si.object_id)
					OPTION (RECOMPILE);';

			SET @params='@ObjectID INT OUTPUT'				

			IF @dsql IS NULL 
				RAISERROR('@dsql is null',16,1);

			EXEC sp_executesql @dsql, @params, @ObjectID=@ObjectID OUTPUT;
			
			IF @ObjectID IS NULL
					BEGIN
						SET @msg=N'Oh, this is awkward. I can''t find the table or indexed view you''re looking for in that database.' + CHAR(10) +
							N'Please check your parameters.'
						RAISERROR(@msg,1,1);
						RETURN;
					END
		END

		RAISERROR(N'Starting run. sp_BlitzIndex(TM) v2.02 - Jan 30, 2014', 0,1) WITH NOWAIT;

		IF OBJECT_ID('tempdb..#IndexSanity') IS NOT NULL 
			DROP TABLE #IndexSanity;

		IF OBJECT_ID('tempdb..#IndexPartitionSanity') IS NOT NULL 
			DROP TABLE #IndexPartitionSanity;

		IF OBJECT_ID('tempdb..#IndexSanitySize') IS NOT NULL 
			DROP TABLE #IndexSanitySize;

		IF OBJECT_ID('tempdb..#IndexColumns') IS NOT NULL 
			DROP TABLE #IndexColumns;

		IF OBJECT_ID('tempdb..#MissingIndexes') IS NOT NULL 
			DROP TABLE #MissingIndexes;

		IF OBJECT_ID('tempdb..#ForeignKeys') IS NOT NULL 
			DROP TABLE #ForeignKeys;

		IF OBJECT_ID('tempdb..#BlitzIndexResults') IS NOT NULL 
			DROP TABLE #BlitzIndexResults;
		
		IF OBJECT_ID('tempdb..#IndexCreateTsql') IS NOT NULL	
			DROP TABLE #IndexCreateTsql;

		RAISERROR (N'Create temp tables.',0,1) WITH NOWAIT;
		CREATE TABLE #BlitzIndexResults
			(
			  blitz_result_id INT IDENTITY PRIMARY KEY,
			  check_id INT NOT NULL,
			  index_sanity_id INT NULL,
			  findings_group VARCHAR(4000) NOT NULL,
			  finding VARCHAR(200) NOT NULL,
			  URL VARCHAR(200) NOT NULL,
			  details NVARCHAR(4000) NOT NULL,
			  index_definition NVARCHAR(MAX) NOT NULL,
			  secret_columns NVARCHAR(MAX) NULL,
			  index_usage_summary NVARCHAR(MAX) NULL,
			  index_size_summary NVARCHAR(MAX) NULL,
			  create_tsql NVARCHAR(MAX) NULL,
			  more_info NVARCHAR(MAX)NULL
			);

		CREATE TABLE #IndexSanity
			(
			  [index_sanity_id] INT IDENTITY PRIMARY KEY,
			  [database_id] SMALLINT NOT NULL ,
			  [object_id] INT NOT NULL ,
			  [index_id] INT NOT NULL ,
			  [index_type] TINYINT NOT NULL,
			  [database_name] NVARCHAR(128) NOT NULL ,
			  [schema_name] NVARCHAR(128) NOT NULL ,
			  [object_name] NVARCHAR(128) NOT NULL ,
			  index_name NVARCHAR(128) NULL ,
			  key_column_names NVARCHAR(MAX) NULL ,
			  key_column_names_with_sort_order NVARCHAR(MAX) NULL ,
			  key_column_names_with_sort_order_no_types NVARCHAR(MAX) NULL ,
			  count_key_columns INT NULL ,
			  include_column_names NVARCHAR(MAX) NULL ,
			  include_column_names_no_types NVARCHAR(MAX) NULL ,
			  count_included_columns INT NULL ,
			  partition_key_column_name NVARCHAR(MAX) NULL,
			  filter_definition NVARCHAR(MAX) NOT NULL ,
			  is_indexed_view BIT NOT NULL ,
			  is_unique BIT NOT NULL ,
			  is_primary_key BIT NOT NULL ,
			  is_XML BIT NOT NULL,
			  is_spatial BIT NOT NULL,
			  is_NC_columnstore BIT NOT NULL,
			  is_CX_columnstore BIT NOT NULL,
			  is_disabled BIT NOT NULL ,
			  is_hypothetical BIT NOT NULL ,
			  is_padded BIT NOT NULL ,
			  fill_factor SMALLINT NOT NULL ,
			  user_seeks BIGINT NOT NULL ,
			  user_scans BIGINT NOT NULL ,
			  user_lookups BIGINT NOT  NULL ,
			  user_updates BIGINT NULL ,
			  last_user_seek DATETIME NULL ,
			  last_user_scan DATETIME NULL ,
			  last_user_lookup DATETIME NULL ,
			  last_user_update DATETIME NULL ,
			  is_referenced_by_foreign_key BIT DEFAULT(0),
			  secret_columns NVARCHAR(MAX) NULL,
			  count_secret_columns INT NULL,
			  create_date DATETIME NOT NULL,
			  modify_date DATETIME NOT NULL
			);	

		CREATE TABLE #IndexPartitionSanity
			(
			  [index_partition_sanity_id] INT IDENTITY PRIMARY KEY ,
			  [index_sanity_id] INT NULL ,
			  [object_id] INT NOT NULL ,
			  [index_id] INT NOT NULL ,
			  [partition_number] INT NOT NULL ,
			  row_count BIGINT NOT NULL ,
			  reserved_MB NUMERIC(29,2) NOT NULL ,
			  reserved_LOB_MB NUMERIC(29,2) NOT NULL ,
			  reserved_row_overflow_MB NUMERIC(29,2) NOT NULL ,
			  leaf_insert_count BIGINT NULL ,
			  leaf_delete_count BIGINT NULL ,
			  leaf_update_count BIGINT NULL ,
			  range_scan_count BIGINT NULL ,
			  singleton_lookup_count BIGINT NULL , 
			  forwarded_fetch_count BIGINT NULL ,
			  lob_fetch_in_pages BIGINT NULL ,
			  lob_fetch_in_bytes BIGINT NULL ,
			  row_overflow_fetch_in_pages BIGINT NULL ,
			  row_overflow_fetch_in_bytes BIGINT NULL ,
			  row_lock_count BIGINT NULL ,
			  row_lock_wait_count BIGINT NULL ,
			  row_lock_wait_in_ms BIGINT NULL ,
			  page_lock_count BIGINT NULL ,
			  page_lock_wait_count BIGINT NULL ,
			  page_lock_wait_in_ms BIGINT NULL ,
			  index_lock_promotion_attempt_count BIGINT NULL ,
			  index_lock_promotion_count BIGINT NULL,
  			  data_compression_desc VARCHAR(60) NULL
			);

		CREATE TABLE #IndexSanitySize
			(
			  [index_sanity_size_id] INT IDENTITY NOT NULL ,
			  [index_sanity_id] INT NOT NULL ,
			  partition_count INT NOT NULL ,
			  total_rows BIGINT NOT NULL ,
			  total_reserved_MB NUMERIC(29,2) NOT NULL ,
			  total_reserved_LOB_MB NUMERIC(29,2) NOT NULL ,
			  total_reserved_row_overflow_MB NUMERIC(29,2) NOT NULL ,
			  total_leaf_delete_count BIGINT NULL,
			  total_leaf_update_count BIGINT NULL,
			  total_range_scan_count BIGINT NULL,
			  total_singleton_lookup_count BIGINT NULL,
			  total_forwarded_fetch_count BIGINT NULL,
			  total_row_lock_count BIGINT NULL ,
			  total_row_lock_wait_count BIGINT NULL ,
			  total_row_lock_wait_in_ms BIGINT NULL ,
			  avg_row_lock_wait_in_ms BIGINT NULL ,
			  total_page_lock_count BIGINT NULL ,
			  total_page_lock_wait_count BIGINT NULL ,
			  total_page_lock_wait_in_ms BIGINT NULL ,
			  avg_page_lock_wait_in_ms BIGINT NULL ,
 			  total_index_lock_promotion_attempt_count BIGINT NULL ,
			  total_index_lock_promotion_count BIGINT NULL ,
			  data_compression_desc VARCHAR(8000) NULL
			);

		CREATE TABLE #IndexColumns
			(
			  [object_id] INT NOT NULL ,
			  [index_id] INT NOT NULL ,
			  [key_ordinal] INT NULL ,
			  is_included_column BIT NULL ,
			  is_descending_key BIT NULL ,
			  [partition_ordinal] INT NULL ,
			  column_name NVARCHAR(256) NOT NULL ,
			  system_type_name NVARCHAR(256) NOT NULL,
			  max_length SMALLINT NOT NULL,
			  [precision] TINYINT NOT NULL,
			  [scale] TINYINT NOT NULL,
			  collation_name NVARCHAR(256) NULL,
			  is_nullable bit NULL,
			  is_identity bit NULL,
			  is_computed bit NULL,
			  is_replicated bit NULL,
			  is_sparse bit NULL,
			  is_filestream bit NULL,
			  seed_value BIGINT NULL,
			  increment_value INT NULL ,
			  last_value BIGINT NULL,
			  is_not_for_replication BIT NULL
			);

		CREATE TABLE #MissingIndexes
			([object_id] INT NOT NULL,
			[database_name] NVARCHAR(128) NOT NULL ,
			[schema_name] NVARCHAR(128) NOT NULL ,
			[table_name] NVARCHAR(128),
			[statement] NVARCHAR(512) NOT NULL,
			magic_benefit_number AS (( user_seeks + user_scans ) * avg_total_user_cost * avg_user_impact),
			avg_total_user_cost NUMERIC(29,1) NOT NULL,
			avg_user_impact NUMERIC(29,1) NOT NULL,
			user_seeks BIGINT NOT NULL,
			user_scans BIGINT NOT NULL,
			unique_compiles BIGINT NULL,
			equality_columns NVARCHAR(4000), 
			inequality_columns NVARCHAR(4000),
			included_columns NVARCHAR(4000)
			);

		CREATE TABLE #ForeignKeys (
			foreign_key_name NVARCHAR(256),
			parent_object_id INT,
			parent_object_name NVARCHAR(256),
			referenced_object_id INT,
			referenced_object_name NVARCHAR(256),
			is_disabled BIT,
			is_not_trusted BIT,
			is_not_for_replication BIT,
			parent_fk_columns NVARCHAR(MAX),
			referenced_fk_columns NVARCHAR(MAX),
			update_referential_action_desc NVARCHAR(16),
			delete_referential_action_desc NVARCHAR(60)
		)
		
		CREATE TABLE #IndexCreateTsql (
			index_sanity_id INT NOT NULL,
			create_tsql NVARCHAR(MAX) NOT NULL
		)

		--set @collation
		SELECT @collation=collation_name
		FROM sys.databases
		where database_id=@DatabaseID;

		--insert columns for clustered indexes and heaps
		--collect info on identity columns for this one
		SET @dsql = N'SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;
				SELECT	
					si.object_id, 
					si.index_id, 
					sc.key_ordinal, 
					sc.is_included_column, 
					sc.is_descending_key,
					sc.partition_ordinal,
					c.name as column_name, 
					st.name as system_type_name,
					c.max_length,
					c.[precision],
					c.[scale],
					c.collation_name,
					c.is_nullable,
					c.is_identity,
					c.is_computed,
					c.is_replicated,
					' + case when @SQLServerProductVersion not like '9%' THEN N'c.is_sparse' else N'NULL as is_sparse' END + N',
					' + case when @SQLServerProductVersion not like '9%' THEN N'c.is_filestream' else N'NULL as is_filestream' END + N',
					CAST(ic.seed_value AS BIGINT),
					CAST(ic.increment_value AS INT),
					CAST(ic.last_value AS BIGINT),
					ic.is_not_for_replication
				FROM	' + QUOTENAME(@DatabaseName) + N'.sys.indexes si
				JOIN	' + QUOTENAME(@DatabaseName) + N'.sys.columns c ON
					si.object_id=c.object_id
				LEFT JOIN ' + QUOTENAME(@DatabaseName) + N'.sys.index_columns sc ON 
					sc.object_id = si.object_id
					and sc.index_id=si.index_id
					AND sc.column_id=c.column_id
				LEFT JOIN ' + QUOTENAME(@DatabaseName) + N'.sys.identity_columns ic ON
					c.object_id=ic.object_id and
					c.column_id=ic.column_id
				JOIN ' + QUOTENAME(@DatabaseName) + N'.sys.types st ON 
					c.system_type_id=st.system_type_id
					AND c.user_type_id=st.user_type_id
				WHERE si.index_id in (0,1) ' 
					+ CASE WHEN @ObjectID IS NOT NULL 
						THEN N' AND si.object_id=' + CAST(@ObjectID AS NVARCHAR(30)) 
					ELSE N'' END 
				+ N';';

		IF @dsql IS NULL 
			RAISERROR('@dsql is null',16,1);

		RAISERROR (N'Inserting data into #IndexColumns for clustered indexes and heaps',0,1) WITH NOWAIT;
		INSERT	#IndexColumns ( object_id, index_id, key_ordinal, is_included_column, is_descending_key, partition_ordinal,
			column_name, system_type_name, max_length, precision, scale, collation_name, is_nullable, is_identity, is_computed,
			is_replicated, is_sparse, is_filestream, seed_value, increment_value, last_value, is_not_for_replication )
				EXEC sp_executesql @dsql;

		--insert columns for nonclustered indexes
		--this uses a full join to sys.index_columns
		--We don't collect info on identity columns here. They may be in NC indexes, but we just analyze identities in the base table.
		SET @dsql = N'SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;
				SELECT	
					si.object_id, 
					si.index_id, 
					sc.key_ordinal, 
					sc.is_included_column, 
					sc.is_descending_key,
					sc.partition_ordinal,
					c.name as column_name, 
					st.name as system_type_name,
					c.max_length,
					c.[precision],
					c.[scale],
					c.collation_name,
					c.is_nullable,
					c.is_identity,
					c.is_computed,
					c.is_replicated,
					' + case when @SQLServerProductVersion not like '9%' THEN N'c.is_sparse' else N'NULL AS is_sparse' END + N',
					' + case when @SQLServerProductVersion not like '9%' THEN N'c.is_filestream' else N'NULL AS is_filestream' END + N'				
				FROM	' + QUOTENAME(@DatabaseName) + N'.sys.indexes AS si
				JOIN	' + QUOTENAME(@DatabaseName) + N'.sys.columns AS c ON
					si.object_id=c.object_id
				JOIN ' + QUOTENAME(@DatabaseName) + N'.sys.index_columns AS sc ON 
					sc.object_id = si.object_id
					and sc.index_id=si.index_id
					AND sc.column_id=c.column_id
				JOIN ' + QUOTENAME(@DatabaseName) + N'.sys.types AS st ON 
					c.system_type_id=st.system_type_id
					AND c.user_type_id=st.user_type_id
				WHERE si.index_id not in (0,1) ' 
					+ CASE WHEN @ObjectID IS NOT NULL 
						THEN N' AND si.object_id=' + CAST(@ObjectID AS NVARCHAR(30)) 
					ELSE N'' END 
				+ N';';

		IF @dsql IS NULL 
			RAISERROR('@dsql is null',16,1);

		RAISERROR (N'Inserting data into #IndexColumns for nonclustered indexes',0,1) WITH NOWAIT;
		INSERT	#IndexColumns ( object_id, index_id, key_ordinal, is_included_column, is_descending_key, partition_ordinal,
			column_name, system_type_name, max_length, precision, scale, collation_name, is_nullable, is_identity, is_computed,
			is_replicated, is_sparse, is_filestream )
				EXEC sp_executesql @dsql;
					
		SET @dsql = N'SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;
				SELECT	' + CAST(@DatabaseID AS NVARCHAR(10)) + ' AS database_id, 
						so.object_id, 
						si.index_id, 
						si.type,
						' + QUOTENAME(@DatabaseName, '''') + ' AS database_name, 
						sc.NAME AS [schema_name],
						so.name AS [object_name], 
						si.name AS [index_name],
						CASE	WHEN so.[type] = CAST(''V'' AS CHAR(2)) THEN 1 ELSE 0 END, 
						si.is_unique, 
						si.is_primary_key, 
						CASE when si.type = 3 THEN 1 ELSE 0 END AS is_XML,
						CASE when si.type = 4 THEN 1 ELSE 0 END AS is_spatial,
						CASE when si.type = 6 THEN 1 ELSE 0 END AS is_NC_columnstore,
						CASE when si.type = 5 then 1 else 0 end as is_CX_columnstore,
						si.is_disabled,
						si.is_hypothetical, 
						si.is_padded, 
						si.fill_factor,'
						+ case when @SQLServerProductVersion not like '9%' THEN '
						CASE WHEN si.filter_definition IS NOT NULL THEN si.filter_definition
							 ELSE ''''
						END AS filter_definition' ELSE ''''' AS filter_definition' END + '
						, ISNULL(us.user_seeks, 0), ISNULL(us.user_scans, 0),
						ISNULL(us.user_lookups, 0), ISNULL(us.user_updates, 0), us.last_user_seek, us.last_user_scan,
						us.last_user_lookup, us.last_user_update,
						so.create_date, so.modify_date
				FROM	' + QUOTENAME(@DatabaseName) + '.sys.indexes AS si WITH (NOLOCK)
						JOIN ' + QUOTENAME(@DatabaseName) + '.sys.objects AS so WITH (NOLOCK) ON si.object_id = so.object_id
											   AND so.is_ms_shipped = 0 /*Exclude objects shipped by Microsoft*/
											   AND so.type &lt;&gt; ''TF'' /*Exclude table valued functions*/
						JOIN ' + QUOTENAME(@DatabaseName) + '.sys.schemas sc ON so.schema_id = sc.schema_id
						LEFT JOIN sys.dm_db_index_usage_stats AS us WITH (NOLOCK) ON si.[object_id] = us.[object_id]
																	   AND si.index_id = us.index_id
																	   AND us.database_id = '+ CAST(@DatabaseID AS NVARCHAR(10)) + '
				WHERE	si.[type] IN ( 0, 1, 2, 3, 4, 5, 6 ) 
				/* Heaps, clustered, nonclustered, XML, spatial, Cluster Columnstore, NC Columnstore */ ' +
				CASE WHEN @TableName IS NOT NULL THEN ' and so.name=' + QUOTENAME(@TableName,'''') + ' ' ELSE '' END + 
		'OPTION	( RECOMPILE );
		';
		IF @dsql IS NULL 
			RAISERROR('@dsql is null',16,1);

		RAISERROR (N'Inserting data into #IndexSanity',0,1) WITH NOWAIT;
		INSERT	#IndexSanity ( [database_id], [object_id], [index_id], [index_type], [database_name], [schema_name], [object_name],
								index_name, is_indexed_view, is_unique, is_primary_key, is_XML, is_spatial, is_NC_columnstore, is_CX_columnstore,
								is_disabled, is_hypothetical, is_padded, fill_factor, filter_definition, user_seeks, user_scans, 
								user_lookups, user_updates, last_user_seek, last_user_scan, last_user_lookup, last_user_update,
								create_date, modify_date )
				EXEC sp_executesql @dsql;

		RAISERROR (N'Updating #IndexSanity.key_column_names',0,1) WITH NOWAIT;
		UPDATE	#IndexSanity
		SET		key_column_names = D1.key_column_names
		FROM	#IndexSanity si
				CROSS APPLY ( SELECT	RTRIM(STUFF( (SELECT	N', ' + c.column_name 
									+ N' {' + system_type_name + N' ' + CAST(max_length AS NVARCHAR(50)) +  N'}'
										AS col_definition
									FROM	#IndexColumns c
									WHERE	c.object_id = si.object_id
											AND c.index_id = si.index_id
											AND c.is_included_column = 0 /*Just Keys*/
											AND c.key_ordinal &gt; 0 /*Ignore non-key columns, such as partitioning keys*/
									ORDER BY c.object_id, c.index_id, c.key_ordinal	
							FOR	  XML PATH('') ,TYPE).value('.', 'varchar(max)'), 1, 1, ''))
										) D1 ( key_column_names )

		RAISERROR (N'Updating #IndexSanity.partition_key_column_name',0,1) WITH NOWAIT;
		UPDATE	#IndexSanity
		SET		partition_key_column_name = D1.partition_key_column_name
		FROM	#IndexSanity si
				CROSS APPLY ( SELECT	RTRIM(STUFF( (SELECT	N', ' + c.column_name AS col_definition
									FROM	#IndexColumns c
									WHERE	c.object_id = si.object_id
											AND c.index_id = si.index_id
											AND c.partition_ordinal &lt;&gt; 0 /*Just Partitioned Keys*/
									ORDER BY c.object_id, c.index_id, c.key_ordinal	
							FOR	  XML PATH('') , TYPE).value('.', 'varchar(max)'), 1, 1,''))) D1 
										( partition_key_column_name )

		RAISERROR (N'Updating #IndexSanity.key_column_names_with_sort_order',0,1) WITH NOWAIT;
		UPDATE	#IndexSanity
		SET		key_column_names_with_sort_order = D2.key_column_names_with_sort_order
		FROM	#IndexSanity si
				CROSS APPLY ( SELECT	RTRIM(STUFF( (SELECT	N', ' + c.column_name + CASE c.is_descending_key
									WHEN 1 THEN N' DESC'
									ELSE N''
								+ N' {' + system_type_name + N' ' + CAST(max_length AS NVARCHAR(50)) +  N'}'
								END AS col_definition
							FROM	#IndexColumns c
							WHERE	c.object_id = si.object_id
									AND c.index_id = si.index_id
									AND c.is_included_column = 0 /*Just Keys*/
									AND c.key_ordinal &gt; 0 /*Ignore non-key columns, such as partitioning keys*/
							ORDER BY c.object_id, c.index_id, c.key_ordinal	
					FOR	  XML PATH('') , TYPE).value('.', 'varchar(max)'), 1, 1, ''))
					) D2 ( key_column_names_with_sort_order )

		RAISERROR (N'Updating #IndexSanity.key_column_names_with_sort_order_no_types (for create tsql)',0,1) WITH NOWAIT;
		UPDATE	#IndexSanity
		SET		key_column_names_with_sort_order_no_types = D2.key_column_names_with_sort_order_no_types
		FROM	#IndexSanity si
				CROSS APPLY ( SELECT	RTRIM(STUFF( (SELECT	N', ' + QUOTENAME(c.column_name) + CASE c.is_descending_key
									WHEN 1 THEN N' [DESC]'
									ELSE N''
								END AS col_definition
							FROM	#IndexColumns c
							WHERE	c.object_id = si.object_id
									AND c.index_id = si.index_id
									AND c.is_included_column = 0 /*Just Keys*/
									AND c.key_ordinal &gt; 0 /*Ignore non-key columns, such as partitioning keys*/
							ORDER BY c.object_id, c.index_id, c.key_ordinal	
					FOR	  XML PATH('') , TYPE).value('.', 'varchar(max)'), 1, 1, ''))
					) D2 ( key_column_names_with_sort_order_no_types )

		RAISERROR (N'Updating #IndexSanity.include_column_names',0,1) WITH NOWAIT;
		UPDATE	#IndexSanity
		SET		include_column_names = D3.include_column_names
		FROM	#IndexSanity si
				CROSS APPLY ( SELECT	RTRIM(STUFF( (SELECT	N', ' + c.column_name
								+ N' {' + system_type_name + N' ' + CAST(max_length AS NVARCHAR(50)) +  N'}'
								FROM	#IndexColumns c
								WHERE	c.object_id = si.object_id
										AND c.index_id = si.index_id
										AND c.is_included_column = 1 /*Just includes*/
								ORDER BY c.column_name /*Order doesn't matter in includes, 
										this is here to make rows easy to compare.*/ 
						FOR	  XML PATH('') ,  TYPE).value('.', 'varchar(max)'), 1, 1, ''))
						) D3 ( include_column_names );

		RAISERROR (N'Updating #IndexSanity.include_column_names_no_types (for create tsql)',0,1) WITH NOWAIT;
		UPDATE	#IndexSanity
		SET		include_column_names_no_types = D3.include_column_names_no_types
		FROM	#IndexSanity si
				CROSS APPLY ( SELECT	RTRIM(STUFF( (SELECT	N', ' + QUOTENAME(c.column_name)
								FROM	#IndexColumns c
								WHERE	c.object_id = si.object_id
										AND c.index_id = si.index_id
										AND c.is_included_column = 1 /*Just includes*/
								ORDER BY c.column_name /*Order doesn't matter in includes, 
										this is here to make rows easy to compare.*/ 
						FOR	  XML PATH('') ,  TYPE).value('.', 'varchar(max)'), 1, 1, ''))
						) D3 ( include_column_names_no_types );

		RAISERROR (N'Updating #IndexSanity.count_key_columns and count_include_columns',0,1) WITH NOWAIT;
		UPDATE	#IndexSanity
		SET		count_included_columns = D4.count_included_columns,
				count_key_columns = D4.count_key_columns
		FROM	#IndexSanity si
				CROSS APPLY ( SELECT	SUM(CASE WHEN is_included_column = 'true' THEN 1
												 ELSE 0
											END) AS count_included_columns,
										SUM(CASE WHEN is_included_column = 'false' AND c.key_ordinal &gt; 0 THEN 1
												 ELSE 0
											END) AS count_key_columns
							  FROM		#IndexColumns c
							  WHERE		c.object_id = si.object_id
										AND c.index_id = si.index_id 
										) AS D4 ( count_included_columns, count_key_columns );

		IF (SELECT LEFT(@SQLServerProductVersion,
			  CHARINDEX('.',@SQLServerProductVersion,0)-1
			  )) &lt;&gt; 11 --Anything other than 2012
		BEGIN

			RAISERROR (N'Using non-2012 syntax to query sys.dm_db_index_operational_stats',0,1) WITH NOWAIT;

			--NOTE: we're joining to sys.dm_db_index_operational_stats differently than you might think (not using a cross apply)
			--This is because of quirks prior to SQL Server 2012 and in 2014 with this DMV.
			SET @dsql = N'SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;
						SELECT	ps.object_id, 
								ps.index_id, 
								ps.partition_number, 
								ps.row_count,
								ps.reserved_page_count * 8. / 1024. AS reserved_MB,
								ps.lob_reserved_page_count * 8. / 1024. AS reserved_LOB_MB,
								ps.row_overflow_reserved_page_count * 8. / 1024. AS reserved_row_overflow_MB,
								os.leaf_insert_count, 
								os.leaf_delete_count, 
								os.leaf_update_count, 
								os.range_scan_count, 
								os.singleton_lookup_count,  
								os.forwarded_fetch_count,
								os.lob_fetch_in_pages, 
								os.lob_fetch_in_bytes, 
								os.row_overflow_fetch_in_pages,
								os.row_overflow_fetch_in_bytes, 
								os.row_lock_count, 
								os.row_lock_wait_count,
								os.row_lock_wait_in_ms, 
								os.page_lock_count, 
								os.page_lock_wait_count, 
								os.page_lock_wait_in_ms,
								os.index_lock_promotion_attempt_count, 
								os.index_lock_promotion_count, 
							' + case when @SQLServerProductVersion not like '9%' THEN 'par.data_compression_desc ' ELSE 'null as data_compression_desc' END + '
					FROM	' + QUOTENAME(@DatabaseName) + '.sys.dm_db_partition_stats AS ps  
					JOIN ' + QUOTENAME(@DatabaseName) + '.sys.partitions AS par on ps.partition_id=par.partition_id
					JOIN ' + QUOTENAME(@DatabaseName) + '.sys.objects AS so ON ps.object_id = so.object_id
							   AND so.is_ms_shipped = 0 /*Exclude objects shipped by Microsoft*/
							   AND so.type &lt;&gt; ''TF'' /*Exclude table valued functions*/
					LEFT JOIN ' + QUOTENAME(@DatabaseName) + '.sys.dm_db_index_operational_stats('
				+ CAST(@DatabaseID AS NVARCHAR(10)) + ', NULL, NULL,NULL) AS os ON
					ps.object_id=os.object_id and ps.index_id=os.index_id and ps.partition_number=os.partition_number 
					WHERE 1=1 
					' + CASE WHEN @ObjectID IS NOT NULL THEN N'AND so.object_id=' + CAST(@ObjectID AS NVARCHAR(30)) + N' ' ELSE N' ' END + '
					' + CASE WHEN @Filter = 2 THEN N'AND ps.reserved_page_count * 8./1024. &gt; ' + CAST(@FilterMB AS NVARCHAR(5)) + N' ' ELSE N' ' END + '
			ORDER BY ps.object_id,  ps.index_id, ps.partition_number
			OPTION	( RECOMPILE );
			';
		END
		ELSE /* Otherwise use this syntax which takes advantage of OUTER APPLY on the os_partitions DMV. 
		This performs better on 2012 tables using 1000+ partitions. */
		BEGIN
		RAISERROR (N'Using 2012 syntax to query sys.dm_db_index_operational_stats',0,1) WITH NOWAIT;

 		SET @dsql = N'SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;
						SELECT	ps.object_id, 
								ps.index_id, 
								ps.partition_number, 
								ps.row_count,
								ps.reserved_page_count * 8. / 1024. AS reserved_MB,
								ps.lob_reserved_page_count * 8. / 1024. AS reserved_LOB_MB,
								ps.row_overflow_reserved_page_count * 8. / 1024. AS reserved_row_overflow_MB,
								os.leaf_insert_count, 
								os.leaf_delete_count, 
								os.leaf_update_count, 
								os.range_scan_count, 
								os.singleton_lookup_count,  
								os.forwarded_fetch_count,
								os.lob_fetch_in_pages, 
								os.lob_fetch_in_bytes, 
								os.row_overflow_fetch_in_pages,
								os.row_overflow_fetch_in_bytes, 
								os.row_lock_count, 
								os.row_lock_wait_count,
								os.row_lock_wait_in_ms, 
								os.page_lock_count, 
								os.page_lock_wait_count, 
								os.page_lock_wait_in_ms,
								os.index_lock_promotion_attempt_count, 
								os.index_lock_promotion_count, 
								' + case when @SQLServerProductVersion not like '9%' THEN N'par.data_compression_desc ' ELSE N'null as data_compression_desc' END + N'
						FROM	' + QUOTENAME(@DatabaseName) + N'.sys.dm_db_partition_stats AS ps  
						JOIN ' + QUOTENAME(@DatabaseName) + N'.sys.partitions AS par on ps.partition_id=par.partition_id
						JOIN ' + QUOTENAME(@DatabaseName) + N'.sys.objects AS so ON ps.object_id = so.object_id
								   AND so.is_ms_shipped = 0 /*Exclude objects shipped by Microsoft*/
								   AND so.type &lt;&gt; ''TF'' /*Exclude table valued functions*/
						OUTER APPLY ' + QUOTENAME(@DatabaseName) + N'.sys.dm_db_index_operational_stats('
					+ CAST(@DatabaseID AS NVARCHAR(10)) + N', ps.object_id, ps.index_id,ps.partition_number) AS os
						WHERE 1=1 
						' + CASE WHEN @ObjectID IS NOT NULL THEN N'AND so.object_id=' + CAST(@ObjectID AS NVARCHAR(30)) + N' ' ELSE N' ' END + N'
						' + CASE WHEN @Filter = 2 THEN N'AND ps.reserved_page_count * 8./1024. &gt; ' + CAST(@FilterMB AS NVARCHAR(5)) + N' ' ELSE N' ' END + '
				ORDER BY ps.object_id,  ps.index_id, ps.partition_number
				OPTION	( RECOMPILE );
				';
 
		END       

		IF @dsql IS NULL 
			RAISERROR('@dsql is null',16,1);

		RAISERROR (N'Inserting data into #IndexPartitionSanity',0,1) WITH NOWAIT;
		insert	#IndexPartitionSanity ( 
											[object_id], 
											index_id, 
											partition_number, 
											row_count, 
											reserved_MB,
										  reserved_LOB_MB, 
										  reserved_row_overflow_MB, 
										  leaf_insert_count,
										  leaf_delete_count, 
										  leaf_update_count, 
										  range_scan_count,
										  singleton_lookup_count,
										  forwarded_fetch_count, 
										  lob_fetch_in_pages, 
										  lob_fetch_in_bytes, 
										  row_overflow_fetch_in_pages,
										  row_overflow_fetch_in_bytes, 
										  row_lock_count, 
										  row_lock_wait_count,
										  row_lock_wait_in_ms, 
										  page_lock_count, 
										  page_lock_wait_count,
										  page_lock_wait_in_ms, 
										  index_lock_promotion_attempt_count,
										  index_lock_promotion_count, 
										  data_compression_desc )
				EXEC sp_executesql @dsql;


		RAISERROR (N'Updating index_sanity_id on #IndexPartitionSanity',0,1) WITH NOWAIT;
		UPDATE	#IndexPartitionSanity
		SET		index_sanity_id = i.index_sanity_id
		FROM #IndexPartitionSanity ps
				JOIN #IndexSanity i ON ps.[object_id] = i.[object_id]
										AND ps.index_id = i.index_id


		RAISERROR (N'Inserting data into #IndexSanitySize',0,1) WITH NOWAIT;
		INSERT	#IndexSanitySize ( [index_sanity_id], partition_count, total_rows, total_reserved_MB,
									 total_reserved_LOB_MB, total_reserved_row_overflow_MB, total_range_scan_count,
									 total_singleton_lookup_count, total_leaf_delete_count, total_leaf_update_count, 
									 total_forwarded_fetch_count,total_row_lock_count,
									 total_row_lock_wait_count, total_row_lock_wait_in_ms, avg_row_lock_wait_in_ms,
									 total_page_lock_count, total_page_lock_wait_count, total_page_lock_wait_in_ms,
									 avg_page_lock_wait_in_ms, total_index_lock_promotion_attempt_count, 
									 total_index_lock_promotion_count, data_compression_desc )
				SELECT	index_sanity_id, COUNT(*), SUM(row_count), SUM(reserved_MB), SUM(reserved_LOB_MB),
						SUM(reserved_row_overflow_MB), 
						SUM(range_scan_count),
						SUM(singleton_lookup_count),
						SUM(leaf_delete_count), 
						SUM(leaf_update_count),
						SUM(forwarded_fetch_count),
						SUM(row_lock_count), 
						SUM(row_lock_wait_count),
						SUM(row_lock_wait_in_ms), 
						CASE WHEN SUM(row_lock_wait_in_ms) &gt; 0 THEN
							SUM(row_lock_wait_in_ms)/(1.*SUM(row_lock_wait_count))
						ELSE 0 END AS avg_row_lock_wait_in_ms,           
						SUM(page_lock_count), 
						SUM(page_lock_wait_count),
						SUM(page_lock_wait_in_ms), 
						CASE WHEN SUM(page_lock_wait_in_ms) &gt; 0 THEN
							SUM(page_lock_wait_in_ms)/(1.*SUM(page_lock_wait_count))
						ELSE 0 END AS avg_page_lock_wait_in_ms,           
						SUM(index_lock_promotion_attempt_count),
						SUM(index_lock_promotion_count),
						LEFT(MAX(data_compression_info.data_compression_rollup),8000)
				FROM #IndexPartitionSanity ipp
				/* individual partitions can have distinct compression settings, just roll them into a list here*/
				OUTER APPLY (SELECT STUFF((
					SELECT	N', ' + data_compression_desc
					FROM #IndexPartitionSanity ipp2
					WHERE ipp.[object_id]=ipp2.[object_id]
						AND ipp.[index_id]=ipp2.[index_id]
					ORDER BY ipp2.partition_number
					FOR	  XML PATH(''),TYPE).value('.', 'varchar(max)'), 1, 1, '')) 
						data_compression_info(data_compression_rollup)
				GROUP BY index_sanity_id
				ORDER BY index_sanity_id 
		OPTION	( RECOMPILE );

		RAISERROR (N'Adding UQ index on #IndexSanity (object_id,index_id)',0,1) WITH NOWAIT;
		CREATE UNIQUE INDEX uq_object_id_index_id ON #IndexSanity (object_id,index_id);

		RAISERROR (N'Inserting data into #MissingIndexes',0,1) WITH NOWAIT;
		SET @dsql=N'SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;
				SELECT	id.object_id, ' + QUOTENAME(@DatabaseName,'''') + N', sc.[name], so.[name], id.statement , gs.avg_total_user_cost, 
						gs.avg_user_impact, gs.user_seeks, gs.user_scans, gs.unique_compiles,id.equality_columns, 
						id.inequality_columns,id.included_columns
				FROM	sys.dm_db_missing_index_groups ig
						JOIN sys.dm_db_missing_index_details id ON ig.index_handle = id.index_handle
						JOIN sys.dm_db_missing_index_group_stats gs ON ig.index_group_handle = gs.group_handle
						JOIN ' + QUOTENAME(@DatabaseName) + N'.sys.objects so on 
							id.object_id=so.object_id
						JOIN ' + QUOTENAME(@DatabaseName) + N'.sys.schemas sc on 
							so.schema_id=sc.schema_id
				WHERE	id.database_id = ' + CAST(@DatabaseID AS NVARCHAR(30)) + '
				' + CASE WHEN @ObjectID IS NULL THEN N'' 
					ELSE N'and id.object_id=' + CAST(@ObjectID AS NVARCHAR(30)) 
				END +
		N';'

		IF @dsql IS NULL 
			RAISERROR('@dsql is null',16,1);
		INSERT	#MissingIndexes ( [object_id], [database_name], [schema_name], [table_name], [statement], avg_total_user_cost, 
									avg_user_impact, user_seeks, user_scans, unique_compiles, equality_columns, 
									inequality_columns,included_columns)
		EXEC sp_executesql @dsql;

		SET @dsql = N'
			SELECT 
				fk_object.name AS foreign_key_name,
				parent_object.[object_id] AS parent_object_id,
				parent_object.name AS parent_object_name,
				referenced_object.[object_id] AS referenced_object_id,
				referenced_object.name AS referenced_object_name,
				fk.is_disabled,
				fk.is_not_trusted,
				fk.is_not_for_replication,
				parent.fk_columns,
				referenced.fk_columns,
				[update_referential_action_desc],
				[delete_referential_action_desc]
			FROM ' + QUOTENAME(@DatabaseName) + N'.sys.foreign_keys fk
			JOIN ' + QUOTENAME(@DatabaseName) + N'.sys.objects fk_object ON fk.object_id=fk_object.object_id
			JOIN ' + QUOTENAME(@DatabaseName) + N'.sys.objects parent_object ON fk.parent_object_id=parent_object.object_id
			JOIN ' + QUOTENAME(@DatabaseName) + N'.sys.objects referenced_object ON fk.referenced_object_id=referenced_object.object_id
			CROSS APPLY ( SELECT	STUFF( (SELECT	N'', '' + c_parent.name AS fk_columns
											FROM	' + QUOTENAME(@DatabaseName) + N'.sys.foreign_key_columns fkc 
											JOIN ' + QUOTENAME(@DatabaseName) + N'.sys.columns c_parent ON fkc.parent_object_id=c_parent.[object_id]
												AND fkc.parent_column_id=c_parent.column_id
											WHERE	fk.parent_object_id=fkc.parent_object_id
												AND fk.[object_id]=fkc.constraint_object_id
											ORDER BY fkc.constraint_column_id 
									FOR	  XML PATH('''') ,
											  TYPE).value(''.'', ''varchar(max)''), 1, 1, '''')/*This is how we remove the first comma*/ ) parent ( fk_columns )
			CROSS APPLY ( SELECT	STUFF( (SELECT	N'', '' + c_referenced.name AS fk_columns
											FROM	' + QUOTENAME(@DatabaseName) + N'.sys.	foreign_key_columns fkc 
											JOIN ' + QUOTENAME(@DatabaseName) + N'.sys.columns c_referenced ON fkc.referenced_object_id=c_referenced.[object_id]
												AND fkc.referenced_column_id=c_referenced.column_id
											WHERE	fk.referenced_object_id=fkc.referenced_object_id
												and fk.[object_id]=fkc.constraint_object_id
											ORDER BY fkc.constraint_column_id  /*order by col name, we don''t have anything better*/
									FOR	  XML PATH('''') ,
											  TYPE).value(''.'', ''varchar(max)''), 1, 1, '''') ) referenced ( fk_columns )
			' + CASE WHEN @ObjectID IS NOT NULL THEN 
					'WHERE fk.parent_object_id=' + CAST(@ObjectID AS NVARCHAR(30)) + N' OR fk.referenced_object_id=' + CAST(@ObjectID AS NVARCHAR(30)) + N' ' 
					ELSE N' ' END + '
			ORDER BY parent_object_name, foreign_key_name;
		';
		IF @dsql IS NULL 
			RAISERROR('@dsql is null',16,1);

        RAISERROR (N'Inserting data into #ForeignKeys',0,1) WITH NOWAIT;
        INSERT  #ForeignKeys ( foreign_key_name, parent_object_id,parent_object_name, referenced_object_id, referenced_object_name,
                                is_disabled, is_not_trusted, is_not_for_replication, parent_fk_columns, referenced_fk_columns,
								[update_referential_action_desc], [delete_referential_action_desc] )
                EXEC sp_executesql @dsql;

        RAISERROR (N'Updating #IndexSanity.referenced_by_foreign_key',0,1) WITH NOWAIT;
		UPDATE #IndexSanity
			SET is_referenced_by_foreign_key=1
		FROM #IndexSanity s
		JOIN #ForeignKeys fk ON 
			s.object_id=fk.referenced_object_id
			AND LEFT(s.key_column_names,LEN(fk.referenced_fk_columns)) = fk.referenced_fk_columns

		RAISERROR (N'Add computed columns to #IndexSanity to simplify queries.',0,1) WITH NOWAIT;
		ALTER TABLE #IndexSanity ADD 
		[schema_object_name] AS [schema_name] + '.' + [object_name]  ,
		[schema_object_indexid] AS [schema_name] + '.' + [object_name]
			+ CASE WHEN [index_name] IS NOT NULL THEN '.' + index_name
			ELSE ''
			END + ' (' + CAST(index_id AS NVARCHAR(20)) + ')' ,
		first_key_column_name AS CASE	WHEN count_key_columns &gt; 1
			THEN LEFT(key_column_names, CHARINDEX(',', key_column_names, 0) - 1)
			ELSE key_column_names
			END ,
		index_definition AS 
		CASE WHEN partition_key_column_name IS NOT NULL 
			THEN N'[PARTITIONED BY:' + partition_key_column_name +  N']' 
			ELSE '' 
			END +
			CASE index_id
				WHEN 0 THEN N'[HEAP] '
				WHEN 1 THEN N'[CX] '
				ELSE N'' END + CASE WHEN is_indexed_view = 1 THEN '[VIEW] '
				ELSE N'' END + CASE WHEN is_primary_key = 1 THEN N'[PK] '
				ELSE N'' END + CASE WHEN is_XML = 1 THEN N'[XML] '
				ELSE N'' END + CASE WHEN is_spatial = 1 THEN N'[SPATIAL] '
				ELSE N'' END + CASE WHEN is_NC_columnstore = 1 THEN N'[COLUMNSTORE] '
				ELSE N'' END + CASE WHEN is_disabled = 1 THEN N'[DISABLED] '
				ELSE N'' END + CASE WHEN is_hypothetical = 1 THEN N'[HYPOTHETICAL] '
				ELSE N'' END + CASE WHEN is_unique = 1 AND is_primary_key = 0 THEN N'[UNIQUE] '
				ELSE N'' END + CASE WHEN count_key_columns &gt; 0 THEN 
					N'[' + CAST(count_key_columns AS VARCHAR(10)) + N' KEY' 
						+ CASE WHEN count_key_columns &gt; 1 then  N'S' ELSE N'' END
						+ N'] ' + LTRIM(key_column_names_with_sort_order)
				ELSE N'' END + CASE WHEN count_included_columns &gt; 0 THEN 
					N' [' + CAST(count_included_columns AS VARCHAR(10))  + N' INCLUDE' + 
						+ CASE WHEN count_included_columns &gt; 1 then  N'S' ELSE N'' END					
						+ N'] ' + include_column_names
				ELSE N'' END + CASE WHEN filter_definition &lt;&gt; N'' THEN N' [FILTER] ' + filter_definition
				ELSE N'' END ,
		[total_reads] AS user_seeks + user_scans + user_lookups,
		[reads_per_write] AS CAST(CASE WHEN user_updates &gt; 0
			THEN ( user_seeks + user_scans + user_lookups )  / (1.0 * user_updates)
			ELSE 0 END AS MONEY) ,
		[index_usage_summary] AS N'Reads: ' + 
			REPLACE(CONVERT(NVARCHAR(30),CAST((user_seeks + user_scans + user_lookups) AS money), 1), '.00', '')
			+ case when user_seeks + user_scans + user_lookups &gt; 0 then
				N' (' 
					+ RTRIM(
					CASE WHEN user_seeks &gt; 0 then REPLACE(CONVERT(NVARCHAR(30),CAST((user_seeks) AS money), 1), '.00', '') + N' seek ' ELSE N'' END
					+ CASE WHEN user_scans &gt; 0 then REPLACE(CONVERT(NVARCHAR(30),CAST((user_scans) AS money), 1), '.00', '') + N' scan '  ELSE N'' END
					+ CASE WHEN user_lookups &gt; 0 then  REPLACE(CONVERT(NVARCHAR(30),CAST((user_lookups) AS money), 1), '.00', '') + N' lookup' ELSE N'' END
					)
					+ N') '
				else N' ' end 
			+ N'Writes:' + 
			REPLACE(CONVERT(NVARCHAR(30),CAST(user_updates AS money), 1), '.00', ''),
		[more_info] AS N'EXEC dbo.sp_BlitzIndex @DatabaseName=' + QUOTENAME([database_name],'''') + 
			N', @SchemaName=' + QUOTENAME([schema_name],'''') + N', @TableName=' + QUOTENAME([object_name],'''') + N';'

		RAISERROR (N'Update index_secret on #IndexSanity for NC indexes.',0,1) WITH NOWAIT;
		UPDATE nc 
		SET secret_columns=
			N'[' + 
			CASE tb.count_key_columns WHEN 0 THEN '1' ELSE CAST(tb.count_key_columns AS VARCHAR(10)) END +
			CASE nc.is_unique WHEN 1 THEN N' INCLUDE' ELSE N' KEY' END +
			CASE WHEN tb.count_key_columns &gt; 1 then  N'S] ' ELSE N'] ' END +
			CASE tb.index_id WHEN 0 THEN '[RID]' ELSE LTRIM(tb.key_column_names) +
				/* Uniquifiers only needed on non-unique clustereds-- not heaps */
				CASE tb.is_unique WHEN 0 THEN ' [UNIQUIFIER]' ELSE N'' END
			END
			, count_secret_columns=
			CASE tb.index_id WHEN 0 THEN 1 ELSE 
				tb.count_key_columns +
					CASE tb.is_unique WHEN 0 THEN 1 ELSE 0 END
			END
		FROM #IndexSanity AS nc
		JOIN #IndexSanity AS tb ON nc.object_id=tb.object_id
			and tb.index_id in (0,1) 
		WHERE nc.index_id &gt; 1;

		RAISERROR (N'Update index_secret on #IndexSanity for heaps and non-unique clustered.',0,1) WITH NOWAIT;
		UPDATE tb
		SET secret_columns=	CASE tb.index_id WHEN 0 THEN '[RID]' ELSE '[UNIQUIFIER]' END
			, count_secret_columns = 1
		FROM #IndexSanity AS tb
		WHERE tb.index_id = 0 /*Heaps-- these have the RID */
			or (tb.index_id=1 and tb.is_unique=0); /* Non-unique CX: has uniquifer (when needed) */

		RAISERROR (N'Add computed columns to #IndexSanitySize to simplify queries.',0,1) WITH NOWAIT;
		ALTER TABLE #IndexSanitySize ADD 
			  index_size_summary AS ISNULL(
				CASE WHEN partition_count &gt; 1
						THEN N'[' + CAST(partition_count AS NVARCHAR(10)) + N' PARTITIONS] '
						ELSE N''
				END + REPLACE(CONVERT(NVARCHAR(30),CAST([total_rows] AS money), 1), N'.00', N'') + N' rows; '
				+ CASE WHEN total_reserved_MB &gt; 1024 THEN 
					CAST(CAST(total_reserved_MB/1024. AS NUMERIC(29,1)) AS NVARCHAR(30)) + N'GB'
				ELSE 
					CAST(CAST(total_reserved_MB AS NUMERIC(29,1)) AS NVARCHAR(30)) + N'MB'
				END
				+ CASE WHEN total_reserved_LOB_MB &gt; 1024 THEN 
					N'; ' + CAST(CAST(total_reserved_LOB_MB/1024. AS NUMERIC(29,1)) AS NVARCHAR(30)) + N'GB LOB'
				WHEN total_reserved_LOB_MB &gt; 0 THEN
					N'; ' + CAST(CAST(total_reserved_LOB_MB AS NUMERIC(29,1)) AS NVARCHAR(30)) + N'MB LOB'
				ELSE ''
				END
				 + CASE WHEN total_reserved_row_overflow_MB &gt; 1024 THEN
					N'; ' + CAST(CAST(total_reserved_row_overflow_MB/1024. AS NUMERIC(29,1)) AS NVARCHAR(30)) + N'GB Row Overflow'
				WHEN total_reserved_row_overflow_MB &gt; 0 THEN
					N'; ' + CAST(CAST(total_reserved_row_overflow_MB AS NUMERIC(29,1)) AS NVARCHAR(30)) + N'MB Row Overflow'
				ELSE ''
				END ,
					N'Error- NULL in computed column'),
			index_op_stats AS ISNULL(
				(
					REPLACE(CONVERT(NVARCHAR(30),CAST(total_singleton_lookup_count AS MONEY), 1),N'.00',N'') + N' singleton lookups; '
					+ REPLACE(CONVERT(NVARCHAR(30),CAST(total_range_scan_count AS MONEY), 1),N'.00',N'') + N' scans/seeks; '
					+ REPLACE(CONVERT(NVARCHAR(30),CAST(total_leaf_delete_count AS MONEY), 1),N'.00',N'') + N' deletes; '
					+ REPLACE(CONVERT(NVARCHAR(30),CAST(total_leaf_update_count AS MONEY), 1),N'.00',N'') + N' updates; '
					+ CASE WHEN ISNULL(total_forwarded_fetch_count,0) &gt;0 THEN
						REPLACE(CONVERT(NVARCHAR(30),CAST(total_forwarded_fetch_count AS MONEY), 1),N'.00',N'') + N' forward records fetched; '
					ELSE N'' END

					/* rows will only be in this dmv when data is in memory for the table */
				), N'Table metadata not in memory'),
			index_lock_wait_summary AS ISNULL(
				CASE WHEN total_row_lock_wait_count = 0 and  total_page_lock_wait_count = 0 and
					total_index_lock_promotion_attempt_count = 0 THEN N'0 lock waits.'
				ELSE
					CASE WHEN total_row_lock_wait_count &gt; 0 THEN
						N'Row lock waits: ' + REPLACE(CONVERT(NVARCHAR(30),CAST(total_row_lock_wait_count AS money), 1), N'.00', N'')
						+ N'; total duration: ' + 
							CASE WHEN total_row_lock_wait_in_ms &gt;= 60000 THEN /*More than 1 min*/
								REPLACE(CONVERT(NVARCHAR(30),CAST((total_row_lock_wait_in_ms/60000) AS money), 1), N'.00', N'') + N' minutes; '
							ELSE                         
								REPLACE(CONVERT(NVARCHAR(30),CAST(ISNULL(total_row_lock_wait_in_ms/1000,0) AS money), 1), N'.00', N'') + N' seconds; '
							END
						+ N'avg duration: ' + 
							CASE WHEN avg_row_lock_wait_in_ms &gt;= 60000 THEN /*More than 1 min*/
								REPLACE(CONVERT(NVARCHAR(30),CAST((avg_row_lock_wait_in_ms/60000) AS money), 1), N'.00', N'') + N' minutes; '
							ELSE                         
								REPLACE(CONVERT(NVARCHAR(30),CAST(ISNULL(avg_row_lock_wait_in_ms/1000,0) AS money), 1), N'.00', N'') + N' seconds; '
							END
					ELSE N''
					END +
					CASE WHEN total_page_lock_wait_count &gt; 0 THEN
						N'Page lock waits: ' + REPLACE(CONVERT(NVARCHAR(30),CAST(total_page_lock_wait_count AS money), 1), N'.00', N'')
						+ N'; total duration: ' + 
							CASE WHEN total_page_lock_wait_in_ms &gt;= 60000 THEN /*More than 1 min*/
								REPLACE(CONVERT(NVARCHAR(30),CAST((total_page_lock_wait_in_ms/60000) AS money), 1), N'.00', N'') + N' minutes; '
							ELSE                         
								REPLACE(CONVERT(NVARCHAR(30),CAST(ISNULL(total_page_lock_wait_in_ms/1000,0) AS money), 1), N'.00', N'') + N' seconds; '
							END
						+ N'avg duration: ' + 
							CASE WHEN avg_page_lock_wait_in_ms &gt;= 60000 THEN /*More than 1 min*/
								REPLACE(CONVERT(NVARCHAR(30),CAST((avg_page_lock_wait_in_ms/60000) AS money), 1), N'.00', N'') + N' minutes; '
							ELSE                         
								REPLACE(CONVERT(NVARCHAR(30),CAST(ISNULL(avg_page_lock_wait_in_ms/1000,0) AS money), 1), N'.00', N'') + N' seconds; '
							END
					ELSE N''
					END +
					CASE WHEN total_index_lock_promotion_attempt_count &gt; 0 THEN
						N'Lock escalation attempts: ' + REPLACE(CONVERT(NVARCHAR(30),CAST(total_index_lock_promotion_attempt_count AS money), 1), N'.00', N'')
						+ N'; Actual Escalations: ' + REPLACE(CONVERT(NVARCHAR(30),CAST(ISNULL(total_index_lock_promotion_count,0) AS money), 1), N'.00', N'') + N'.'
					ELSE N''
					END
				END                  
					,'Error- NULL in computed column')


		RAISERROR (N'Add computed columns to #missing_index to simplify queries.',0,1) WITH NOWAIT;
		ALTER TABLE #MissingIndexes ADD 
				[index_estimated_impact] AS 
					CAST(user_seeks + user_scans AS NVARCHAR(30)) + N' use' 
						+ CASE WHEN (user_seeks + user_scans) &gt; 1 THEN N's' ELSE N'' END
						 +N'; Impact: ' + CAST(avg_user_impact AS NVARCHAR(30))
						+ N'%; Avg query cost: '
						+ CAST(avg_total_user_cost AS NVARCHAR(30)),
				[missing_index_details] AS
					CASE WHEN equality_columns IS NOT NULL THEN N'EQUALITY: ' + equality_columns + N' '
						 ELSE N''
					END + CASE WHEN inequality_columns IS NOT NULL THEN N'INEQUALITY: ' + inequality_columns + N' '
					   ELSE N''
					END + CASE WHEN included_columns IS NOT NULL THEN N'INCLUDES: ' + included_columns + N' '
						ELSE N''
					END,
				[create_tsql] AS N'CREATE INDEX [ix_' + table_name + N'_' 
					+ REPLACE(REPLACE(REPLACE(REPLACE(
						ISNULL(equality_columns,N'')+ 
						CASE when equality_columns is not null and inequality_columns is not null then N'_' else N'' END
						+ ISNULL(inequality_columns,''),',','')
						,'[',''),']',''),' ','_') 
					+ CASE WHEN included_columns IS NOT NULL THEN N'_includes' ELSE N'' END + N'] ON ' 
					+ [statement] + N' (' + ISNULL(equality_columns,N'')
					+ CASE WHEN equality_columns IS NOT NULL AND inequality_columns IS NOT NULL THEN N', ' ELSE N'' END
					+ CASE WHEN inequality_columns IS NOT NULL THEN inequality_columns ELSE N'' END + 
					') ' + CASE WHEN included_columns IS NOT NULL THEN N' INCLUDE (' + included_columns + N')' ELSE N'' END
					+ N' WITH (' 
						+ N'FILLFACTOR=100, ONLINE=?, SORT_IN_TEMPDB=?' 
					+ N')'
					+ N';'
					,
				[more_info] AS N'EXEC dbo.sp_BlitzIndex @DatabaseName=' + QUOTENAME([database_name],'''') + 
					N', @SchemaName=' + QUOTENAME([schema_name],'''') + N', @TableName=' + QUOTENAME([table_name],'''') + N';'
				;


		RAISERROR (N'Populate #IndexCreateTsql.',0,1) WITH NOWAIT;
		INSERT #IndexCreateTsql (index_sanity_id, create_tsql)
		SELECT
			index_sanity_id,
			ISNULL (
			/* Script drops for disabled non-clustered indexes*/
			CASE WHEN is_disabled = 1 AND index_id &lt;&gt; 1
				THEN N'--DROP INDEX ' + QUOTENAME([index_name]) + N' ON '
				 + QUOTENAME([schema_name]) + N'.' + QUOTENAME([object_name]) 
			ELSE
				CASE index_id WHEN 0 THEN N'--I''m a Heap!' 
				ELSE 
					CASE WHEN is_XML = 1 OR is_spatial=1 THEN N'' /* Not even trying for these just yet...*/
					ELSE 
						CASE WHEN is_primary_key=1 THEN
							N'ALTER TABLE ' + QUOTENAME([schema_name]) +
								N'.' + QUOTENAME([object_name]) + 
								N' ADD CONSTRAINT [' +
								index_name + 
								N'] PRIMARY KEY ' + 
								CASE WHEN index_id=1 THEN N'CLUSTERED (' ELSE N'(' END +
								key_column_names_with_sort_order_no_types + N' )' 
							WHEN is_CX_columnstore= 1 THEN
								 N'CREATE CLUSTERED COLUMNSTORE INDEX ' + QUOTENAME(index_name) + N' on ' + QUOTENAME([schema_name]) + '.' + QUOTENAME([object_name])
						ELSE /*Else not a PK or cx columnstore */ 
							N'CREATE ' + 
							CASE WHEN is_unique=1 THEN N'UNIQUE ' ELSE N'' END +
							CASE WHEN index_id=1 THEN N'CLUSTERED ' ELSE N'' END +
							CASE WHEN is_NC_columnstore=1 THEN N'NONCLUSTERED COLUMNSTORE ' 
							ELSE N'' END +
							N'INDEX ['
								 + index_name + N'] ON ' + 
								QUOTENAME([schema_name]) + '.' + QUOTENAME([object_name]) + 
									CASE WHEN is_NC_columnstore=1 THEN 
										N' (' + ISNULL(include_column_names_no_types,'') +  N' )' 
									ELSE /*Else not colunnstore */ 
										N' (' + ISNULL(key_column_names_with_sort_order_no_types,'') +  N' )' 
										+ CASE WHEN include_column_names_no_types IS NOT NULL THEN 
											N' INCLUDE (' + include_column_names_no_types + N')' 
											ELSE N'' 
										END
									END /*End non-colunnstore case */ 
								+ CASE WHEN filter_definition &lt;&gt; N'' THEN N' WHERE ' + filter_definition ELSE N'' END
							END /*End Non-PK index CASE */ 
						+ CASE WHEN is_NC_columnstore=0 and is_CX_columnstore=0 then
							N' WITH (' 
								+ N'FILLFACTOR=' + CASE fill_factor when 0 then N'100' else CAST(fill_factor AS NVARCHAR(5)) END + ', '
								+ N'ONLINE=?, SORT_IN_TEMPDB=?'
							+ N')'
						else N'' end
						+ N';'
  					END /*End non-spatial and non-xml CASE */ 
				END
			END, '[Unknown Error]')
				AS create_tsql
		FROM #IndexSanity;
					
	END
END TRY
BEGIN CATCH
		RAISERROR (N'Failure populating temp tables.', 0,1) WITH NOWAIT;

		IF @dsql IS NOT NULL
		BEGIN
			SET @msg= 'Last @dsql: ' + @dsql;
			RAISERROR(@msg, 0, 1) WITH NOWAIT;
		END

		SELECT	@msg = ERROR_MESSAGE(), @ErrorSeverity = ERROR_SEVERITY(), @ErrorState = ERROR_STATE();
		RAISERROR (@msg,@ErrorSeverity, @ErrorState )WITH NOWAIT;
		
		
		WHILE @@trancount &gt; 0 
			ROLLBACK;

		RETURN;
END CATCH;

----------------------------------------
--STEP 2: DIAGNOSE THE PATIENT
--EVERY QUERY AFTER THIS GOES AGAINST TEMP TABLES ONLY.
----------------------------------------
BEGIN TRY
----------------------------------------
--If @TableName is specified, just return information for that table.
--The @Mode parameter doesn't matter if you're looking at a specific table.
----------------------------------------
IF @TableName IS NOT NULL
BEGIN
	RAISERROR(N'@TableName specified, giving detail only on that table.', 0,1) WITH NOWAIT;

	--We do a left join here in case this is a disabled NC.
	--In that case, it won't have any size info/pages allocated.
	WITH table_mode_cte AS (
		SELECT 
			s.schema_object_indexid, 
			s.key_column_names,
			s.index_definition, 
			ISNULL(s.secret_columns,N'') AS secret_columns,
			s.fill_factor,
			s.index_usage_summary, 
			sz.index_op_stats,
			ISNULL(sz.index_size_summary,'') /*disabled NCs will be null*/ AS index_size_summary,
			ISNULL(sz.index_lock_wait_summary,'') AS index_lock_wait_summary,
			s.is_referenced_by_foreign_key,
			(SELECT COUNT(*)
				FROM #ForeignKeys fk WHERE fk.parent_object_id=s.object_id
				AND PATINDEX (fk.parent_fk_columns, s.key_column_names)=1) AS FKs_covered_by_index,
			s.last_user_seek,
			s.last_user_scan,
			s.last_user_lookup,
			s.last_user_update,
			s.create_date,
			s.modify_date,
			ct.create_tsql,
			1 as display_order
		FROM #IndexSanity s
		LEFT JOIN #IndexSanitySize sz ON 
			s.index_sanity_id=sz.index_sanity_id
		LEFT JOIN #IndexCreateTsql ct ON 
			s.index_sanity_id=ct.index_sanity_id
		WHERE s.[object_id]=@ObjectID
		UNION ALL
		SELECT 	N'Database ' + QUOTENAME(@DatabaseName) + N' as of ' + convert(nvarchar(16),getdate(),121) + 			
				N' (sp_BlitzIndex(TM) v2.02 - Jan 30, 2014)' ,   
				N'From Brent Ozar Unlimited(TM)' ,   
				N'http://BrentOzar.com/BlitzIndex' ,
				N'Thanks from the Brent Ozar Unlimited(TM) team.  We hope you found this tool useful, and if you need help relieving your SQL Server pains, email us at Help@BrentOzar.com.',
				NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,
				0 as display_order
	)
	SELECT 
			schema_object_indexid AS [Details: schema.table.index(indexid)], 
			index_definition AS [Definition: [Property]] ColumnName {datatype maxbytes}], 
			secret_columns AS [Secret Columns],
			fill_factor AS [Fillfactor],
			index_usage_summary AS [Usage Stats], 
			index_op_stats as [Op Stats],
			index_size_summary AS [Size],
			index_lock_wait_summary AS [Lock Waits],
			is_referenced_by_foreign_key AS [Referenced by FK?],
			FKs_covered_by_index AS [FK Covered by Index?],
			last_user_seek AS [Last User Seek],
			last_user_scan AS [Last User Scan],
			last_user_lookup AS [Last User Lookup],
			last_user_update as [Last User Write],
			create_date AS [Created],
			modify_date AS [Last Modified],
			create_tsql AS [Create TSQL]
	FROM table_mode_cte
	ORDER BY display_order ASC, key_column_names ASC
	OPTION	( RECOMPILE );						

	IF (SELECT TOP 1 [object_id] FROM    #MissingIndexes mi) IS NOT NULL
	BEGIN  
		SELECT  N'Missing index.' AS Finding ,
				N'http://BrentOzar.com/go/Indexaphobia' AS URL ,
				mi.[statement] + ' Est Benefit: '
					+ CASE WHEN magic_benefit_number &gt;= 922337203685477 THEN '&gt;= 922,337,203,685,477'
					ELSE REPLACE(CONVERT(NVARCHAR(256),CAST(CAST(magic_benefit_number AS BIGINT) AS money), 1), '.00', '')
					END AS [Estimated Benefit],
				missing_index_details AS [Missing Index Request] ,
				index_estimated_impact AS [Estimated Impact],
				create_tsql AS [Create TSQL]
		FROM    #MissingIndexes mi
		WHERE   [object_id] = @ObjectID
		ORDER BY magic_benefit_number DESC
		OPTION	( RECOMPILE );
	END       
	ELSE     
	SELECT 'No missing indexes.' AS finding;

	SELECT 	
		column_name AS [Column Name],
		(SELECT COUNT(*)  
			FROM #IndexColumns c2 
			WHERE c2.column_name=c.column_name
			and c2.key_ordinal is not null)
		+ CASE WHEN c.index_id = 1 and c.key_ordinal is not null THEN
			-1+ (SELECT COUNT(DISTINCT index_id)
			from #IndexColumns c3
			where c3.index_id not in (0,1))
			ELSE 0 END
				AS [Found In],
		system_type_name + 
			CASE max_length WHEN -1 THEN N' (max)' ELSE
				CASE  
					WHEN system_type_name in (N'char',N'nchar',N'binary',N'varbinary') THEN N' (' + CAST(max_length as NVARCHAR(20)) + N')' 
					WHEN system_type_name in (N'varchar',N'nvarchar') THEN N' (' + CAST(max_length/2 as NVARCHAR(20)) + N')' 
					ELSE '' 
				END
			END
			AS [Type],
		CASE is_computed WHEN 1 THEN 'yes' ELSE '' END AS [Computed?],
		max_length AS [Length (max bytes)],
		[precision] AS [Prec],
		[scale] AS [Scale],
		CASE is_nullable WHEN 1 THEN 'yes' ELSE '' END AS [Nullable?],
		CASE is_identity WHEN 1 THEN 'yes' ELSE '' END AS [Identity?],
		CASE is_replicated WHEN 1 THEN 'yes' ELSE '' END AS [Replicated?],
		CASE is_sparse WHEN 1 THEN 'yes' ELSE '' END AS [Sparse?],
		CASE is_filestream WHEN 1 THEN 'yes' ELSE '' END AS [Filestream?],
		collation_name AS [Collation]
	FROM #IndexColumns AS c
	where index_id in (0,1);

	IF (SELECT TOP 1 parent_object_id FROM #ForeignKeys) IS NOT NULL
	BEGIN
		SELECT parent_object_name + N': ' + foreign_key_name AS [Foreign Key],
			parent_fk_columns AS [Foreign Key Columns],
			referenced_object_name AS [Referenced Table],
			referenced_fk_columns AS [Referenced Table Columns],
			is_disabled AS [Is Disabled?],
			is_not_trusted as [Not Trusted?],
			is_not_for_replication [Not for Replication?],
			[update_referential_action_desc] as [Cascading Updates?],
			[delete_referential_action_desc] as [Cascading Deletes?]
		FROM #ForeignKeys
		ORDER BY [Foreign Key]
		OPTION	( RECOMPILE );
	END
	ELSE
	SELECT 'No foreign keys.' AS finding;
END 

--If @TableName is NOT specified...
--Act based on the @Mode and @Filter. (@Filter applies only when @Mode=0 "diagnose")
ELSE
BEGIN;
	IF @Mode=0 /* DIAGNOSE*/
	BEGIN;
		RAISERROR(N'@Mode=0, we are diagnosing.', 0,1) WITH NOWAIT;

		RAISERROR(N'Insert a row to help people find help', 0,1) WITH NOWAIT;
		INSERT	#BlitzIndexResults ( check_id, findings_group, finding, URL, details, index_definition,
										index_usage_summary, index_size_summary )
		VALUES  ( 0 , 
				N'Database ' + QUOTENAME(@DatabaseName) + N' as of ' + convert(nvarchar(16),getdate(),121), 
				N'sp_BlitzIndex(TM) v2.02 - Jan 30, 2014' ,
				N'From Brent Ozar Unlimited(TM)' ,   N'http://BrentOzar.com/BlitzIndex' ,
				N'Thanks from the Brent Ozar Unlimited(TM) team.  We hope you found this tool useful, and if you need help relieving your SQL Server pains, email us at Help@BrentOzar.com.'
				, N'',N''
				);

		----------------------------------------
		--Multiple Index Personalities: Check_id 0-10
		----------------------------------------
		BEGIN;
		RAISERROR('check_id 1: Duplicate keys', 0,1) WITH NOWAIT;
			WITH	duplicate_indexes
					  AS ( SELECT	[object_id], key_column_names
						   FROM		#IndexSanity
						   WHERE  index_type IN (1,2) /* Clustered, NC only*/
								AND is_hypothetical = 0
								AND is_disabled = 0
						   GROUP BY	[object_id], key_column_names
						   HAVING	COUNT(*) &gt; 1)
				INSERT	#BlitzIndexResults ( check_id, index_sanity_id, findings_group, finding, URL, details, index_definition,
											   secret_columns, index_usage_summary, index_size_summary )
						SELECT	1 AS check_id, 
								ip.index_sanity_id,
								'Multiple Index Personalities' AS findings_group,
								'Duplicate keys' AS finding,
								N'http://BrentOzar.com/go/duplicateindex' AS URL,
								ip.schema_object_indexid AS details,
								ip.index_definition, 
								ip.secret_columns, 
								ip.index_usage_summary,
								ips.index_size_summary
						FROM	duplicate_indexes di
								JOIN #IndexSanity ip ON di.[object_id] = ip.[object_id]
														 AND ip.key_column_names = di.key_column_names
								JOIN #IndexSanitySize ips ON ip.index_sanity_id = ips.index_sanity_id
						ORDER BY ip.object_id, ip.key_column_names_with_sort_order	
				OPTION	( RECOMPILE );

		RAISERROR('check_id 2: Keys w/ identical leading columns.', 0,1) WITH NOWAIT;
			WITH	borderline_duplicate_indexes
					  AS ( SELECT DISTINCT [object_id], first_key_column_name, key_column_names,
									COUNT([object_id]) OVER ( PARTITION BY [object_id], first_key_column_name ) AS number_dupes
						   FROM		#IndexSanity
						   WHERE index_type IN (1,2) /* Clustered, NC only*/
							AND is_hypothetical=0
							AND is_disabled=0)
				INSERT	#BlitzIndexResults ( check_id, index_sanity_id,  findings_group, finding, URL, details, index_definition,
											   secret_columns, index_usage_summary, index_size_summary )
						SELECT	2 AS check_id, 
								ip.index_sanity_id,
								'Multiple Index Personalities' AS findings_group,
								'Borderline duplicate keys' AS finding,
								N'http://BrentOzar.com/go/duplicateindex' AS URL,
								ip.schema_object_indexid AS details, 
								ip.index_definition, 
								ip.secret_columns,
								ip.index_usage_summary,
								ips.index_size_summary
						FROM	#IndexSanity AS ip 
						JOIN #IndexSanitySize ips ON ip.index_sanity_id = ips.index_sanity_id
						WHERE EXISTS (
							SELECT di.[object_id]
							FROM borderline_duplicate_indexes AS di
							WHERE di.[object_id] = ip.[object_id] AND
								di.first_key_column_name = ip.first_key_column_name AND
								di.key_column_names &lt;&gt; ip.key_column_names AND
								di.number_dupes &gt; 1	
						)
						ORDER BY ip.[schema_name], ip.[object_name], ip.key_column_names, ip.include_column_names
			OPTION	( RECOMPILE );

		END
		----------------------------------------
		--Aggressive Indexes: Check_id 10-19
		----------------------------------------
		BEGIN;

		RAISERROR(N'check_id 11: Total lock wait time &gt; 5 minutes (row + page)', 0,1) WITH NOWAIT;
		INSERT	#BlitzIndexResults ( check_id, index_sanity_id, findings_group, finding, URL, details, index_definition,
										secret_columns, index_usage_summary, index_size_summary )
				SELECT	11 AS check_id, 
						i.index_sanity_id,
						N'Aggressive Indexes' AS findings_group,
						N'Total lock wait time &gt; 5 minutes (row + page)' AS finding, 
						N'http://BrentOzar.com/go/AggressiveIndexes' AS URL,
						i.schema_object_indexid + N': ' +
							sz.index_lock_wait_summary AS details, 
						i.index_definition,
						i.secret_columns,
						i.index_usage_summary,
						sz.index_size_summary
				FROM	#IndexSanity AS i
				JOIN #IndexSanitySize AS sz ON i.index_sanity_id = sz.index_sanity_id
				WHERE	(total_row_lock_wait_in_ms + total_page_lock_wait_in_ms) &gt; 300000
				OPTION	( RECOMPILE );
		END

		---------------------------------------- 
		--Index Hoarder: Check_id 20-29
		----------------------------------------
		BEGIN
			RAISERROR(N'check_id 20: &gt;=7 NC indexes on any given table. Yes, 7 is an arbitrary number.', 0,1) WITH NOWAIT;
				INSERT	#BlitzIndexResults ( check_id, index_sanity_id, findings_group, finding, URL, details, index_definition,
											   secret_columns, index_usage_summary, index_size_summary )
						SELECT	20 AS check_id, 
								MAX(i.index_sanity_id) AS index_sanity_id, 
								'Index Hoarder' AS findings_group,
								'Many NC indexes on a single table' AS finding,
								N'http://BrentOzar.com/go/IndexHoarder' AS URL,
								CAST (COUNT(*) AS NVARCHAR(30)) + ' NC indexes on ' + i.schema_object_name AS details,
								i.schema_object_name + ' (' + CAST (COUNT(*) AS NVARCHAR(30)) + ' indexes)' AS index_definition,
								'' AS secret_columns,
								REPLACE(CONVERT(NVARCHAR(30),CAST(SUM(total_reads) AS money), 1), N'.00', N'') + N' reads (ALL); '
									+ REPLACE(CONVERT(NVARCHAR(30),CAST(SUM(user_updates) AS money), 1), N'.00', N'') + N' writes (ALL); ',
								REPLACE(CONVERT(NVARCHAR(30),CAST(MAX(total_rows) AS money), 1), N'.00', N'') + N' rows (MAX)'
									+ CASE WHEN SUM(total_reserved_MB) &gt; 1024 THEN 
										N'; ' + CAST(CAST(SUM(total_reserved_MB)/1024. AS NUMERIC(29,1)) AS NVARCHAR(30)) + 'GB (ALL)'
									WHEN SUM(total_reserved_MB) &gt; 0 THEN
										N'; ' + CAST(CAST(SUM(total_reserved_MB) AS NUMERIC(29,1)) AS NVARCHAR(30)) + 'MB (ALL)'
									ELSE ''
									END AS index_size_summary
						FROM	#IndexSanity i
						JOIN #IndexSanitySize ip ON i.index_sanity_id = ip.index_sanity_id
						WHERE	index_id NOT IN ( 0, 1 )
						GROUP BY schema_object_name
						HAVING	COUNT(*) &gt;= 7
						ORDER BY i.schema_object_name DESC  OPTION	( RECOMPILE );

			if @Filter = 1 /*@Filter=1 is "ignore unusued" */
			BEGIN
				RAISERROR(N'Skipping checks on unused indexes (21 and 22) because @Filter=1', 0,1) WITH NOWAIT;
			END
			ELSE /*Otherwise, go ahead and do the checks*/
			BEGIN
				RAISERROR(N'check_id 21: &gt;=5 percent of indexes are unused. Yes, 5 is an arbitrary number.', 0,1) WITH NOWAIT;
					DECLARE @percent_NC_indexes_unused NUMERIC(29,1);
					DECLARE @NC_indexes_unused_reserved_MB NUMERIC(29,1);

					SELECT	@percent_NC_indexes_unused =( 100.00 * SUM(CASE	WHEN total_reads = 0 THEN 1
												ELSE 0
										   END) ) / COUNT(*) ,
							@NC_indexes_unused_reserved_MB = SUM(CASE WHEN total_reads = 0 THEN sz.total_reserved_MB
									 ELSE 0
								END) 
					FROM	#IndexSanity i
					JOIN	#IndexSanitySize sz ON i.index_sanity_id = sz.index_sanity_id
					WHERE	index_id NOT IN ( 0, 1 ) 
							and i.is_unique = 0
					OPTION	( RECOMPILE );

				IF @percent_NC_indexes_unused &gt;= 5 
					INSERT	#BlitzIndexResults ( check_id, index_sanity_id,  findings_group, finding, URL, details, index_definition,
												   secret_columns, index_usage_summary, index_size_summary )
							SELECT	21 AS check_id, 
									MAX(i.index_sanity_id) AS index_sanity_id, 
									N'Index Hoarder' AS findings_group,
									N'More than 5 percent NC indexes are unused' AS finding,
									N'http://BrentOzar.com/go/IndexHoarder' AS URL,
									CAST (@percent_NC_indexes_unused AS NVARCHAR(30)) + N' percent NC indexes (' + CAST(COUNT(*) AS NVARCHAR(10)) + N') unused. ' +
									N'These take up ' + CAST (@NC_indexes_unused_reserved_MB AS NVARCHAR(30)) + N'MB of space.' AS details,
									i.database_name + ' (' + CAST (COUNT(*) AS NVARCHAR(30)) + N' indexes)' AS index_definition,
									'' AS secret_columns, 
									CAST(SUM(total_reads) AS NVARCHAR(256)) + N' reads (ALL); '
										+ CAST(SUM([user_updates]) AS NVARCHAR(256)) + N' writes (ALL)' AS index_usage_summary,
								
									REPLACE(CONVERT(NVARCHAR(30),CAST(MAX([total_rows]) AS money), 1), '.00', '') + N' rows (MAX)'
										+ CASE WHEN SUM(total_reserved_MB) &gt; 1024 THEN 
											N'; ' + CAST(CAST(SUM(total_reserved_MB)/1024. AS NUMERIC(29,1)) AS NVARCHAR(30)) + 'GB (ALL)'
										WHEN SUM(total_reserved_MB) &gt; 0 THEN
											N'; ' + CAST(CAST(SUM(total_reserved_MB) AS NUMERIC(29,1)) AS NVARCHAR(30)) + 'MB (ALL)'
										ELSE ''
										END AS index_size_summary
							FROM	#IndexSanity i
							JOIN	#IndexSanitySize sz ON i.index_sanity_id = sz.index_sanity_id
							WHERE	index_id NOT IN ( 0, 1 )
									AND i.is_unique = 0
									AND total_reads = 0
							GROUP BY i.database_name 
					OPTION	( RECOMPILE );

				RAISERROR(N'check_id 22: NC indexes with 0 reads. (Borderline)', 0,1) WITH NOWAIT;
				INSERT	#BlitzIndexResults ( check_id, index_sanity_id, findings_group, finding, URL, details, index_definition,
											   secret_columns, index_usage_summary, index_size_summary )
						SELECT	22 AS check_id, 
								i.index_sanity_id,
								N'Index Hoarder' AS findings_group,
								N'Unused NC index' AS finding, 
								N'http://BrentOzar.com/go/IndexHoarder' AS URL,
								N'0 reads: ' + i.schema_object_indexid AS details, 
								i.index_definition, 
								i.secret_columns, 
								i.index_usage_summary,
								sz.index_size_summary
						FROM	#IndexSanity AS i
						JOIN	#IndexSanitySize AS sz ON i.index_sanity_id = sz.index_sanity_id
						WHERE	i.total_reads=0
								AND i.index_id NOT IN (0,1) /*NCs only*/
								and i.is_unique = 0
						ORDER BY i.schema_object_indexid
						OPTION	( RECOMPILE );
			END /*end checks only run when @Filter &lt;&gt; 1*/

			RAISERROR(N'check_id 23: Indexes with 7 or more columns. (Borderline)', 0,1) WITH NOWAIT;
			INSERT	#BlitzIndexResults ( check_id, index_sanity_id, findings_group, finding, URL, details, index_definition,
										   secret_columns, index_usage_summary, index_size_summary )
					SELECT	23 AS check_id, 
							i.index_sanity_id, 
							N'Index Hoarder' AS findings_group,
							N'Borderline: Wide indexes (7 or more columns)' AS finding, 
							N'http://BrentOzar.com/go/IndexHoarder' AS URL,
							CAST(count_key_columns + count_included_columns AS NVARCHAR(10)) + ' columns on '
							+ i.schema_object_indexid AS details, i.index_definition, 
							i.secret_columns, 
							i.index_usage_summary,
							sz.index_size_summary
					FROM	#IndexSanity AS i
					JOIN	#IndexSanitySize AS sz ON i.index_sanity_id = sz.index_sanity_id
					WHERE	( count_key_columns + count_included_columns ) &gt;= 7
					OPTION	( RECOMPILE );

			RAISERROR(N'check_id 24: Wide clustered indexes (&gt; 3 columns or &gt; 16 bytes).', 0,1) WITH NOWAIT;
				WITH count_columns AS (
							SELECT [object_id],
								SUM(CASE max_length when -1 THEN 0 ELSE max_length END) AS sum_max_length
							FROM #IndexColumns ic
							WHERE index_id in (1,0) /*Heap or clustered only*/
							and key_ordinal &gt; 0
							GROUP BY object_id
							)
				INSERT	#BlitzIndexResults ( check_id, index_sanity_id, findings_group, finding, URL, details, index_definition,
											   secret_columns, index_usage_summary, index_size_summary )
						SELECT	24 AS check_id, 
								i.index_sanity_id, 
								N'Index Hoarder' AS findings_group,
								N'Wide clustered index (&gt; 3 columns OR &gt; 16 bytes)' AS finding,
								N'http://BrentOzar.com/go/IndexHoarder' AS URL,
								CAST (i.count_key_columns AS NVARCHAR(10)) + N' columns with potential size of '
									+ CAST(cc.sum_max_length AS NVARCHAR(10))
									+ N' bytes in clustered index:' + i.schema_object_name 
									+ N'. ' + 
										(SELECT CAST(COUNT(*) AS NVARCHAR(23)) FROM #IndexSanity i2 
										WHERE i2.[object_id]=i.[object_id] AND i2.index_id &lt;&gt; 1
										AND i2.is_disabled=0 AND i2.is_hypothetical=0)
										+ N' NC indexes on the table.'
									AS details,
								i.index_definition,
								secret_columns, 
								i.index_usage_summary,
								ip.index_size_summary
						FROM	#IndexSanity i
						JOIN	#IndexSanitySize ip ON i.index_sanity_id = ip.index_sanity_id
						JOIN	count_columns AS cc ON i.[object_id]=cc.[object_id]	
						WHERE	index_id = 1 /* clustered only */
								AND 
									(count_key_columns &gt; 3 /*More than three key columns.*/
									OR cc.sum_max_length &gt; 15 /*More than 16 bytes in key */)
						ORDER BY i.schema_object_name DESC OPTION	( RECOMPILE );

			RAISERROR(N'check_id 25: Addicted to nullable columns.', 0,1) WITH NOWAIT;
				WITH count_columns AS (
							SELECT [object_id],
								SUM(CASE is_nullable WHEN 1 THEN 0 ELSE 1 END) as non_nullable_columns,
								COUNT(*) as total_columns
							FROM #IndexColumns ic
							WHERE index_id in (1,0) /*Heap or clustered only*/
							GROUP BY object_id
							)
				INSERT	#BlitzIndexResults ( check_id, index_sanity_id, findings_group, finding, URL, details, index_definition,
											   secret_columns, index_usage_summary, index_size_summary )
						SELECT	25 AS check_id, 
								i.index_sanity_id, 
								N'Index Hoarder' AS findings_group,
								N'Addicted to nulls' AS finding,
								N'http://BrentOzar.com/go/IndexHoarder' AS URL,
								i.schema_object_name 
									+ N' allows null in ' + CAST((total_columns-non_nullable_columns) as NVARCHAR(10))
									+ N' of ' + CAST(total_columns as NVARCHAR(10))
									+ N' columns.' AS details,
								i.index_definition,
								secret_columns, 
								ISNULL(i.index_usage_summary,''),
								ISNULL(ip.index_size_summary,'')
						FROM	#IndexSanity i
						JOIN	#IndexSanitySize ip ON i.index_sanity_id = ip.index_sanity_id
						JOIN	count_columns AS cc ON i.[object_id]=cc.[object_id]
						WHERE	i.index_id in (1,0)
							AND cc.non_nullable_columns &lt; 2
							and cc.total_columns &gt; 3
						ORDER BY i.schema_object_name DESC OPTION	( RECOMPILE );

			RAISERROR(N'check_id 26: Wide tables (35+ cols or &gt; 2000 non-LOB bytes).', 0,1) WITH NOWAIT;
				WITH count_columns AS (
							SELECT [object_id],
								SUM(CASE max_length when -1 THEN 1 ELSE 0 END) AS count_lob_columns,
								SUM(CASE max_length when -1 THEN 0 ELSE max_length END) AS sum_max_length,
								COUNT(*) as total_columns
							FROM #IndexColumns ic
							WHERE index_id in (1,0) /*Heap or clustered only*/
							GROUP BY object_id
							)
				INSERT	#BlitzIndexResults ( check_id, index_sanity_id, findings_group, finding, URL, details, index_definition,
											   secret_columns, index_usage_summary, index_size_summary )
						SELECT	26 AS check_id, 
								i.index_sanity_id, 
								N'Index Hoarder' AS findings_group,
								N'Wide tables: 35+ cols or &gt; 2000 non-LOB bytes' AS finding,
								N'http://BrentOzar.com/go/IndexHoarder' AS URL,
								i.schema_object_name 
									+ N' has ' + CAST((total_columns) as NVARCHAR(10))
									+ N' total columns with a max possible width of ' + CAST(sum_max_length as NVARCHAR(10))
									+ N' bytes.' +
									CASE WHEN count_lob_columns &gt; 0 THEN CAST((count_lob_columns) as NVARCHAR(10))
										+ ' columns are LOB types.' ELSE ''
									END
										AS details,
								i.index_definition,
								secret_columns, 
								ISNULL(i.index_usage_summary,''),
								ISNULL(ip.index_size_summary,'')
						FROM	#IndexSanity i
						JOIN	#IndexSanitySize ip ON i.index_sanity_id = ip.index_sanity_id
						JOIN	count_columns AS cc ON i.[object_id]=cc.[object_id]
						WHERE	i.index_id in (1,0)
							and 
							(cc.total_columns &gt;= 35 OR
							cc.sum_max_length &gt;= 2000)
						ORDER BY i.schema_object_name DESC OPTION	( RECOMPILE );
					
			RAISERROR(N'check_id 27: Addicted to strings.', 0,1) WITH NOWAIT;
				WITH count_columns AS (
							SELECT [object_id],
								SUM(CASE WHEN system_type_name in ('varchar','nvarchar','char') or max_length=-1 THEN 1 ELSE 0 END) as string_or_LOB_columns,
								COUNT(*) as total_columns
							FROM #IndexColumns ic
							WHERE index_id in (1,0) /*Heap or clustered only*/
							GROUP BY object_id
							)
				INSERT	#BlitzIndexResults ( check_id, index_sanity_id, findings_group, finding, URL, details, index_definition,
											   secret_columns, index_usage_summary, index_size_summary )
						SELECT	27 AS check_id, 
								i.index_sanity_id, 
								N'Index Hoarder' AS findings_group,
								N'Addicted to strings' AS finding,
								N'http://BrentOzar.com/go/IndexHoarder' AS URL,
								i.schema_object_name 
									+ N' uses string or LOB types for ' + CAST((string_or_LOB_columns) as NVARCHAR(10))
									+ N' of ' + CAST(total_columns as NVARCHAR(10))
									+ N' columns. Check if data types are valid.' AS details,
								i.index_definition,
								secret_columns, 
								ISNULL(i.index_usage_summary,''),
								ISNULL(ip.index_size_summary,'')
						FROM	#IndexSanity i
						JOIN	#IndexSanitySize ip ON i.index_sanity_id = ip.index_sanity_id
						JOIN	count_columns AS cc ON i.[object_id]=cc.[object_id]
						CROSS APPLY (SELECT cc.total_columns - string_or_LOB_columns AS non_string_or_lob_columns) AS calc1
						WHERE	i.index_id in (1,0)
							AND calc1.non_string_or_lob_columns &lt;= 1
							AND cc.total_columns &gt; 3
						ORDER BY i.schema_object_name DESC OPTION	( RECOMPILE );

			RAISERROR(N'check_id 28: Non-unique clustered index.', 0,1) WITH NOWAIT;
				INSERT	#BlitzIndexResults ( check_id, index_sanity_id, findings_group, finding, URL, details, index_definition,
											   secret_columns, index_usage_summary, index_size_summary )
						SELECT	28 AS check_id, 
								i.index_sanity_id, 
								N'Index Hoarder' AS findings_group,
								N'Non-Unique clustered index' AS finding,
								N'http://BrentOzar.com/go/IndexHoarder' AS URL,
								N'Uniquifiers will be required! Clustered index: ' + i.schema_object_name 
									+ N' and all NC indexes. ' + 
										(SELECT CAST(COUNT(*) AS NVARCHAR(23)) FROM #IndexSanity i2 
										WHERE i2.[object_id]=i.[object_id] AND i2.index_id &lt;&gt; 1
										AND i2.is_disabled=0 AND i2.is_hypothetical=0)
										+ N' NC indexes on the table.'
									AS details,
								i.index_definition,
								secret_columns, 
								i.index_usage_summary,
								ip.index_size_summary
						FROM	#IndexSanity i
						JOIN	#IndexSanitySize ip ON i.index_sanity_id = ip.index_sanity_id
						WHERE	index_id = 1 /* clustered only */
								AND is_unique=0 /* not unique */
								AND is_CX_columnstore=0 /* not a clustered columnstore-- no unique option on those */
						ORDER BY i.schema_object_name DESC OPTION	( RECOMPILE );


		END
		 ----------------------------------------
		--Feature-Phobic Indexes: Check_id 30-39
		---------------------------------------- 
		BEGIN
			RAISERROR(N'check_id 30: No indexes with includes', 0,1) WITH NOWAIT;

			DECLARE	@number_indexes_with_includes INT;
			DECLARE	@percent_indexes_with_includes NUMERIC(10, 1);

			SELECT	@number_indexes_with_includes = SUM(CASE WHEN count_included_columns &gt; 0 THEN 1 ELSE 0	END),
					@percent_indexes_with_includes = 100.* 
						SUM(CASE WHEN count_included_columns &gt; 0 THEN 1 ELSE 0 END) / ( 1.0 * COUNT(*) )
			FROM	#IndexSanity;

			IF @number_indexes_with_includes = 0 
				INSERT	#BlitzIndexResults ( check_id, index_sanity_id, findings_group, finding, URL, details, index_definition,
											   secret_columns, index_usage_summary, index_size_summary )
						SELECT	30 AS check_id, 
								NULL AS index_sanity_id, 
								N'Feature-Phobic Indexes' AS findings_group,
								N'No indexes use includes' AS finding, 'http://BrentOzar.com/go/IndexFeatures' AS URL,
								N'No indexes use includes' AS details,
								N'Entire database' AS index_definition, 
								N'' AS secret_columns, 
								N'N/A' AS index_usage_summary, 
								N'N/A' AS index_size_summary OPTION	( RECOMPILE );

			RAISERROR(N'check_id 31: &lt; 3 percent of indexes have includes', 0,1) WITH NOWAIT;
			IF @percent_indexes_with_includes &lt;= 3 AND @number_indexes_with_includes &gt; 0 
				INSERT	#BlitzIndexResults ( check_id, index_sanity_id, findings_group, finding, URL, details, index_definition,
											   secret_columns, index_usage_summary, index_size_summary )
						SELECT	31 AS check_id,
								NULL AS index_sanity_id, 
								N'Feature-Phobic Indexes' AS findings_group,
								N'Borderline: Includes are used in &lt; 3% of indexes' AS findings,
								N'http://BrentOzar.com/go/IndexFeatures' AS URL,
								N'Only ' + CAST(@percent_indexes_with_includes AS NVARCHAR(10)) + '% of indexes have includes' AS details, 
								N'Entire database' AS index_definition, 
								N'' AS secret_columns,
								N'N/A' AS index_usage_summary, 
								N'N/A' AS index_size_summary OPTION	( RECOMPILE );

			RAISERROR(N'check_id 32: filtered indexes and indexed views', 0,1) WITH NOWAIT;
			DECLARE @count_filtered_indexes INT;
			DECLARE @count_indexed_views INT;

				SELECT	@count_filtered_indexes=COUNT(*)
				FROM	#IndexSanity
				WHERE	filter_definition &lt;&gt; '' OPTION	( RECOMPILE );

				SELECT	@count_indexed_views=COUNT(*)
				FROM	#IndexSanity AS i
						JOIN #IndexSanitySize AS sz ON i.index_sanity_id = sz.index_sanity_id
				WHERE	is_indexed_view = 1 OPTION	( RECOMPILE );

			IF @count_filtered_indexes = 0 AND @count_indexed_views=0
				INSERT	#BlitzIndexResults ( check_id, index_sanity_id, findings_group, finding, URL, details, index_definition,
											   secret_columns, index_usage_summary, index_size_summary )
						SELECT	32 AS check_id, 
								NULL AS index_sanity_id,
								N'Feature-Phobic Indexes' AS findings_group,
								N'Borderline: No filtered indexes or indexed views exist' AS finding, 
								N'http://BrentOzar.com/go/IndexFeatures' AS URL,
								N'These are NOT always needed-- but do you know when you would use them?' AS details,
								N'Entire database' AS index_definition, 
								N'' AS secret_columns,
								N'N/A' AS index_usage_summary, 
								N'N/A' AS index_size_summary OPTION	( RECOMPILE );
		END;

		RAISERROR(N'check_id 33: Potential filtered indexes based on column names.', 0,1) WITH NOWAIT;

		INSERT	#BlitzIndexResults ( check_id, index_sanity_id, findings_group, finding, URL, details, index_definition,
										secret_columns, index_usage_summary, index_size_summary )
		SELECT	33 AS check_id, 
				i.index_sanity_id AS index_sanity_id,
				N'Feature-Phobic Indexes' AS findings_group,
				N'Potential filtered index (based on column name)' AS finding, 
				N'http://BrentOzar.com/go/IndexFeatures' AS URL,
				N'A column name in this index suggests it might be a candidate for filtering (is%, %archive%, %active%, %flag%)' AS details,
				i.index_definition, 
				i.secret_columns,
				i.index_usage_summary, 
				sz.index_size_summary
		FROM #IndexColumns ic 
		join #IndexSanity i on 
			ic.[object_id]=i.[object_id] and
			ic.[index_id]=i.[index_id] and
			i.[index_id] &gt; 1 /* non-clustered index */
		JOIN	#IndexSanitySize AS sz ON i.index_sanity_id = sz.index_sanity_id
		WHERE column_name like 'is%'
			or column_name like '%archive%'
			or column_name like '%active%'
			or column_name like '%flag%'
		OPTION	( RECOMPILE );

		 ----------------------------------------
		--Self Loathing Indexes : Check_id 40-49
		----------------------------------------
		BEGIN

			RAISERROR(N'check_id 40: Fillfactor in nonclustered 80 percent or less', 0,1) WITH NOWAIT;
			INSERT	#BlitzIndexResults ( check_id, index_sanity_id, findings_group, finding, URL, details, index_definition,
										   secret_columns, index_usage_summary, index_size_summary )
					SELECT	40 AS check_id, 
							i.index_sanity_id,
							N'Self Loathing Indexes' AS findings_group,
							N'Low Fill Factor: nonclustered index' AS finding, 
							N'http://BrentOzar.com/go/SelfLoathing' AS URL,
							N'Fill factor on ' + schema_object_indexid + N' is ' + CAST(fill_factor AS NVARCHAR(10)) + N'%. '+
								CASE WHEN (last_user_update is null OR user_updates &lt; 1)
								THEN N'No writes have been made.'
								ELSE
									N'Last write was ' +  CONVERT(NVARCHAR(16),last_user_update,121) + N' and ' + 
									CAST(user_updates as NVARCHAR(25)) + N' updates have been made.'
								END
								AS details, 
							i.index_definition,
							i.secret_columns,
							i.index_usage_summary,
							sz.index_size_summary
					FROM	#IndexSanity AS i
					JOIN	#IndexSanitySize AS sz ON i.index_sanity_id = sz.index_sanity_id
					WHERE	index_id &gt; 1
					and	fill_factor BETWEEN 1 AND 80 OPTION	( RECOMPILE );

			RAISERROR(N'check_id 40: Fillfactor in clustered 90 percent or less', 0,1) WITH NOWAIT;
			INSERT	#BlitzIndexResults ( check_id, index_sanity_id, findings_group, finding, URL, details, index_definition,
										   secret_columns, index_usage_summary, index_size_summary )
					SELECT	40 AS check_id, 
							i.index_sanity_id,
							N'Self Loathing Indexes' AS findings_group,
							N'Low Fill Factor: clustered index' AS finding, 
							N'http://BrentOzar.com/go/SelfLoathing' AS URL,
							N'Fill factor on ' + schema_object_indexid + N' is ' + CAST(fill_factor AS NVARCHAR(10)) + N'%. '+
								CASE WHEN (last_user_update is null OR user_updates &lt; 1)
								THEN N'No writes have been made.'
								ELSE
									N'Last write was ' +  CONVERT(NVARCHAR(16),last_user_update,121) + N' and ' + 
									CAST(user_updates as NVARCHAR(25)) + N' updates have been made.'
								END
								AS details, 
							i.index_definition,
							i.secret_columns,
							i.index_usage_summary,
							sz.index_size_summary
					FROM	#IndexSanity AS i
					JOIN #IndexSanitySize AS sz ON i.index_sanity_id = sz.index_sanity_id
					WHERE	index_id = 1
					and fill_factor BETWEEN 1 AND 90 OPTION	( RECOMPILE );


			RAISERROR(N'check_id 41: Hypothetical indexes ', 0,1) WITH NOWAIT;
			INSERT	#BlitzIndexResults ( check_id, findings_group, finding, URL, details, index_definition,
										   secret_columns, index_usage_summary, index_size_summary )
					SELECT	41 AS check_id, 
							N'Self Loathing Indexes' AS findings_group,
							N'Hypothetical Index' AS finding, 'http://BrentOzar.com/go/SelfLoathing' AS URL,
							N'Hypothetical Index: ' + schema_object_indexid AS details, 
							i.index_definition,
							i.secret_columns,
							N'' AS index_usage_summary, 
							N'' AS index_size_summary
					FROM	#IndexSanity AS i
					WHERE	is_hypothetical = 1 OPTION	( RECOMPILE );


			RAISERROR(N'check_id 42: Disabled indexes', 0,1) WITH NOWAIT;
			--Note: disabled NC indexes will have O rows in #IndexSanitySize!
			INSERT	#BlitzIndexResults ( check_id, index_sanity_id, findings_group, finding, URL, details, index_definition,
										   secret_columns, index_usage_summary, index_size_summary )
					SELECT	42 AS check_id, 
							index_sanity_id,
							N'Self Loathing Indexes' AS findings_group,
							N'Disabled Index' AS finding, 
							N'http://BrentOzar.com/go/SelfLoathing' AS URL,
							N'Disabled Index:' + schema_object_indexid AS details, 
							i.index_definition,
							i.secret_columns,
							i.index_usage_summary,
							'DISABLED' AS index_size_summary
					FROM	#IndexSanity AS i
					WHERE	is_disabled = 1 OPTION	( RECOMPILE );

			RAISERROR(N'check_id 43: Heaps with forwarded records or deletes', 0,1) WITH NOWAIT;
			WITH	heaps_cte
					  AS ( SELECT	[object_id], 
									SUM(forwarded_fetch_count) AS forwarded_fetch_count,
									SUM(leaf_delete_count) AS leaf_delete_count
						   FROM		#IndexPartitionSanity
						   GROUP BY	[object_id]
						   HAVING	SUM(forwarded_fetch_count) &gt; 0
									OR SUM(leaf_delete_count) &gt; 0)
				INSERT	#BlitzIndexResults ( check_id, index_sanity_id, findings_group, finding, URL, details, index_definition,
											   secret_columns, index_usage_summary, index_size_summary )
						SELECT	43 AS check_id, 
								i.index_sanity_id,
								N'Self Loathing Indexes' AS findings_group,
								N'Heaps with forwarded records or deletes' AS finding, 
								N'http://BrentOzar.com/go/SelfLoathing' AS URL,
								CAST(h.forwarded_fetch_count AS NVARCHAR(256)) + ' forwarded fetches, '
								+ CAST(h.leaf_delete_count AS NVARCHAR(256)) + ' deletes against heap:'
								+ schema_object_indexid AS details, 
								i.index_definition, 
								i.secret_columns,
								i.index_usage_summary,
								sz.index_size_summary
						FROM	#IndexSanity i
						JOIN heaps_cte h ON i.[object_id] = h.[object_id]
						JOIN #IndexSanitySize sz ON i.index_sanity_id = sz.index_sanity_id
						WHERE	i.index_id = 0 
				OPTION	( RECOMPILE );

			RAISERROR(N'check_id 44: Heaps with reads or writes.', 0,1) WITH NOWAIT;
			WITH	heaps_cte
					  AS ( SELECT	[object_id], SUM(forwarded_fetch_count) AS forwarded_fetch_count,
									SUM(leaf_delete_count) AS leaf_delete_count
						   FROM		#IndexPartitionSanity
						   GROUP BY	[object_id]
						   HAVING	SUM(forwarded_fetch_count) &gt; 0
									OR SUM(leaf_delete_count) &gt; 0)
				INSERT	#BlitzIndexResults ( check_id, index_sanity_id, findings_group, finding, URL, details, index_definition,
											   secret_columns, index_usage_summary, index_size_summary )
						SELECT	44 AS check_id, 
								i.index_sanity_id,
								N'Self Loathing Indexes' AS findings_group,
								N'Active heap' AS finding, 
								N'http://BrentOzar.com/go/SelfLoathing' AS URL,
								N'Should this table be a heap? ' + schema_object_indexid AS details, 
								i.index_definition, 
								'N/A' AS secret_columns,
								i.index_usage_summary,
								sz.index_size_summary
						FROM	#IndexSanity i
						LEFT JOIN heaps_cte h ON i.[object_id] = h.[object_id]
						JOIN #IndexSanitySize sz ON i.index_sanity_id = sz.index_sanity_id
						WHERE	i.index_id = 0 
								AND 
									(i.total_reads &gt; 0 OR i.user_updates &gt; 0)
								AND h.[object_id] IS NULL /*don't duplicate the prior check.*/
				OPTION	( RECOMPILE );


			END;
		----------------------------------------
		--Indexaphobia
		--Missing indexes with value &gt;= 5 million: : Check_id 50-59
		----------------------------------------
		BEGIN
			RAISERROR(N'check_id 50: Indexaphobia.', 0,1) WITH NOWAIT;
			WITH	index_size_cte
					  AS ( SELECT	i.[object_id], 
									MAX(i.index_sanity_id) AS index_sanity_id,
								ISNULL (
									CAST(SUM(CASE WHEN index_id NOT IN (0,1) THEN 1 ELSE 0 END)
										 AS NVARCHAR(30))+ N' NC indexes exist (' + 
									CASE WHEN SUM(CASE WHEN index_id NOT IN (0,1) THEN sz.total_reserved_MB ELSE 0 END) &gt; 1024
										THEN CAST(CAST(SUM(CASE WHEN index_id NOT IN (0,1) THEN sz.total_reserved_MB ELSE 0 END )/1024. 
											AS NUMERIC(29,1)) AS NVARCHAR(30)) + N'GB); ' 
										ELSE CAST(SUM(CASE WHEN index_id NOT IN (0,1) THEN sz.total_reserved_MB ELSE 0 END) 
											AS NVARCHAR(30)) + N'MB); '
									END + 
										CASE WHEN MAX(sz.[total_rows]) &gt;= 922337203685477 THEN '&gt;= 922,337,203,685,477'
										ELSE REPLACE(CONVERT(NVARCHAR(30),CAST(MAX(sz.[total_rows]) AS money), 1), '.00', '') 
										END +
									+ N' Estimated Rows;' 
								,N'') AS index_size_summary
							FROM	#IndexSanity AS i
							LEFT	JOIN #IndexSanitySize AS sz ON i.index_sanity_id = sz.index_sanity_id
						   GROUP BY	i.[object_id])
				INSERT	#BlitzIndexResults ( check_id, index_sanity_id, findings_group, finding, URL, details, index_definition,
											   index_usage_summary, index_size_summary, create_tsql, more_info )
						SELECT	50 AS check_id, 
								sz.index_sanity_id,
								N'Indexaphobia' AS findings_group,
								N'High value missing index' AS finding, 
								N'http://BrentOzar.com/go/Indexaphobia' AS URL,
								mi.[statement] + ' estimated benefit: ' + 
									CASE WHEN magic_benefit_number &gt;= 922337203685477 THEN '&gt;= 922,337,203,685,477'
									ELSE REPLACE(CONVERT(NVARCHAR(256),CAST(CAST(magic_benefit_number AS BIGINT) AS money), 1), '.00', '') 
									END AS details,
								missing_index_details AS [definition],
								index_estimated_impact,
								sz.index_size_summary,
								mi.create_tsql,
								mi.more_info
				FROM	#MissingIndexes mi
						LEFT JOIN index_size_cte sz ON mi.[object_id] = sz.object_id
				WHERE magic_benefit_number &gt; 500000
				ORDER BY magic_benefit_number DESC;

	END
		 ----------------------------------------
		--Abnormal Psychology : Check_id 60-79
		----------------------------------------
	BEGIN
			RAISERROR(N'check_id 60: XML indexes', 0,1) WITH NOWAIT;
			INSERT	#BlitzIndexResults ( check_id, index_sanity_id, findings_group, finding, URL, details, index_definition,
										   secret_columns, index_usage_summary, index_size_summary )
					SELECT	60 AS check_id, 
							i.index_sanity_id,
							N'Abnormal Psychology' AS findings_group,
							N'XML Indexes' AS finding, 
							N'http://BrentOzar.com/go/AbnormalPsychology' AS URL,
							i.schema_object_indexid AS details, 
							i.index_definition,
							i.secret_columns,
							N'' AS index_usage_summary,
							ISNULL(sz.index_size_summary,'') AS index_size_summary
					FROM	#IndexSanity AS i
					JOIN #IndexSanitySize sz ON i.index_sanity_id = sz.index_sanity_id
					WHERE i.is_XML = 1 OPTION	( RECOMPILE );

			RAISERROR(N'check_id 61: Columnstore indexes', 0,1) WITH NOWAIT;
			INSERT	#BlitzIndexResults ( check_id, index_sanity_id, findings_group, finding, URL, details, index_definition,
										   secret_columns, index_usage_summary, index_size_summary )
					SELECT	61 AS check_id, 
							i.index_sanity_id,
							N'Abnormal Psychology' AS findings_group,
							CASE WHEN i.is_NC_columnstore=1
								THEN N'NC Columnstore Index' 
								ELSE N'Clustered Columnstore Index' 
								END AS finding, 
							N'http://BrentOzar.com/go/AbnormalPsychology' AS URL,
							i.schema_object_indexid AS details, 
							i.index_definition,
							i.secret_columns,
							i.index_usage_summary,
							ISNULL(sz.index_size_summary,'') AS index_size_summary
					FROM	#IndexSanity AS i
					JOIN #IndexSanitySize sz ON i.index_sanity_id = sz.index_sanity_id
					WHERE i.is_NC_columnstore = 1 OR i.is_CX_columnstore=1
					OPTION	( RECOMPILE );


			RAISERROR(N'check_id 62: Spatial indexes', 0,1) WITH NOWAIT;
			INSERT	#BlitzIndexResults ( check_id, index_sanity_id, findings_group, finding, URL, details, index_definition,
										   secret_columns, index_usage_summary, index_size_summary )
					SELECT	62 AS check_id, 
							i.index_sanity_id,
							N'Abnormal Psychology' AS findings_group,
							N'Spatial indexes' AS finding, 
							N'http://BrentOzar.com/go/AbnormalPsychology' AS URL,
							i.schema_object_indexid AS details, 
							i.index_definition,
							i.secret_columns,
							i.index_usage_summary,
							ISNULL(sz.index_size_summary,'') AS index_size_summary
					FROM	#IndexSanity AS i
					JOIN #IndexSanitySize sz ON i.index_sanity_id = sz.index_sanity_id
					WHERE i.is_spatial = 1 OPTION	( RECOMPILE );

			RAISERROR(N'check_id 63: Compressed indexes', 0,1) WITH NOWAIT;
			INSERT	#BlitzIndexResults ( check_id, index_sanity_id, findings_group, finding, URL, details, index_definition,
										   secret_columns, index_usage_summary, index_size_summary )
					SELECT	63 AS check_id, 
							i.index_sanity_id,
							N'Abnormal Psychology' AS findings_group,
							N'Compressed indexes' AS finding, 
							N'http://BrentOzar.com/go/AbnormalPsychology' AS URL,
							i.schema_object_indexid  + N'. COMPRESSION: ' + sz.data_compression_desc AS details, 
							i.index_definition,
							i.secret_columns,
							i.index_usage_summary,
							ISNULL(sz.index_size_summary,'') AS index_size_summary
					FROM	#IndexSanity AS i
					JOIN #IndexSanitySize sz ON i.index_sanity_id = sz.index_sanity_id
					WHERE sz.data_compression_desc LIKE '%PAGE%' OR sz.data_compression_desc LIKE '%ROW%' OPTION	( RECOMPILE );

			RAISERROR(N'check_id 64: Partitioned', 0,1) WITH NOWAIT;
			INSERT	#BlitzIndexResults ( check_id, index_sanity_id, findings_group, finding, URL, details, index_definition,
										   secret_columns, index_usage_summary, index_size_summary )
					SELECT	64 AS check_id, 
							i.index_sanity_id,
							N'Abnormal Psychology' AS findings_group,
							N'Partitioned indexes' AS finding, 
							N'http://BrentOzar.com/go/AbnormalPsychology' AS URL,
							i.schema_object_indexid AS details, 
							i.index_definition,
							i.secret_columns,
							i.index_usage_summary,
							ISNULL(sz.index_size_summary,'') AS index_size_summary
					FROM	#IndexSanity AS i
					JOIN #IndexSanitySize sz ON i.index_sanity_id = sz.index_sanity_id
					WHERE i.partition_key_column_name IS NOT NULL OPTION	( RECOMPILE );

			RAISERROR(N'check_id 65: Non-Aligned Partitioned', 0,1) WITH NOWAIT;
			INSERT	#BlitzIndexResults ( check_id, index_sanity_id, findings_group, finding, URL, details, index_definition,
										   secret_columns, index_usage_summary, index_size_summary )
					SELECT	65 AS check_id, 
							i.index_sanity_id,
							N'Abnormal Psychology' AS findings_group,
							N'Non-Aligned index on a partitioned table' AS finding, 
							N'http://BrentOzar.com/go/AbnormalPsychology' AS URL,
							i.schema_object_indexid AS details, 
							i.index_definition,
							i.secret_columns,
							i.index_usage_summary,
							ISNULL(sz.index_size_summary,'') AS index_size_summary
					FROM	#IndexSanity AS i
					JOIN #IndexSanity AS iParent ON
						i.[object_id]=iParent.[object_id]
						AND iParent.index_id IN (0,1) /* could be a partitioned heap or clustered table */
						AND iParent.partition_key_column_name IS NOT NULL /* parent is partitioned*/         
					JOIN #IndexSanitySize sz ON i.index_sanity_id = sz.index_sanity_id
					WHERE i.partition_key_column_name IS NULL 
						OPTION	( RECOMPILE );

			RAISERROR(N'check_id 66: Recently created tables/indexes (1 week)', 0,1) WITH NOWAIT;
			INSERT	#BlitzIndexResults ( check_id, index_sanity_id, findings_group, finding, URL, details, index_definition,
										   secret_columns, index_usage_summary, index_size_summary )
					SELECT	66 AS check_id, 
							i.index_sanity_id,
							N'Abnormal Psychology' AS findings_group,
							N'Recently created tables/indexes (1 week)' AS finding, 
							N'http://BrentOzar.com/go/AbnormalPsychology' AS URL,
							i.schema_object_indexid + N' was created on ' + 
								CONVERT(NVARCHAR(16),i.create_date,121) + 
								N'. Tables/indexes which are dropped/created regularly require special methods for index tuning.'
									 AS details, 
							i.index_definition,
							i.secret_columns,
							i.index_usage_summary,
							ISNULL(sz.index_size_summary,'') AS index_size_summary
					FROM	#IndexSanity AS i
					JOIN #IndexSanitySize sz ON i.index_sanity_id = sz.index_sanity_id
					WHERE i.create_date &gt;= DATEADD(dd,-7,GETDATE()) 
						OPTION	( RECOMPILE );

			RAISERROR(N'check_id 67: Recently modified tables/indexes (2 days)', 0,1) WITH NOWAIT;
			INSERT	#BlitzIndexResults ( check_id, index_sanity_id, findings_group, finding, URL, details, index_definition,
										   secret_columns, index_usage_summary, index_size_summary )
					SELECT	67 AS check_id, 
							i.index_sanity_id,
							N'Abnormal Psychology' AS findings_group,
							N'Recently modified tables/indexes (2 days)' AS finding, 
							N'http://BrentOzar.com/go/AbnormalPsychology' AS URL,
							i.schema_object_indexid + N' was modified on ' + 
								CONVERT(NVARCHAR(16),i.modify_date,121) + 
								N'. A large amount of recently modified indexes may mean a lot of rebuilds are occurring each night.'
									 AS details, 
							i.index_definition,
							i.secret_columns,
							i.index_usage_summary,
							ISNULL(sz.index_size_summary,'') AS index_size_summary
					FROM	#IndexSanity AS i
					JOIN #IndexSanitySize sz ON i.index_sanity_id = sz.index_sanity_id
					WHERE i.modify_date &gt; DATEADD(dd,-2,GETDATE()) 
					and /*Exclude recently created tables unless they've been modified after being created.*/
					(i.create_date &lt; DATEADD(dd,-7,GETDATE()) or i.create_date &lt;&gt; i.modify_date)
						OPTION	( RECOMPILE );

			RAISERROR(N'check_id 68: Identity columns within 30 percent of the end of range', 0,1) WITH NOWAIT;
			-- Allowed Ranges: 
				--int -2,147,483,648 to 2,147,483,647
				--smallint -32,768 to 32,768
				--tinyint 0 to 255

				INSERT	#BlitzIndexResults ( check_id, index_sanity_id, findings_group, finding, URL, details, index_definition,
											   secret_columns, index_usage_summary, index_size_summary )
						SELECT	68 AS check_id, 
								i.index_sanity_id, 
								N'Abnormal Psychology' AS findings_group,
								N'Identity column within ' + 									
									CAST (calc1.percent_remaining as nvarchar(256))
									+ N' percent  end of range' AS finding,
								N'http://BrentOzar.com/go/AbnormalPsychology' AS URL,
								i.schema_object_name + N'.' +  QUOTENAME(ic.column_name)
									+ N' is an identity with type ' + ic.system_type_name 
									+ N', last value of ' 
										+ ISNULL(REPLACE(CONVERT(NVARCHAR(256),CAST(CAST(ic.last_value AS BIGINT) AS money), 1), '.00', ''),N'NULL')
									+ N', seed of '
										+ ISNULL(REPLACE(CONVERT(NVARCHAR(256),CAST(CAST(ic.seed_value AS BIGINT) AS money), 1), '.00', ''),N'NULL')
									+ N', increment of ' + CAST(ic.increment_value AS NVARCHAR(256)) 
									+ N', and range of ' +
										CASE ic.system_type_name WHEN 'int' THEN N'+/- 2,147,483,647'
											WHEN 'smallint' THEN N'+/- 32,768'
											WHEN 'tinyint' THEN N'0 to 255'
										END
										AS details,
								i.index_definition,
								secret_columns, 
								ISNULL(i.index_usage_summary,''),
								ISNULL(ip.index_size_summary,'')
						FROM	#IndexSanity i
						JOIN	#IndexColumns ic on
							i.object_id=ic.object_id
							and i.index_id in (0,1) /* heaps and cx only */
							and ic.is_identity=1
							and ic.system_type_name in ('tinyint', 'smallint', 'int')
						JOIN	#IndexSanitySize ip ON i.index_sanity_id = ip.index_sanity_id
						CROSS APPLY (
							SELECT CAST(CASE WHEN ic.increment_value &gt;= 0
									THEN
										CASE ic.system_type_name 
											WHEN 'int' then (2147483647 - (ISNULL(ic.last_value,ic.seed_value) + ic.increment_value)) / 2147483647.*100
											WHEN 'smallint' then (32768 - (ISNULL(ic.last_value,ic.seed_value) + ic.increment_value)) / 32768.*100
											WHEN 'tinyint' then ( 255 - (ISNULL(ic.last_value,ic.seed_value) + ic.increment_value)) / 255.*100
											ELSE 999
										END
								ELSE --ic.increment_value is negative
										CASE ic.system_type_name 
											WHEN 'int' then ABS(-2147483647 - (ISNULL(ic.last_value,ic.seed_value) + ic.increment_value)) / 2147483647.*100
											WHEN 'smallint' then ABS(-32768 - (ISNULL(ic.last_value,ic.seed_value) + ic.increment_value)) / 32768.*100
											WHEN 'tinyint' then ABS( 0 - (ISNULL(ic.last_value,ic.seed_value) + ic.increment_value)) / 255.*100
											ELSE -1
										END 
								END AS NUMERIC(5,1)) AS percent_remaining
								) as calc1
						WHERE	i.index_id in (1,0)
							and calc1.percent_remaining &lt;= 30
						UNION ALL
						SELECT	68 AS check_id, 
								i.index_sanity_id, 
								N'Abnormal Psychology' AS findings_group,
								N'Identity column using a negative seed or increment other than 1' AS finding,
								N'http://BrentOzar.com/go/AbnormalPsychology' AS URL,
								i.schema_object_name + N'.' +  QUOTENAME(ic.column_name)
									+ N' is an identity with type ' + ic.system_type_name 
									+ N', last value of ' 
										+ ISNULL(REPLACE(CONVERT(NVARCHAR(256),CAST(CAST(ic.last_value AS BIGINT) AS money), 1), '.00', ''),N'NULL')
									+ N', seed of '
										+ ISNULL(REPLACE(CONVERT(NVARCHAR(256),CAST(CAST(ic.seed_value AS BIGINT) AS money), 1), '.00', ''),N'NULL')
									+ N', increment of ' + CAST(ic.increment_value AS NVARCHAR(256)) 
									+ N', and range of ' +
										CASE ic.system_type_name WHEN 'int' THEN N'+/- 2,147,483,647'
											WHEN 'smallint' THEN N'+/- 32,768'
											WHEN 'tinyint' THEN N'0 to 255'
										END
										AS details,
								i.index_definition,
								secret_columns, 
								ISNULL(i.index_usage_summary,''),
								ISNULL(ip.index_size_summary,'')
						FROM	#IndexSanity i
						JOIN	#IndexColumns ic on
							i.object_id=ic.object_id
							and i.index_id in (0,1) /* heaps and cx only */
							and ic.is_identity=1
							and ic.system_type_name in ('tinyint', 'smallint', 'int')
						JOIN	#IndexSanitySize ip ON i.index_sanity_id = ip.index_sanity_id
						WHERE	i.index_id in (1,0)
							and (ic.seed_value &lt; 0 or ic.increment_value &lt;&gt; 1)
						ORDER BY finding, details DESC OPTION	( RECOMPILE );

			RAISERROR(N'check_id 69: Column collation does not match database collation', 0,1) WITH NOWAIT;
				WITH count_columns AS (
							SELECT [object_id],
								COUNT(*) as column_count
							FROM #IndexColumns ic
							WHERE index_id in (1,0) /*Heap or clustered only*/
								and collation_name &lt;&gt; @collation
							GROUP BY object_id
							)
				INSERT	#BlitzIndexResults ( check_id, index_sanity_id, findings_group, finding, URL, details, index_definition,
											   secret_columns, index_usage_summary, index_size_summary )
						SELECT	69 AS check_id, 
								i.index_sanity_id, 
								N'Abnormal Psychology' AS findings_group,
								N'Column collation does not match database collation' AS finding,
								N'http://BrentOzar.com/go/AbnormalPsychology' AS URL,
								i.schema_object_name 
									+ N' has ' + CAST(column_count AS NVARCHAR(20))
									+ N' column' + CASE WHEN column_count &gt; 1 THEN 's' ELSE '' END
									+ N' with a different collation than the db collation of '
									+ @collation	AS details,
								i.index_definition,
								secret_columns, 
								ISNULL(i.index_usage_summary,''),
								ISNULL(ip.index_size_summary,'')
						FROM	#IndexSanity i
						JOIN	#IndexSanitySize ip ON i.index_sanity_id = ip.index_sanity_id
						JOIN	count_columns AS cc ON i.[object_id]=cc.[object_id]
						WHERE	i.index_id in (1,0)
						ORDER BY i.schema_object_name DESC OPTION	( RECOMPILE );

			RAISERROR(N'check_id 70: Replicated columns', 0,1) WITH NOWAIT;
				WITH count_columns AS (
							SELECT [object_id],
								COUNT(*) as column_count,
								SUM(CASE is_replicated WHEN 1 THEN 1 ELSE 0 END) as replicated_column_count
							FROM #IndexColumns ic
							WHERE index_id in (1,0) /*Heap or clustered only*/
							GROUP BY object_id
							)
				INSERT	#BlitzIndexResults ( check_id, index_sanity_id, findings_group, finding, URL, details, index_definition,
											   secret_columns, index_usage_summary, index_size_summary )
						SELECT	70 AS check_id, 
								i.index_sanity_id, 
								N'Abnormal Psychology' AS findings_group,
								N'Replicated columns' AS finding,
								N'http://BrentOzar.com/go/AbnormalPsychology' AS URL,
								i.schema_object_name 
									+ N' has ' + CAST(replicated_column_count AS NVARCHAR(20))
									+ N' out of ' + CAST(column_count AS NVARCHAR(20))
									+ N' column' + CASE WHEN column_count &gt; 1 THEN 's' ELSE '' END
									+ N' in one or more publications.'
										AS details,
								i.index_definition,
								secret_columns, 
								ISNULL(i.index_usage_summary,''),
								ISNULL(ip.index_size_summary,'')
						FROM	#IndexSanity i
						JOIN	#IndexSanitySize ip ON i.index_sanity_id = ip.index_sanity_id
						JOIN	count_columns AS cc ON i.[object_id]=cc.[object_id]
						WHERE	i.index_id in (1,0)
							and replicated_column_count &gt; 0
						ORDER BY i.schema_object_name DESC OPTION	( RECOMPILE );

			RAISERROR(N'check_id 71: Cascading updates or cascading deletes.', 0,1) WITH NOWAIT;
			INSERT	#BlitzIndexResults ( check_id, index_sanity_id, findings_group, finding, URL, details, index_definition,
								   secret_columns, index_usage_summary, index_size_summary, more_info )
			SELECT	71 AS check_id, 
					null as index_sanity_id,
					N'Abnormal Psychology' AS findings_group,
					N'Cascading Updates or Deletes' AS finding, 
					N'http://BrentOzar.com/go/AbnormalPsychology' AS URL,
					N'Foreign Key ' + foreign_key_name +
					N' on ' + QUOTENAME(parent_object_name)  + N'(' + LTRIM(parent_fk_columns) + N')'
						+ N' referencing ' + QUOTENAME(referenced_object_name) + N'(' + LTRIM(referenced_fk_columns) + N')'
						+ N' has settings:'
						+ CASE [delete_referential_action_desc] WHEN N'NO_ACTION' THEN N'' ELSE N' ON DELETE ' +[delete_referential_action_desc] END
						+ CASE [update_referential_action_desc] WHEN N'NO_ACTION' THEN N'' ELSE N' ON UPDATE ' + [update_referential_action_desc] END
							AS details, 
					N'N/A' 
							AS index_definition, 
					N'N/A' AS secret_columns,
					N'N/A' AS index_usage_summary,
					N'N/A' AS index_size_summary,
					(SELECT TOP 1 more_info from #IndexSanity i where i.object_id=fk.parent_object_id)
						AS more_info
			from #ForeignKeys fk
			where [delete_referential_action_desc] &lt;&gt; N'NO_ACTION'
			OR [update_referential_action_desc] &lt;&gt; N'NO_ACTION'

	END

		 ----------------------------------------
		--Workaholics: Check_id 80-89
		----------------------------------------
	BEGIN

		RAISERROR(N'check_id 80: Most scanned indexes (index_usage_stats)', 0,1) WITH NOWAIT;
		INSERT	#BlitzIndexResults ( check_id, index_sanity_id, findings_group, finding, URL, details, index_definition,
							   secret_columns, index_usage_summary, index_size_summary )

		--Workaholics according to index_usage_stats
		--This isn't perfect: it mentions the number of scans present in a plan
		--A "scan" isn't necessarily a full scan, but hey, we gotta do the best with what we've got.
		--in the case of things like indexed views, the operator might be in the plan but never executed
		SELECT TOP 5 
			80 AS check_id,
			i.index_sanity_id as index_sanity_id,
			N'Workaholics' as findings_group,
			N'Scan-a-lots (index_usage_stats)' as finding,
			N'http://BrentOzar.com/go/Workaholics' AS URL,
			REPLACE(CONVERT( NVARCHAR(50),CAST(i.user_scans AS MONEY),1),'.00','')
				+ N' scans against ' + i.schema_object_indexid
				+ N'. Latest scan: ' + ISNULL(cast(i.last_user_scan as nvarchar(128)),'?') + N'. ' 
				+ N'ScanFactor=' + cast(((i.user_scans * iss.total_reserved_MB)/1000000.) as NVARCHAR(256)) as details,
			isnull(i.key_column_names_with_sort_order,'N/A') as index_definition,
			isnull(i.secret_columns,'') as secret_columns,
			i.index_usage_summary as index_usage_summary,
			iss.index_size_summary as index_size_summary
		FROM #IndexSanity i
		JOIN #IndexSanitySize iss on i.index_sanity_id=iss.index_sanity_id
		WHERE isnull(i.user_scans,0) &gt; 0
		ORDER BY  i.user_scans * iss.total_reserved_MB DESC;

		RAISERROR(N'check_id 81: Top recent accesses (op stats)', 0,1) WITH NOWAIT;
		INSERT	#BlitzIndexResults ( check_id, index_sanity_id, findings_group, finding, URL, details, index_definition,
							   secret_columns, index_usage_summary, index_size_summary )
		--Workaholics according to index_operational_stats
		--This isn't perfect either: range_scan_count contains full scans, partial scans, even seeks in nested loop ops
		--But this can help bubble up some most-accessed tables 
		SELECT TOP 5 
			81 as check_id,
			i.index_sanity_id as index_sanity_id,
			N'Workaholics' as findings_group,
			N'Top recent accesses (index_op_stats)' as finding,
			N'http://BrentOzar.com/go/Workaholics' AS URL,
			ISNULL(REPLACE(
					CONVERT(NVARCHAR(50),cast((iss.total_range_scan_count + iss.total_singleton_lookup_count) AS MONEY),1),
					N'.00',N'') 
				+ N' uses of ' + i.schema_object_indexid + N'. '
				+ REPLACE(CONVERT(NVARCHAR(50), CAST(iss.total_range_scan_count AS MONEY),1),N'.00',N'') + N' scans or seeks. '
				+ REPLACE(CONVERT(NVARCHAR(50), CAST(iss.total_singleton_lookup_count AS MONEY), 1),N'.00',N'') + N' singleton lookups. '
				+ N'OpStatsFactor=' + cast(((((iss.total_range_scan_count + iss.total_singleton_lookup_count) * iss.total_reserved_MB))/1000000.) as varchar(256)),'') as details,
			isnull(i.key_column_names_with_sort_order,'N/A') as index_definition,
			isnull(i.secret_columns,'') as secret_columns,
			i.index_usage_summary as index_usage_summary,
			iss.index_size_summary as index_size_summary
		FROM #IndexSanity i
		JOIN #IndexSanitySize iss on i.index_sanity_id=iss.index_sanity_id
		WHERE isnull(iss.total_range_scan_count,0)  &gt; 0 or isnull(iss.total_singleton_lookup_count,0) &gt; 0
		ORDER BY ((iss.total_range_scan_count + iss.total_singleton_lookup_count) * iss.total_reserved_MB) DESC;


	END

		 ----------------------------------------
		--FINISHING UP
		----------------------------------------
	BEGIN
				INSERT	#BlitzIndexResults ( check_id, findings_group, finding, URL, details, index_definition,secret_columns,
											   index_usage_summary, index_size_summary )
				VALUES  ( 1000 , N'Database ' + QUOTENAME(@DatabaseName) + N' as of ' + convert(nvarchar(16),getdate(),121)	,
						N'' ,   N'http://www.BrentOzar.com/BlitzIndex' ,
						N'Thanks from the Brent Ozar Unlimited(TM), LLC team.',
						N'We hope you found this tool useful.',
						N'If you need help relieving your SQL Server pains, email us at Help@BrentOzar.com.'
						, N'',N''
						);


	END
		RAISERROR(N'Returning results.', 0,1) WITH NOWAIT;
			
		/*Return results.*/
		SELECT isnull(br.findings_group,N'') + 
				CASE WHEN ISNULL(br.finding,N'') &lt;&gt; N'' THEN N': ' ELSE N'' END
				+ br.finding AS [Finding], 
			br.URL, 
			br.details AS [Details: schema.table.index(indexid)], 
			br.index_definition AS [Definition: [Property]] ColumnName {datatype maxbytes}], 
			ISNULL(br.secret_columns,'') AS [Secret Columns],          
			br.index_usage_summary AS [Usage], 
			br.index_size_summary AS [Size],
			COALESCE(br.more_info,sn.more_info,'') AS [More Info],
			COALESCE(br.create_tsql,ts.create_tsql,'') AS [Create TSQL]
		FROM #BlitzIndexResults br
		LEFT JOIN #IndexSanity sn ON 
			br.index_sanity_id=sn.index_sanity_id
		LEFT JOIN #IndexCreateTsql ts ON 
			br.index_sanity_id=ts.index_sanity_id
		ORDER BY [check_id] ASC, blitz_result_id ASC, findings_group;

	END; /* End @Mode=0 (diagnose)*/
	ELSE IF @Mode=1 /*Summarize*/
	BEGIN
	--This mode is to give some overall stats on the database.
		RAISERROR(N'@Mode=1, we are summarizing.', 0,1) WITH NOWAIT;

		SELECT 
			CAST((COUNT(*)) AS NVARCHAR(256)) AS [Number Objects],
			CAST(CAST(SUM(sz.total_reserved_MB)/
				1024. AS numeric(29,1)) AS NVARCHAR(500)) AS [All GB],
			CAST(CAST(SUM(sz.total_reserved_LOB_MB)/
				1024. AS numeric(29,1)) AS NVARCHAR(500)) AS [LOB GB],
			CAST(CAST(SUM(sz.total_reserved_row_overflow_MB)/
				1024. AS numeric(29,1)) AS NVARCHAR(500)) AS [Row Overflow GB],
			CAST(SUM(CASE WHEN index_id=1 THEN 1 ELSE 0 END)AS NVARCHAR(50)) AS [Clustered Tables],
			CAST(SUM(CASE WHEN index_id=1 THEN sz.total_reserved_MB ELSE 0 END)
				/1024. AS numeric(29,1)) AS [Clustered Tables GB],
			SUM(CASE WHEN index_id NOT IN (0,1) THEN 1 ELSE 0 END) AS [NC Indexes],
			CAST(SUM(CASE WHEN index_id NOT IN (0,1) THEN sz.total_reserved_MB ELSE 0 END)
				/1024. AS numeric(29,1)) AS [NC Indexes GB],
			CASE WHEN SUM(CASE WHEN index_id NOT IN (0,1) THEN sz.total_reserved_MB ELSE 0 END)  &gt; 0 THEN
				CAST(SUM(CASE WHEN index_id IN (0,1) THEN sz.total_reserved_MB ELSE 0 END)
					/ SUM(CASE WHEN index_id NOT IN (0,1) THEN sz.total_reserved_MB ELSE 0 END) AS NUMERIC(29,1)) 
				ELSE 0 END AS [ratio table: NC Indexes],
			SUM(CASE WHEN index_id=0 THEN 1 ELSE 0 END) AS [Heaps],
			CAST(SUM(CASE WHEN index_id=0 THEN sz.total_reserved_MB ELSE 0 END)
				/1024. AS numeric(29,1)) AS [Heaps GB],
			SUM(CASE WHEN index_id IN (0,1) AND partition_key_column_name IS NOT NULL THEN 1 ELSE 0 END) AS [Partitioned Tables],
			SUM(CASE WHEN index_id NOT IN (0,1) AND  partition_key_column_name IS NOT NULL THEN 1 ELSE 0 END) AS [Partitioned NCs],
			CAST(SUM(CASE WHEN partition_key_column_name IS NOT NULL THEN sz.total_reserved_MB ELSE 0 END)/1024. AS numeric(29,1)) AS [Partitioned GB],
			SUM(CASE WHEN filter_definition &lt;&gt; '' THEN 1 ELSE 0 END) AS [Filtered Indexes],
			SUM(CASE WHEN is_indexed_view=1 THEN 1 ELSE 0 END) AS [Indexed Views],
			MAX(total_rows) AS [Max Row Count],
			CAST(MAX(CASE WHEN index_id IN (0,1) THEN sz.total_reserved_MB ELSE 0 END)
				/1024. AS numeric(29,1)) AS [Max Table GB],
			CAST(MAX(CASE WHEN index_id NOT IN (0,1) THEN sz.total_reserved_MB ELSE 0 END)
				/1024. AS numeric(29,1)) AS [Max NC Index GB],
			SUM(CASE WHEN index_id IN (0,1) AND sz.total_reserved_MB &gt; 1024 THEN 1 ELSE 0 END) AS [Count Tables &gt; 1GB],
			SUM(CASE WHEN index_id IN (0,1) AND sz.total_reserved_MB &gt; 10240 THEN 1 ELSE 0 END) AS [Count Tables &gt; 10GB],
			SUM(CASE WHEN index_id IN (0,1) AND sz.total_reserved_MB &gt; 102400 THEN 1 ELSE 0 END) AS [Count Tables &gt; 100GB],	
			SUM(CASE WHEN index_id NOT IN (0,1) AND sz.total_reserved_MB &gt; 1024 THEN 1 ELSE 0 END) AS [Count NCs &gt; 1GB],
			SUM(CASE WHEN index_id NOT IN (0,1) AND sz.total_reserved_MB &gt; 10240 THEN 1 ELSE 0 END) AS [Count NCs &gt; 10GB],
			SUM(CASE WHEN index_id NOT IN (0,1) AND sz.total_reserved_MB &gt; 102400 THEN 1 ELSE 0 END) AS [Count NCs &gt; 100GB],
			MIN(create_date) AS [Oldest Create Date],
			MAX(create_date) AS [Most Recent Create Date],
			MAX(modify_date) as [Most Recent Modify Date],
			1 as [Display Order]
		FROM #IndexSanity AS i
		--left join here so we don't lose disabled nc indexes
		LEFT JOIN #IndexSanitySize AS sz 
			ON i.index_sanity_id=sz.index_sanity_id 
		UNION ALL
		SELECT	N'Database ' + QUOTENAME(@DatabaseName) + N' as of ' + convert(nvarchar(16),getdate(),121)	,		
				N'sp_BlitzIndex(TM) v2.02 - Jan 30, 2014' ,   
				N'From Brent Ozar Unlimited(TM)' ,   
				N'http://BrentOzar.com/BlitzIndex' ,
				N'Thanks from the Brent Ozar Unlimited(TM) team.  We hope you found this tool useful, and if you need help relieving your SQL Server pains, email us at Help@BrentOzar.com.',
				NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,
				NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,
				NULL,0 as display_order
		ORDER BY [Display Order] ASC
		OPTION (RECOMPILE);
	   	
	END /* End @Mode=1 (summarize)*/
	ELSE IF @Mode=2 /*Index Detail*/
	BEGIN
		--This mode just spits out all the detail without filters.
		--This supports slicing AND dicing in Excel
		RAISERROR(N'@Mode=2, here''s the details on existing indexes.', 0,1) WITH NOWAIT;

		SELECT	database_name AS [Database Name], 
				[schema_name] AS [Schema Name], 
				[object_name] AS [Object Name], 
				ISNULL(index_name, '') AS [Index Name], 
				cast(index_id as VARCHAR(10))AS [Index ID],
				schema_object_indexid AS [Details: schema.table.index(indexid)], 
				CASE	WHEN index_id IN ( 1, 0 ) THEN 'TABLE'
					ELSE 'NonClustered'
					END AS [Object Type], 
				index_definition AS [Definition: [Property]] ColumnName {datatype maxbytes}],
				ISNULL(LTRIM(key_column_names_with_sort_order), '') AS [Key Column Names With Sort],
				ISNULL(count_key_columns, 0) AS [Count Key Columns],
				ISNULL(include_column_names, '') AS [Include Column Names], 
				ISNULL(count_included_columns,0) AS [Count Included Columns],
				ISNULL(secret_columns,'') AS [Secret Column Names], 
				ISNULL(count_secret_columns,0) AS [Count Secret Columns],
				ISNULL(partition_key_column_name, '') AS [Partition Key Column Name],
				ISNULL(filter_definition, '') AS [Filter Definition], 
				is_indexed_view AS [Is Indexed View], 
				is_primary_key AS [Is Primary Key],
				is_XML AS [Is XML],
				is_spatial AS [Is Spatial],
				is_NC_columnstore AS [Is NC Columnstore],
				is_CX_columnstore AS [Is CX Columnstore],
				is_disabled AS [Is Disabled], 
				is_hypothetical AS [Is Hypothetical],
				is_padded AS [Is Padded], 
				fill_factor AS [Fill Factor], 
				is_referenced_by_foreign_key AS [Is Reference by Foreign Key], 
				last_user_seek AS [Last User Seek], 
				last_user_scan AS [Last User Scan], 
				last_user_lookup AS [Last User Lookup],
				last_user_update AS [Last User Update], 
				total_reads AS [Total Reads], 
				user_updates AS [User Updates], 
				reads_per_write AS [Reads Per Write], 
				index_usage_summary AS [Index Usage], 
				sz.partition_count AS [Partition Count],
				sz.total_rows AS [Rows], 
				sz.total_reserved_MB AS [Reserved MB], 
				sz.total_reserved_LOB_MB AS [Reserved LOB MB], 
				sz.total_reserved_row_overflow_MB AS [Reserved Row Overflow MB],
				sz.index_size_summary AS [Index Size], 
				sz.total_row_lock_count AS [Row Lock Count],
				sz.total_row_lock_wait_count AS [Row Lock Wait Count],
				sz.total_row_lock_wait_in_ms AS [Row Lock Wait ms],
				sz.avg_row_lock_wait_in_ms AS [Avg Row Lock Wait ms],
				sz.total_page_lock_count AS [Page Lock Count],
				sz.total_page_lock_wait_count AS [Page Lock Wait Count],
				sz.total_page_lock_wait_in_ms AS [Page Lock Wait ms],
				sz.avg_page_lock_wait_in_ms AS [Avg Page Lock Wait ms],
				sz.total_index_lock_promotion_attempt_count AS [Lock Escalation Attempts],
				sz.total_index_lock_promotion_count AS [Lock Escalations],
				sz.data_compression_desc AS [Data Compression],
				i.create_date AS [Create Date],
				i.modify_date as [Modify Date],
				more_info AS [More Info],
				1 as [Display Order]
		FROM	#IndexSanity AS i --left join here so we don't lose disabled nc indexes
				LEFT JOIN #IndexSanitySize AS sz ON i.index_sanity_id = sz.index_sanity_id
		UNION ALL
		SELECT 	N'Database ' + QUOTENAME(@DatabaseName) + N' as of ' + convert(nvarchar(16),getdate(),121)			
				N'sp_BlitzIndex(TM) v2.02 - Jan 30, 2014' ,   
				N'From Brent Ozar Unlimited(TM)' ,   
				N'http://BrentOzar.com/BlitzIndex' ,
				N'Thanks from the Brent Ozar Unlimited(TM) team.  We hope you found this tool useful, and if you need help relieving your SQL Server pains, email us at Help@BrentOzar.com.',
				NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,
				NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,
				NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,
				NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,
				NULL,NULL,NULL, NULL,NULL, NULL, NULL, NULL, NULL,NULL,NULL,
				0 as [Display Order]
		ORDER BY [Display Order] ASC, [Reserved MB] DESC
		OPTION (RECOMPILE);

	END /* End @Mode=2 (index detail)*/
	ELSE IF @Mode=3 /*Missing index Detail*/
	BEGIN
		SELECT 
			database_name AS [Database], 
			[schema_name] AS [Schema], 
			table_name AS [Table], 
			CAST(magic_benefit_number AS BIGINT)
				AS [Magic Benefit Number], 
			missing_index_details AS [Missing Index Details], 
			avg_total_user_cost AS [Avg Query Cost], 
			avg_user_impact AS [Est Index Improvement], 
			user_seeks AS [Seeks], 
			user_scans AS [Scans],
			unique_compiles AS [Compiles], 
			equality_columns AS [Equality Columns], 
			inequality_columns AS [Inequality Columns], 
			included_columns AS [Included Columns], 
			index_estimated_impact AS [Estimated Impact], 
			create_tsql AS [Create TSQL], 
			more_info AS [More Info],
			1 as [Display Order]
		FROM #MissingIndexes
		UNION ALL
		SELECT 				
			N'sp_BlitzIndex(TM) v2.02 - Jan 30, 2014' ,   
			N'From Brent Ozar Unlimited(TM)' ,   
			N'http://BrentOzar.com/BlitzIndex' ,
			100000000000,
			N'Thanks from the Brent Ozar Unlimited(TM) team. We hope you found this tool useful, and if you need help relieving your SQL Server pains, email us at Help@BrentOzar.com.',
			NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,
			NULL, 0 as display_order
		ORDER BY [Display Order] ASC, [Magic Benefit Number] DESC

	END /* End @Mode=3 (index detail)*/
END
END TRY
BEGIN CATCH
		RAISERROR (N'Failure analyzing temp tables.', 0,1) WITH NOWAIT;

		SELECT	@msg = ERROR_MESSAGE(), @ErrorSeverity = ERROR_SEVERITY(), @ErrorState = ERROR_STATE();

		RAISERROR (@msg, 
               @ErrorSeverity, 
               @ErrorState 
               );
		
		WHILE @@trancount &gt; 0 
			ROLLBACK;

		RETURN;
	END CATCH;
GO

</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>TroubleShooting</Category>
        <Language>SQLSERVER2K SQL</Language>
        <Public>false</Public>
        <Name>sp_BlitzFirst_____script</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>TroubleShooting</Category>
          <Language>SQLSERVER2K SQL</Language>
          <Public>false</Public>
          <Name>sp_BlitzFirst_____script</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>--sp_BlitzFirst_____script

--https://www.brentozar.com/askbrent/

IF OBJECT_ID('dbo.sp_BlitzFirst') IS NULL
  EXEC ('CREATE PROCEDURE dbo.sp_BlitzFirst AS RETURN 0;')
GO


ALTER PROCEDURE [dbo].[sp_BlitzFirst]
    @Question NVARCHAR(MAX) = NULL ,
    @Help TINYINT = 0 ,
    @AsOf DATETIMEOFFSET = NULL ,
    @ExpertMode TINYINT = 0 ,
    @Seconds INT = 5 ,
    @OutputType VARCHAR(20) = 'TABLE' ,
    @OutputServerName NVARCHAR(256) = NULL ,
    @OutputDatabaseName NVARCHAR(256) = NULL ,
    @OutputSchemaName NVARCHAR(256) = NULL ,
    @OutputTableName NVARCHAR(256) = NULL ,
    @OutputTableNameFileStats NVARCHAR(256) = NULL ,
    @OutputTableNamePerfmonStats NVARCHAR(256) = NULL ,
    @OutputTableNameWaitStats NVARCHAR(256) = NULL ,
    @OutputXMLasNVARCHAR TINYINT = 0 ,
    @FilterPlansByDatabase VARCHAR(MAX) = NULL ,
    @CheckProcedureCache TINYINT = 0 ,
    @FileLatencyThresholdMS INT = 100 ,
    @SinceStartup TINYINT = 0 ,
    @VersionDate DATETIME = NULL OUTPUT
    WITH EXECUTE AS CALLER, RECOMPILE
AS
BEGIN
SET NOCOUNT ON;
SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;
SET @VersionDate = '20161210'

IF @Help = 1 PRINT '
sp_BlitzFirst from http://FirstResponderKit.org
	
This script gives you a prioritized list of why your SQL Server is slow right now.

This is not an overall health check - for that, check out sp_Blitz.

To learn more, visit http://FirstResponderKit.org where you can download new
versions for free, watch training videos on how it works, get more info on
the findings, contribute your own code, and more.

Known limitations of this version:
 - Only Microsoft-supported versions of SQL Server. Sorry, 2005 and 2000. It
   may work just fine on 2005, and if it does, hug your parents. Just don''t
   file support issues if it breaks.
 - If a temp table called #CustomPerfmonCounters exists for any other session,
   but not our session, this stored proc will fail with an error saying the
   temp table #CustomPerfmonCounters does not exist.
 - @OutputServerName is not functional yet.

Unknown limitations of this version:
 - None. Like Zombo.com, the only limit is yourself.

Changes - for the full list of improvements and fixes in this version, see:
https://github.com/BrentOzarULTD/SQL-Server-First-Responder-Kit/


MIT License

Copyright (c) 2016 Brent Ozar Unlimited

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

'


RAISERROR('Setting up configuration variables',10,1) WITH NOWAIT;
DECLARE @StringToExecute NVARCHAR(MAX),
    @ParmDefinitions NVARCHAR(4000),
    @Parm1 NVARCHAR(4000),
    @OurSessionID INT,
    @LineFeed NVARCHAR(10),
    @StockWarningHeader NVARCHAR(500),
    @StockWarningFooter NVARCHAR(100),
    @StockDetailsHeader NVARCHAR(100),
    @StockDetailsFooter NVARCHAR(100),
    @StartSampleTime DATETIMEOFFSET,
    @FinishSampleTime DATETIMEOFFSET,
	@FinishSampleTimeWaitFor DATETIME,
    @ServiceName sysname,
    @OutputTableNameFileStats_View NVARCHAR(256),
    @OutputTableNamePerfmonStats_View NVARCHAR(256),
    @OutputTableNameWaitStats_View NVARCHAR(256),
    @ObjectFullName NVARCHAR(2000);

/* Sanitize our inputs */
SELECT
    @OutputTableNameFileStats_View = QUOTENAME(@OutputTableNameFileStats + '_Deltas'),
    @OutputTableNamePerfmonStats_View = QUOTENAME(@OutputTableNamePerfmonStats + '_Deltas'),
    @OutputTableNameWaitStats_View = QUOTENAME(@OutputTableNameWaitStats + '_Deltas');

SELECT
    @OutputDatabaseName = QUOTENAME(@OutputDatabaseName),
    @OutputSchemaName = QUOTENAME(@OutputSchemaName),
    @OutputTableName = QUOTENAME(@OutputTableName),
    @OutputTableNameFileStats = QUOTENAME(@OutputTableNameFileStats),
    @OutputTableNamePerfmonStats = QUOTENAME(@OutputTableNamePerfmonStats),
    @OutputTableNameWaitStats = QUOTENAME(@OutputTableNameWaitStats),
    @LineFeed = CHAR(13) + CHAR(10),
    @StartSampleTime = SYSDATETIMEOFFSET(),
    @FinishSampleTime = DATEADD(ss, @Seconds, SYSDATETIMEOFFSET()),
	@FinishSampleTimeWaitFor = DATEADD(ss, @Seconds, GETDATE()),
    @OurSessionID = @@SPID;


IF @SinceStartup = 1
    SELECT @Seconds = 0, @ExpertMode = 1;

IF @Seconds = 0 AND CAST(SERVERPROPERTY('edition') AS VARCHAR(100)) = 'SQL Azure'
    SELECT @StartSampleTime = DATEADD(ms, AVG(-wait_time_ms), SYSDATETIMEOFFSET()), @FinishSampleTime = SYSDATETIMEOFFSET()
        FROM sys.dm_os_wait_stats w
        WHERE wait_type IN ('BROKER_TASK_STOP','DIRTY_PAGE_POLL','HADR_FILESTREAM_IOMGR_IOCOMPLETION','LAZYWRITER_SLEEP',
                            'LOGMGR_QUEUE','REQUEST_FOR_DEADLOCK_SEARCH','XE_DISPATCHER_WAIT','XE_TIMER_EVENT')
ELSE IF @Seconds = 0 AND CAST(SERVERPROPERTY('edition') AS VARCHAR(100)) &lt;&gt; 'SQL Azure'
    SELECT @StartSampleTime = create_date , @FinishSampleTime = SYSDATETIMEOFFSET()
        FROM sys.databases
        WHERE database_id = 2;
ELSE
    SELECT @StartSampleTime = SYSDATETIMEOFFSET(), @FinishSampleTime = DATEADD(ss, @Seconds, SYSDATETIMEOFFSET());

IF @OutputType = 'SCHEMA'
BEGIN
    SELECT FieldList = '[Priority] TINYINT, [FindingsGroup] VARCHAR(50), [Finding] VARCHAR(200), [URL] VARCHAR(200), [Details] NVARCHAR(4000), [HowToStopIt] NVARCHAR(MAX), [QueryPlan] XML, [QueryText] NVARCHAR(MAX)'

END
ELSE IF @AsOf IS NOT NULL AND @OutputDatabaseName IS NOT NULL AND @OutputSchemaName IS NOT NULL AND @OutputTableName IS NOT NULL
BEGIN
    /* They want to look into the past. */

        SET @StringToExecute = N' IF EXISTS(SELECT * FROM '
            + @OutputDatabaseName
            + '.INFORMATION_SCHEMA.SCHEMATA WHERE QUOTENAME(SCHEMA_NAME) = '''
            + @OutputSchemaName + ''') SELECT CheckDate, [Priority], [FindingsGroup], [Finding], [URL], CAST([Details] AS [XML]) AS Details,'
            + '[HowToStopIt], [CheckID], [StartTime], [LoginName], [NTUserName], [OriginalLoginName], [ProgramName], [HostName], [DatabaseID],'
            + '[DatabaseName], [OpenTransactionCount], [QueryPlan], [QueryText] FROM '
            + @OutputDatabaseName + '.'
            + @OutputSchemaName + '.'
            + @OutputTableName
            + ' WHERE CheckDate &gt;= DATEADD(mi, -15, ''' + CAST(@AsOf AS NVARCHAR(100)) + ''')'
            + ' AND CheckDate &lt;= DATEADD(mi, 15, ''' + CAST(@AsOf AS NVARCHAR(100)) + ''')'
            + ' /*ORDER BY CheckDate, Priority , FindingsGroup , Finding , Details*/;';
        EXEC(@StringToExecute);


END /* IF @AsOf IS NOT NULL AND @OutputDatabaseName IS NOT NULL AND @OutputSchemaName IS NOT NULL AND @OutputTableName IS NOT NULL */
ELSE IF @Question IS NULL /* IF @OutputType = 'SCHEMA' */
BEGIN
    /* What's running right now? This is the first and last result set. */
    IF @SinceStartup = 0 AND @Seconds &gt; 0 AND @ExpertMode = 1 
    BEGIN
		IF OBJECT_ID('dbo.sp_BlitzWho') IS NULL
		BEGIN
			PRINT N'sp_BlitzWho is not installed in the current database_files.  You can get a copy from http://FirstResponderKit.org'
		END
		ELSE
		BEGIN
			EXEC [dbo].[sp_BlitzWho]
		END
    END /* IF @SinceStartup = 0 AND @Seconds &gt; 0 AND @ExpertMode = 1   -   What's running right now? This is the first and last result set. */
     

    RAISERROR('Now starting diagnostic analysis',10,1) WITH NOWAIT;

    /*
    We start by creating #BlitzFirstResults. It's a temp table that will store
    the results from our checks. Throughout the rest of this stored procedure,
    we're running a series of checks looking for dangerous things inside the SQL
    Server. When we find a problem, we insert rows into #BlitzResults. At the
    end, we return these results to the end user.

    #BlitzFirstResults has a CheckID field, but there's no Check table. As we do
    checks, we insert data into this table, and we manually put in the CheckID.
    We (Brent Ozar Unlimited) maintain a list of the checks by ID#. You can
    download that from http://FirstResponderKit.org if you want to build
    a tool that relies on the output of sp_BlitzFirst.
    */

    IF OBJECT_ID('tempdb..#BlitzFirstResults') IS NOT NULL
        DROP TABLE #BlitzFirstResults;
    CREATE TABLE #BlitzFirstResults
        (
          ID INT IDENTITY(1, 1) PRIMARY KEY CLUSTERED,
          CheckID INT NOT NULL,
          Priority TINYINT NOT NULL,
          FindingsGroup VARCHAR(50) NOT NULL,
          Finding VARCHAR(200) NOT NULL,
          URL VARCHAR(200) NULL,
          Details NVARCHAR(4000) NULL,
          HowToStopIt NVARCHAR(MAX) NULL,
          QueryPlan [XML] NULL,
          QueryText NVARCHAR(MAX) NULL,
          StartTime DATETIMEOFFSET NULL,
          LoginName NVARCHAR(128) NULL,
          NTUserName NVARCHAR(128) NULL,
          OriginalLoginName NVARCHAR(128) NULL,
          ProgramName NVARCHAR(128) NULL,
          HostName NVARCHAR(128) NULL,
          DatabaseID INT NULL,
          DatabaseName NVARCHAR(128) NULL,
          OpenTransactionCount INT NULL,
          QueryStatsNowID INT NULL,
          QueryStatsFirstID INT NULL,
          PlanHandle VARBINARY(64) NULL,
          DetailsInt INT NULL,
        );

    IF OBJECT_ID('tempdb..#WaitStats') IS NOT NULL
        DROP TABLE #WaitStats;
    CREATE TABLE #WaitStats (Pass TINYINT NOT NULL, wait_type NVARCHAR(60), wait_time_ms BIGINT, signal_wait_time_ms BIGINT, waiting_tasks_count BIGINT, SampleTime DATETIMEOFFSET);

    IF OBJECT_ID('tempdb..#FileStats') IS NOT NULL
        DROP TABLE #FileStats;
    CREATE TABLE #FileStats (
        ID INT IDENTITY(1, 1) PRIMARY KEY CLUSTERED,
        Pass TINYINT NOT NULL,
        SampleTime DATETIMEOFFSET NOT NULL,
        DatabaseID INT NOT NULL,
        FileID INT NOT NULL,
        DatabaseName NVARCHAR(256) ,
        FileLogicalName NVARCHAR(256) ,
        TypeDesc NVARCHAR(60) ,
        SizeOnDiskMB BIGINT ,
        io_stall_read_ms BIGINT ,
        num_of_reads BIGINT ,
        bytes_read BIGINT ,
        io_stall_write_ms BIGINT ,
        num_of_writes BIGINT ,
        bytes_written BIGINT,
        PhysicalName NVARCHAR(520) ,
        avg_stall_read_ms INT ,
        avg_stall_write_ms INT
    );

    IF OBJECT_ID('tempdb..#QueryStats') IS NOT NULL
        DROP TABLE #QueryStats;
    CREATE TABLE #QueryStats (
        ID INT IDENTITY(1, 1) PRIMARY KEY CLUSTERED,
        Pass INT NOT NULL,
        SampleTime DATETIMEOFFSET NOT NULL,
        [sql_handle] VARBINARY(64),
        statement_start_offset INT,
        statement_end_offset INT,
        plan_generation_num BIGINT,
        plan_handle VARBINARY(64),
        execution_count BIGINT,
        total_worker_time BIGINT,
        total_physical_reads BIGINT,
        total_logical_writes BIGINT,
        total_logical_reads BIGINT,
        total_clr_time BIGINT,
        total_elapsed_time BIGINT,
        creation_time DATETIMEOFFSET,
        query_hash BINARY(8),
        query_plan_hash BINARY(8),
        Points TINYINT
    );

    IF OBJECT_ID('tempdb..#PerfmonStats') IS NOT NULL
        DROP TABLE #PerfmonStats;
    CREATE TABLE #PerfmonStats (
        ID INT IDENTITY(1, 1) PRIMARY KEY CLUSTERED,
        Pass TINYINT NOT NULL,
        SampleTime DATETIMEOFFSET NOT NULL,
        [object_name] NVARCHAR(128) NOT NULL,
        [counter_name] NVARCHAR(128) NOT NULL,
        [instance_name] NVARCHAR(128) NULL,
        [cntr_value] BIGINT NULL,
        [cntr_type] INT NOT NULL,
        [value_delta] BIGINT NULL,
        [value_per_second] DECIMAL(18,2) NULL
    );

    IF OBJECT_ID('tempdb..#PerfmonCounters') IS NOT NULL
        DROP TABLE #PerfmonCounters;
    CREATE TABLE #PerfmonCounters (
        ID INT IDENTITY(1, 1) PRIMARY KEY CLUSTERED,
        [object_name] NVARCHAR(128) NOT NULL,
        [counter_name] NVARCHAR(128) NOT NULL,
        [instance_name] NVARCHAR(128) NULL
    );

    IF OBJECT_ID('tempdb..#FilterPlansByDatabase') IS NOT NULL
        DROP TABLE #FilterPlansByDatabase;
    CREATE TABLE #FilterPlansByDatabase (DatabaseID INT PRIMARY KEY CLUSTERED);

    IF OBJECT_ID('tempdb..#MasterFiles') IS NOT NULL
        DROP TABLE #MasterFiles;
    CREATE TABLE #MasterFiles (database_id INT, file_id INT, type_desc NVARCHAR(50), name NVARCHAR(255), physical_name NVARCHAR(255), size BIGINT);
    /* Azure SQL Database doesn't have sys.master_files, so we have to build our own. */
    IF CAST(SERVERPROPERTY('edition') AS VARCHAR(100)) = 'SQL Azure'
        SET @StringToExecute = 'INSERT INTO #MasterFiles (database_id, file_id, type_desc, name, physical_name, size) SELECT DB_ID(), file_id, type_desc, name, physical_name, size FROM sys.database_files;'
    ELSE
        SET @StringToExecute = 'INSERT INTO #MasterFiles (database_id, file_id, type_desc, name, physical_name, size) SELECT database_id, file_id, type_desc, name, physical_name, size FROM sys.master_files;'
    EXEC(@StringToExecute);

    IF @FilterPlansByDatabase IS NOT NULL
        BEGIN
        IF UPPER(LEFT(@FilterPlansByDatabase,4)) = 'USER'
            BEGIN
            INSERT INTO #FilterPlansByDatabase (DatabaseID)
            SELECT database_id
                FROM sys.databases
                WHERE [name] NOT IN ('master', 'model', 'msdb', 'tempdb')
            END
        ELSE
            BEGIN
            SET @FilterPlansByDatabase = @FilterPlansByDatabase + ','
            ;WITH a AS
                (
                SELECT CAST(1 AS BIGINT) f, CHARINDEX(',', @FilterPlansByDatabase) t, 1 SEQ
                UNION ALL
                SELECT t + 1, CHARINDEX(',', @FilterPlansByDatabase, t + 1), SEQ + 1
                FROM a
                WHERE CHARINDEX(',', @FilterPlansByDatabase, t + 1) &gt; 0
                )
            INSERT #FilterPlansByDatabase (DatabaseID)
                SELECT SUBSTRING(@FilterPlansByDatabase, f, t - f)
                FROM a
                WHERE SUBSTRING(@FilterPlansByDatabase, f, t - f) IS NOT NULL
                OPTION (MAXRECURSION 0)
            END
        END


    SET @StockWarningHeader = '&lt;?ClickToSeeCommmand -- ' + @LineFeed + @LineFeed
        + 'WARNING: Running this command may result in data loss or an outage.' + @LineFeed
        + 'This tool is meant as a shortcut to help generate scripts for DBAs.' + @LineFeed
        + 'It is not a substitute for database training and experience.' + @LineFeed
        + 'Now, having said that, here''s the details:' + @LineFeed + @LineFeed;

    SELECT @StockWarningFooter = @LineFeed + @LineFeed + '-- ?&gt;',
        @StockDetailsHeader = '&lt;?ClickToSeeDetails -- ' + @LineFeed,
        @StockDetailsFooter = @LineFeed + ' -- ?&gt;';

    /* Get the instance name to use as a Perfmon counter prefix. */
    IF CAST(SERVERPROPERTY('edition') AS VARCHAR(100)) = 'SQL Azure'
        SELECT TOP 1 @ServiceName = LEFT(object_name, (CHARINDEX(':', object_name) - 1))
        FROM sys.dm_os_performance_counters;
    ELSE
        BEGIN
        SET @StringToExecute = 'INSERT INTO #PerfmonStats(object_name, Pass, SampleTime, counter_name, cntr_type) SELECT CASE WHEN @@SERVICENAME = ''MSSQLSERVER'' THEN ''SQLServer'' ELSE ''MSSQL$'' + @@SERVICENAME END, 0, SYSDATETIMEOFFSET(), ''stuffing'', 0 ;'
        EXEC(@StringToExecute);
        SELECT @ServiceName = object_name FROM #PerfmonStats;
        DELETE #PerfmonStats;
        END

    /* Build a list of queries that were run in the last 10 seconds.
       We're looking for the death-by-a-thousand-small-cuts scenario
       where a query is constantly running, and it doesn't have that
       big of an impact individually, but it has a ton of impact
       overall. We're going to build this list, and then after we
       finish our @Seconds sample, we'll compare our plan cache to
       this list to see what ran the most. */

    /* Populate #QueryStats. SQL 2005 doesn't have query hash or query plan hash. */
    IF @CheckProcedureCache = 1 
	BEGIN
		RAISERROR('@CheckProcedureCache = 1, capturing first pass of plan cache',10,1) WITH NOWAIT;
		IF @@VERSION LIKE 'Microsoft SQL Server 2005%'
			BEGIN
			IF @FilterPlansByDatabase IS NULL
				BEGIN
				SET @StringToExecute = N'INSERT INTO #QueryStats ([sql_handle], Pass, SampleTime, statement_start_offset, statement_end_offset, plan_generation_num, plan_handle, execution_count, total_worker_time, total_physical_reads, total_logical_writes, total_logical_reads, total_clr_time, total_elapsed_time, creation_time, query_hash, query_plan_hash, Points)
											SELECT [sql_handle], 1 AS Pass, SYSDATETIMEOFFSET(), statement_start_offset, statement_end_offset, plan_generation_num, plan_handle, execution_count, total_worker_time, total_physical_reads, total_logical_writes, total_logical_reads, total_clr_time, total_elapsed_time, creation_time, NULL AS query_hash, NULL AS query_plan_hash, 0
											FROM sys.dm_exec_query_stats qs
											WHERE qs.last_execution_time &gt;= (DATEADD(ss, -10, SYSDATETIMEOFFSET()));';
				END
			ELSE
				BEGIN
				SET @StringToExecute = N'INSERT INTO #QueryStats ([sql_handle], Pass, SampleTime, statement_start_offset, statement_end_offset, plan_generation_num, plan_handle, execution_count, total_worker_time, total_physical_reads, total_logical_writes, total_logical_reads, total_clr_time, total_elapsed_time, creation_time, query_hash, query_plan_hash, Points)
											SELECT [sql_handle], 1 AS Pass, SYSDATETIMEOFFSET(), statement_start_offset, statement_end_offset, plan_generation_num, plan_handle, execution_count, total_worker_time, total_physical_reads, total_logical_writes, total_logical_reads, total_clr_time, total_elapsed_time, creation_time, NULL AS query_hash, NULL AS query_plan_hash, 0
											FROM sys.dm_exec_query_stats qs
												CROSS APPLY sys.dm_exec_plan_attributes(qs.plan_handle) AS attr
												INNER JOIN #FilterPlansByDatabase dbs ON CAST(attr.value AS INT) = dbs.DatabaseID
											WHERE qs.last_execution_time &gt;= (DATEADD(ss, -10, SYSDATETIMEOFFSET()))
												AND attr.attribute = ''dbid'';';
				END
			END
		ELSE
			BEGIN
			IF @FilterPlansByDatabase IS NULL
				BEGIN
				SET @StringToExecute = N'INSERT INTO #QueryStats ([sql_handle], Pass, SampleTime, statement_start_offset, statement_end_offset, plan_generation_num, plan_handle, execution_count, total_worker_time, total_physical_reads, total_logical_writes, total_logical_reads, total_clr_time, total_elapsed_time, creation_time, query_hash, query_plan_hash, Points)
											SELECT [sql_handle], 1 AS Pass, SYSDATETIMEOFFSET(), statement_start_offset, statement_end_offset, plan_generation_num, plan_handle, execution_count, total_worker_time, total_physical_reads, total_logical_writes, total_logical_reads, total_clr_time, total_elapsed_time, creation_time, query_hash, query_plan_hash, 0
											FROM sys.dm_exec_query_stats qs
											WHERE qs.last_execution_time &gt;= (DATEADD(ss, -10, SYSDATETIMEOFFSET()));';
				END
			ELSE
				BEGIN
				SET @StringToExecute = N'INSERT INTO #QueryStats ([sql_handle], Pass, SampleTime, statement_start_offset, statement_end_offset, plan_generation_num, plan_handle, execution_count, total_worker_time, total_physical_reads, total_logical_writes, total_logical_reads, total_clr_time, total_elapsed_time, creation_time, query_hash, query_plan_hash, Points)
											SELECT [sql_handle], 1 AS Pass, SYSDATETIMEOFFSET(), statement_start_offset, statement_end_offset, plan_generation_num, plan_handle, execution_count, total_worker_time, total_physical_reads, total_logical_writes, total_logical_reads, total_clr_time, total_elapsed_time, creation_time, query_hash, query_plan_hash, 0
											FROM sys.dm_exec_query_stats qs
											CROSS APPLY sys.dm_exec_plan_attributes(qs.plan_handle) AS attr
											INNER JOIN #FilterPlansByDatabase dbs ON CAST(attr.value AS INT) = dbs.DatabaseID
											WHERE qs.last_execution_time &gt;= (DATEADD(ss, -10, SYSDATETIMEOFFSET()))
												AND attr.attribute = ''dbid'';';
				END
			END
		EXEC(@StringToExecute);

		/* Get the totals for the entire plan cache */
		INSERT INTO #QueryStats (Pass, SampleTime, execution_count, total_worker_time, total_physical_reads, total_logical_writes, total_logical_reads, total_clr_time, total_elapsed_time, creation_time)
		SELECT -1 AS Pass, SYSDATETIMEOFFSET(), SUM(execution_count), SUM(total_worker_time), SUM(total_physical_reads), SUM(total_logical_writes), SUM(total_logical_reads), SUM(total_clr_time), SUM(total_elapsed_time), MIN(creation_time)
			FROM sys.dm_exec_query_stats qs;
    END /*IF @CheckProcedureCache = 1 */


    IF EXISTS (SELECT *
                    FROM tempdb.sys.all_objects obj
                    INNER JOIN tempdb.sys.all_columns col1 ON obj.object_id = col1.object_id AND col1.name = 'object_name'
                    INNER JOIN tempdb.sys.all_columns col2 ON obj.object_id = col2.object_id AND col2.name = 'counter_name'
                    INNER JOIN tempdb.sys.all_columns col3 ON obj.object_id = col3.object_id AND col3.name = 'instance_name'
                    WHERE obj.name LIKE '%CustomPerfmonCounters%')
        BEGIN
        SET @StringToExecute = 'INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) SELECT [object_name],[counter_name],[instance_name] FROM #CustomPerfmonCounters'
        EXEC(@StringToExecute);
        END
    ELSE
        BEGIN
        /* Add our default Perfmon counters */
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Access Methods','Forwarded Records/sec', NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Access Methods','Page compression attempts/sec', NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Access Methods','Page Splits/sec', NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Access Methods','Skipped Ghosted Records/sec', NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Access Methods','Table Lock Escalations/sec', NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Access Methods','Worktables Created/sec', NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Buffer Manager','Page life expectancy', NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Buffer Manager','Page reads/sec', NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Buffer Manager','Page writes/sec', NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Buffer Manager','Readahead pages/sec', NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Buffer Manager','Target pages', NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Buffer Manager','Total pages', NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Databases','', NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Buffer Manager','Active Transactions','_Total')
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Databases','Log Growths', '_Total')
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Databases','Log Shrinks', '_Total')
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Exec Statistics','Distributed Query', 'Execs in progress')
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Exec Statistics','DTC calls', 'Execs in progress')
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Exec Statistics','Extended Procedures', 'Execs in progress')
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Exec Statistics','OLEDB calls', 'Execs in progress')
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':General Statistics','Active Temp Tables', NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':General Statistics','Logins/sec', NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':General Statistics','Logouts/sec', NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':General Statistics','Mars Deadlocks', NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':General Statistics','Processes blocked', NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Locks','Number of Deadlocks/sec', NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Memory Manager','Memory Grants Pending', NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':SQL Errors','Errors/sec', '_Total')
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':SQL Statistics','Batch Requests/sec', NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':SQL Statistics','Forced Parameterizations/sec', NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':SQL Statistics','Guided plan executions/sec', NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':SQL Statistics','SQL Attention rate', NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':SQL Statistics','SQL Compilations/sec', NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':SQL Statistics','SQL Re-Compilations/sec', NULL)
        /* Below counters added by Jefferson Elias */
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Access Methods','Worktables From Cache Base',NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Access Methods','Worktables From Cache Ratio',NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Buffer Manager','Database pages',NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Buffer Manager','Free pages',NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Buffer Manager','Stolen pages',NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Memory Manager','Granted Workspace Memory (KB)',NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Memory Manager','Maximum Workspace Memory (KB)',NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Memory Manager','Target Server Memory (KB)',NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Memory Manager','Total Server Memory (KB)',NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Buffer Manager','Buffer cache hit ratio',NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Buffer Manager','Buffer cache hit ratio base',NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Buffer Manager','Checkpoint pages/sec',NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Buffer Manager','Free list stalls/sec',NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Buffer Manager','Lazy writes/sec',NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':SQL Statistics','Auto-Param Attempts/sec',NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':SQL Statistics','Failed Auto-Params/sec',NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':SQL Statistics','Safe Auto-Params/sec',NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':SQL Statistics','Unsafe Auto-Params/sec',NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Access Methods','Workfiles Created/sec',NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':General Statistics','User Connections',NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Latches','Average Latch Wait Time (ms)',NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Latches','Average Latch Wait Time Base',NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Latches','Latch Waits/sec',NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Latches','Total Latch Wait Time (ms)',NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Locks','Average Wait Time (ms)',NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Locks','Average Wait Time Base',NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Locks','Lock Requests/sec',NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Locks','Lock Timeouts/sec',NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Locks','Lock Wait Time (ms)',NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Locks','Lock Waits/sec',NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Transactions','Longest Transaction Running Time',NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Access Methods','Full Scans/sec',NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Access Methods','Index Searches/sec',NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Buffer Manager','Page lookups/sec',NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':Cursor Manager by Type','Active cursors',NULL)
        END

    /* Populate #FileStats, #PerfmonStats, #WaitStats with DMV data.
        After we finish doing our checks, we'll take another sample and compare them. */
	RAISERROR('Capturing first pass of wait stats, perfmon counters, file stats',10,1) WITH NOWAIT;
    INSERT #WaitStats(Pass, SampleTime, wait_type, wait_time_ms, signal_wait_time_ms, waiting_tasks_count)
		SELECT 
		x.Pass, 
		x.SampleTime, 
		x.wait_type, 
		SUM(x.sum_wait_time_ms) AS sum_wait_time_ms, 
		SUM(x.sum_signal_wait_time_ms) AS sum_signal_wait_time_ms, 
		SUM(x.sum_waiting_tasks) AS sum_waiting_tasks
		FROM (
		SELECT  
				1 AS Pass,
				CASE @Seconds WHEN 0 THEN @StartSampleTime ELSE SYSDATETIMEOFFSET() END AS SampleTime,
				owt.wait_type,
		        CASE @Seconds WHEN 0 THEN 0 ELSE SUM(owt.wait_duration_ms) OVER (PARTITION BY owt.wait_type, owt.session_id)
					 - CASE WHEN @Seconds = 0 THEN 0 ELSE (@Seconds * 1000) END END AS sum_wait_time_ms,
				0 AS sum_signal_wait_time_ms,
				0 AS sum_waiting_tasks
			FROM    sys.dm_os_waiting_tasks owt
			WHERE owt.session_id &gt; 50
			AND owt.wait_duration_ms &gt;= CASE @Seconds WHEN 0 THEN 0 ELSE @Seconds * 1000 END
		UNION ALL
		SELECT
		       1 AS Pass,
		       CASE @Seconds WHEN 0 THEN @StartSampleTime ELSE SYSDATETIMEOFFSET() END AS SampleTime,
		       os.wait_type,
		       CASE @Seconds WHEN 0 THEN 0 ELSE SUM(os.wait_time_ms) OVER (PARTITION BY os.wait_type) END AS sum_wait_time_ms,
		       CASE @Seconds WHEN 0 THEN 0 ELSE SUM(os.signal_wait_time_ms) OVER (PARTITION BY os.wait_type ) END AS sum_signal_wait_time_ms,
		       CASE @Seconds WHEN 0 THEN 0 ELSE SUM(os.waiting_tasks_count) OVER (PARTITION BY os.wait_type) END AS sum_waiting_tasks
		   FROM sys.dm_os_wait_stats os
		) x
		   WHERE x.wait_type NOT IN (
		       'REQUEST_FOR_DEADLOCK_SEARCH',
		       'SQLTRACE_INCREMENTAL_FLUSH_SLEEP',
		       'SQLTRACE_BUFFER_FLUSH',
		       'LAZYWRITER_SLEEP',
		       'XE_TIMER_EVENT',
		       'XE_DISPATCHER_WAIT',
		       'FT_IFTS_SCHEDULER_IDLE_WAIT',
		       'LOGMGR_QUEUE',
		       'CHECKPOINT_QUEUE',
		       'BROKER_TO_FLUSH',
		       'BROKER_TASK_STOP',
		       'BROKER_EVENTHANDLER',
		       'SLEEP_TASK',
		       'WAITFOR',
		       'DBMIRROR_DBM_MUTEX',
		       'DBMIRROR_EVENTS_QUEUE',
		       'DBMIRRORING_CMD',
		       'DISPATCHER_QUEUE_SEMAPHORE',
		       'BROKER_RECEIVE_WAITFOR',
		       'CLR_AUTO_EVENT',
		       'DIRTY_PAGE_POLL',
		       'HADR_FILESTREAM_IOMGR_IOCOMPLETION',
		       'ONDEMAND_TASK_QUEUE',
		       'FT_IFTSHC_MUTEX',
		       'CLR_MANUAL_EVENT',
		       'CLR_SEMAPHORE',
		       'DBMIRROR_WORKER_QUEUE',
		       'DBMIRROR_DBM_EVENT',
		       'SP_SERVER_DIAGNOSTICS_SLEEP',
		       'HADR_CLUSAPI_CALL',
		       'HADR_LOGCAPTURE_WAIT',
		       'HADR_NOTIFICATION_DEQUEUE',
		       'HADR_TIMER_TASK',
		       'HADR_WORK_QUEUE',
		       'QDS_PERSIST_TASK_MAIN_LOOP_SLEEP',
		       'QDS_CLEANUP_STALE_QUERIES_TASK_MAIN_LOOP_SLEEP',
		       'RESOURCE_GOVERNOR_IDLE',
		       'QDS_ASYNC_QUEUE',
		       'QDS_SHUTDOWN_QUEUE',
		       'SLEEP_SYSTEMTASK',
		       'BROKER_TRANSMITTER',
		       'REDO_THREAD_PENDING_WORK',
		       'UCS_SESSION_REGISTRATION'
		   )
		GROUP BY x.Pass, x.SampleTime, x.wait_type
		ORDER BY sum_wait_time_ms DESC;


    INSERT INTO #FileStats (Pass, SampleTime, DatabaseID, FileID, DatabaseName, FileLogicalName, SizeOnDiskMB, io_stall_read_ms ,
        num_of_reads, [bytes_read] , io_stall_write_ms,num_of_writes, [bytes_written], PhysicalName, TypeDesc)
    SELECT
        1 AS Pass,
        CASE @Seconds WHEN 0 THEN @StartSampleTime ELSE SYSDATETIMEOFFSET() END AS SampleTime,
        mf.[database_id],
        mf.[file_id],
        DB_NAME(vfs.database_id) AS [db_name],
        mf.name + N' [' + mf.type_desc COLLATE SQL_Latin1_General_CP1_CI_AS + N']' AS file_logical_name ,
        CAST(( ( vfs.size_on_disk_bytes / 1024.0 ) / 1024.0 ) AS INT) AS size_on_disk_mb ,
        CASE @Seconds WHEN 0 THEN 0 ELSE vfs.io_stall_read_ms END ,
        CASE @Seconds WHEN 0 THEN 0 ELSE vfs.num_of_reads END ,
        CASE @Seconds WHEN 0 THEN 0 ELSE vfs.[num_of_bytes_read] END ,
        CASE @Seconds WHEN 0 THEN 0 ELSE vfs.io_stall_write_ms END ,
        CASE @Seconds WHEN 0 THEN 0 ELSE vfs.num_of_writes END ,
        CASE @Seconds WHEN 0 THEN 0 ELSE vfs.[num_of_bytes_written] END ,
        mf.physical_name,
        mf.type_desc
    FROM sys.dm_io_virtual_file_stats (NULL, NULL) AS vfs
    INNER JOIN #MasterFiles AS mf ON vfs.file_id = mf.file_id
        AND vfs.database_id = mf.database_id
    WHERE vfs.num_of_reads &gt; 0
        OR vfs.num_of_writes &gt; 0;

    INSERT INTO #PerfmonStats (Pass, SampleTime, [object_name],[counter_name],[instance_name],[cntr_value],[cntr_type])
    SELECT         1 AS Pass,
        CASE @Seconds WHEN 0 THEN @StartSampleTime ELSE SYSDATETIMEOFFSET() END AS SampleTime, RTRIM(dmv.object_name), RTRIM(dmv.counter_name), RTRIM(dmv.instance_name), CASE @Seconds WHEN 0 THEN 0 ELSE dmv.cntr_value END, dmv.cntr_type
        FROM #PerfmonCounters counters
        INNER JOIN sys.dm_os_performance_counters dmv ON counters.counter_name COLLATE SQL_Latin1_General_CP1_CI_AS = RTRIM(dmv.counter_name) COLLATE SQL_Latin1_General_CP1_CI_AS
            AND counters.[object_name] COLLATE SQL_Latin1_General_CP1_CI_AS = RTRIM(dmv.[object_name]) COLLATE SQL_Latin1_General_CP1_CI_AS
            AND (counters.[instance_name] IS NULL OR counters.[instance_name] COLLATE SQL_Latin1_General_CP1_CI_AS = RTRIM(dmv.[instance_name]) COLLATE SQL_Latin1_General_CP1_CI_AS)


	RAISERROR('Beginning investigatory queries',10,1) WITH NOWAIT;


    /* Maintenance Tasks Running - Backup Running - CheckID 1 */
    IF @Seconds &gt; 0
    INSERT INTO #BlitzFirstResults (CheckID, Priority, FindingsGroup, Finding, URL, Details, HowToStopIt, QueryPlan, StartTime, LoginName, NTUserName, ProgramName, HostName, DatabaseID, DatabaseName, OpenTransactionCount)
    SELECT 1 AS CheckID,
        1 AS Priority,
        'Maintenance Tasks Running' AS FindingGroup,
        'Backup Running' AS Finding,
        'http://www.BrentOzar.com/askbrent/backups/' AS URL,
        'Backup of ' + DB_NAME(db.resource_database_id) + ' database (' + (SELECT CAST(CAST(SUM(size * 8.0 / 1024 / 1024) AS BIGINT) AS NVARCHAR) FROM #MasterFiles WHERE database_id = db.resource_database_id) + 'GB) is ' + CAST(r.percent_complete AS NVARCHAR(100)) + '% complete, has been running since ' + CAST(r.start_time AS NVARCHAR(100)) + '. ' AS Details,
        'KILL ' + CAST(r.session_id AS NVARCHAR(100)) + ';' AS HowToStopIt,
        pl.query_plan AS QueryPlan,
        r.start_time AS StartTime,
        s.login_name AS LoginName,
        s.nt_user_name AS NTUserName,
        s.[program_name] AS ProgramName,
        s.[host_name] AS HostName,
        db.[resource_database_id] AS DatabaseID,
        DB_NAME(db.resource_database_id) AS DatabaseName,
        0 AS OpenTransactionCount
    FROM sys.dm_exec_requests r
    INNER JOIN sys.dm_exec_connections c ON r.session_id = c.session_id
    INNER JOIN sys.dm_exec_sessions s ON r.session_id = s.session_id
    INNER JOIN (
    SELECT DISTINCT request_session_id, resource_database_id
    FROM    sys.dm_tran_locks
    WHERE resource_type = N'DATABASE'
    AND     request_mode = N'S'
    AND     request_status = N'GRANT'
    AND     request_owner_type = N'SHARED_TRANSACTION_WORKSPACE') AS db ON s.session_id = db.request_session_id
    CROSS APPLY sys.dm_exec_query_plan(r.plan_handle) pl
    WHERE r.command LIKE 'BACKUP%';


    /* If there's a backup running, add details explaining how long full backup has been taking in the last month. */
    IF @Seconds &gt; 0 AND CAST(SERVERPROPERTY('edition') AS VARCHAR(100)) &lt;&gt; 'SQL Azure'
    BEGIN
        SET @StringToExecute = 'UPDATE #BlitzFirstResults SET Details = Details + '' Over the last 60 days, the full backup usually takes '' + CAST((SELECT AVG(DATEDIFF(mi, bs.backup_start_date, bs.backup_finish_date)) FROM msdb.dbo.backupset bs WHERE abr.DatabaseName = bs.database_name AND bs.type = ''D'' AND bs.backup_start_date &gt; DATEADD(dd, -60, SYSDATETIMEOFFSET()) AND bs.backup_finish_date IS NOT NULL) AS NVARCHAR(100)) + '' minutes.'' FROM #BlitzFirstResults abr WHERE abr.CheckID = 1 AND EXISTS (SELECT * FROM msdb.dbo.backupset bs WHERE bs.type = ''D'' AND bs.backup_start_date &gt; DATEADD(dd, -60, SYSDATETIMEOFFSET()) AND bs.backup_finish_date IS NOT NULL AND abr.DatabaseName = bs.database_name AND DATEDIFF(mi, bs.backup_start_date, bs.backup_finish_date) &gt; 1)';
        EXEC(@StringToExecute);
    END


    /* Maintenance Tasks Running - DBCC Running - CheckID 2 */
    IF @Seconds &gt; 0
    INSERT INTO #BlitzFirstResults (CheckID, Priority, FindingsGroup, Finding, URL, Details, HowToStopIt, QueryPlan, StartTime, LoginName, NTUserName, ProgramName, HostName, DatabaseID, DatabaseName, OpenTransactionCount)
    SELECT 2 AS CheckID,
        1 AS Priority,
        'Maintenance Tasks Running' AS FindingGroup,
        'DBCC Running' AS Finding,
        'http://www.BrentOzar.com/askbrent/dbcc/' AS URL,
        'Corruption check of ' + DB_NAME(db.resource_database_id) + ' database (' + (SELECT CAST(CAST(SUM(size * 8.0 / 1024 / 1024) AS BIGINT) AS NVARCHAR) FROM #MasterFiles WHERE database_id = db.resource_database_id) + 'GB) has been running since ' + CAST(r.start_time AS NVARCHAR(100)) + '. ' AS Details,
        'KILL ' + CAST(r.session_id AS NVARCHAR(100)) + ';' AS HowToStopIt,
        pl.query_plan AS QueryPlan,
        r.start_time AS StartTime,
        s.login_name AS LoginName,
        s.nt_user_name AS NTUserName,
        s.[program_name] AS ProgramName,
        s.[host_name] AS HostName,
        db.[resource_database_id] AS DatabaseID,
        DB_NAME(db.resource_database_id) AS DatabaseName,
        0 AS OpenTransactionCount
    FROM sys.dm_exec_requests r
    INNER JOIN sys.dm_exec_connections c ON r.session_id = c.session_id
    INNER JOIN sys.dm_exec_sessions s ON r.session_id = s.session_id
    INNER JOIN (SELECT DISTINCT l.request_session_id, l.resource_database_id
    FROM    sys.dm_tran_locks l
    INNER JOIN sys.databases d ON l.resource_database_id = d.database_id
    WHERE l.resource_type = N'DATABASE'
    AND     l.request_mode = N'S'
    AND    l.request_status = N'GRANT'
    AND    l.request_owner_type = N'SHARED_TRANSACTION_WORKSPACE') AS db ON s.session_id = db.request_session_id
    CROSS APPLY sys.dm_exec_query_plan(r.plan_handle) pl
    WHERE r.command LIKE 'DBCC%';


    /* Maintenance Tasks Running - Restore Running - CheckID 3 */
    IF @Seconds &gt; 0
    INSERT INTO #BlitzFirstResults (CheckID, Priority, FindingsGroup, Finding, URL, Details, HowToStopIt, QueryPlan, StartTime, LoginName, NTUserName, ProgramName, HostName, DatabaseID, DatabaseName, OpenTransactionCount)
    SELECT 3 AS CheckID,
        1 AS Priority,
        'Maintenance Tasks Running' AS FindingGroup,
        'Restore Running' AS Finding,
        'http://www.BrentOzar.com/askbrent/backups/' AS URL,
        'Restore of ' + DB_NAME(db.resource_database_id) + ' database (' + (SELECT CAST(CAST(SUM(size * 8.0 / 1024 / 1024) AS BIGINT) AS NVARCHAR) FROM #MasterFiles WHERE database_id = db.resource_database_id) + 'GB) is ' + CAST(r.percent_complete AS NVARCHAR(100)) + '% complete, has been running since ' + CAST(r.start_time AS NVARCHAR(100)) + '. ' AS Details,
        'KILL ' + CAST(r.session_id AS NVARCHAR(100)) + ';' AS HowToStopIt,
        pl.query_plan AS QueryPlan,
        r.start_time AS StartTime,
        s.login_name AS LoginName,
        s.nt_user_name AS NTUserName,
        s.[program_name] AS ProgramName,
        s.[host_name] AS HostName,
        db.[resource_database_id] AS DatabaseID,
        DB_NAME(db.resource_database_id) AS DatabaseName,
        0 AS OpenTransactionCount
    FROM sys.dm_exec_requests r
    INNER JOIN sys.dm_exec_connections c ON r.session_id = c.session_id
    INNER JOIN sys.dm_exec_sessions s ON r.session_id = s.session_id
    INNER JOIN (
    SELECT DISTINCT request_session_id, resource_database_id
    FROM    sys.dm_tran_locks
    WHERE resource_type = N'DATABASE'
    AND     request_mode = N'S'
    AND     request_status = N'GRANT'
    AND     request_owner_type = N'SHARED_TRANSACTION_WORKSPACE') AS db ON s.session_id = db.request_session_id
    CROSS APPLY sys.dm_exec_query_plan(r.plan_handle) pl
    WHERE r.command LIKE 'RESTORE%';


    /* SQL Server Internal Maintenance - Database File Growing - CheckID 4 */
    IF @Seconds &gt; 0
    INSERT INTO #BlitzFirstResults (CheckID, Priority, FindingsGroup, Finding, URL, Details, HowToStopIt, QueryPlan, StartTime, LoginName, NTUserName, ProgramName, HostName, DatabaseID, DatabaseName, OpenTransactionCount)
    SELECT 4 AS CheckID,
        1 AS Priority,
        'SQL Server Internal Maintenance' AS FindingGroup,
        'Database File Growing' AS Finding,
        'http://www.BrentOzar.com/go/instant' AS URL,
        'SQL Server is waiting for Windows to provide storage space for a database restore, a data file growth, or a log file growth. This task has been running since ' + CAST(r.start_time AS NVARCHAR(100)) + '.' + @LineFeed + 'Check the query plan (expert mode) to identify the database involved.' AS Details,
        'Unfortunately, you can''t stop this, but you can prevent it next time. Check out http://www.BrentOzar.com/go/instant for details.' AS HowToStopIt,
        pl.query_plan AS QueryPlan,
        r.start_time AS StartTime,
        s.login_name AS LoginName,
        s.nt_user_name AS NTUserName,
        s.[program_name] AS ProgramName,
        s.[host_name] AS HostName,
        NULL AS DatabaseID,
        NULL AS DatabaseName,
        0 AS OpenTransactionCount
    FROM sys.dm_os_waiting_tasks t
    INNER JOIN sys.dm_exec_connections c ON t.session_id = c.session_id
    INNER JOIN sys.dm_exec_requests r ON t.session_id = r.session_id
    INNER JOIN sys.dm_exec_sessions s ON r.session_id = s.session_id
    CROSS APPLY sys.dm_exec_query_plan(r.plan_handle) pl
    WHERE t.wait_type = 'PREEMPTIVE_OS_WRITEFILEGATHER'


    /* Query Problems - Long-Running Query Blocking Others - CheckID 5 */
    /*
    IF @Seconds &gt; 0
    INSERT INTO #BlitzFirstResults (CheckID, Priority, FindingsGroup, Finding, URL, Details, HowToStopIt, QueryPlan, QueryText, StartTime, LoginName, NTUserName, ProgramName, HostName, DatabaseID, DatabaseName, OpenTransactionCount)
    SELECT 5 AS CheckID,
        1 AS Priority,
        'Query Problems' AS FindingGroup,
        'Long-Running Query Blocking Others' AS Finding,
        'http://www.BrentOzar.com/go/blocking' AS URL,
        'Query in ' + DB_NAME(db.resource_database_id) + ' has been running since ' + CAST(r.start_time AS NVARCHAR(100)) + '. ' + @LineFeed + @LineFeed
            + CAST(COALESCE((SELECT TOP 1 [text] FROM sys.dm_exec_sql_text(rBlocker.sql_handle)),
            (SELECT TOP 1 [text] FROM master..sysprocesses spBlocker CROSS APPLY sys.dm_exec_sql_text(spBlocker.sql_handle) WHERE spBlocker.spid = tBlocked.blocking_session_id), '') AS NVARCHAR(2000)) AS Details,
        'KILL ' + CAST(tBlocked.blocking_session_id AS NVARCHAR(100)) + ';' AS HowToStopIt,
        (SELECT TOP 1 query_plan FROM sys.dm_exec_query_plan(rBlocker.plan_handle)) AS QueryPlan,
        COALESCE((SELECT TOP 1 [text] FROM sys.dm_exec_sql_text(rBlocker.sql_handle)),
            (SELECT TOP 1 [text] FROM master..sysprocesses spBlocker CROSS APPLY sys.dm_exec_sql_text(spBlocker.sql_handle) WHERE spBlocker.spid = tBlocked.blocking_session_id)) AS QueryText,
        r.start_time AS StartTime,
        s.login_name AS LoginName,
        s.nt_user_name AS NTUserName,
        s.[program_name] AS ProgramName,
        s.[host_name] AS HostName,
        db.[resource_database_id] AS DatabaseID,
        DB_NAME(db.resource_database_id) AS DatabaseName,
        0 AS OpenTransactionCount
    FROM sys.dm_exec_sessions s
    INNER JOIN sys.dm_exec_requests r ON s.session_id = r.session_id
    INNER JOIN sys.dm_exec_connections c ON s.session_id = c.session_id
    INNER JOIN sys.dm_os_waiting_tasks tBlocked ON tBlocked.session_id = s.session_id AND tBlocked.session_id &lt;&gt; s.session_id
    INNER JOIN (
    SELECT DISTINCT request_session_id, resource_database_id
    FROM    sys.dm_tran_locks
    WHERE resource_type = N'DATABASE'
    AND     request_mode = N'S'
    AND     request_status = N'GRANT'
    AND     request_owner_type = N'SHARED_TRANSACTION_WORKSPACE') AS db ON s.session_id = db.request_session_id
    LEFT OUTER JOIN sys.dm_exec_requests rBlocker ON tBlocked.blocking_session_id = rBlocker.session_id
      WHERE NOT EXISTS (SELECT * FROM sys.dm_os_waiting_tasks tBlocker WHERE tBlocker.session_id = tBlocked.blocking_session_id AND tBlocker.blocking_session_id IS NOT NULL)
      AND s.last_request_start_time &lt; DATEADD(SECOND, -30, SYSDATETIMEOFFSET())
    */

    /* Query Problems - Plan Cache Erased Recently */
    IF DATEADD(mi, -15, SYSDATETIMEOFFSET()) &lt; (SELECT TOP 1 creation_time FROM sys.dm_exec_query_stats ORDER BY creation_time)
    BEGIN
        INSERT INTO #BlitzFirstResults (CheckID, Priority, FindingsGroup, Finding, URL, Details, HowToStopIt)
        SELECT TOP 1 7 AS CheckID,
            50 AS Priority,
            'Query Problems' AS FindingGroup,
            'Plan Cache Erased Recently' AS Finding,
            'http://www.BrentOzar.com/askbrent/plan-cache-erased-recently/' AS URL,
            'The oldest query in the plan cache was created at ' + CAST(creation_time AS NVARCHAR(50)) + '. ' + @LineFeed + @LineFeed
                + 'This indicates that someone ran DBCC FREEPROCCACHE at that time,' + @LineFeed
                + 'Giving SQL Server temporary amnesia. Now, as queries come in,' + @LineFeed
                + 'SQL Server has to use a lot of CPU power in order to build execution' + @LineFeed
                + 'plans and put them in cache again. This causes high CPU loads.' AS Details,
            'Find who did that, and stop them from doing it again.' AS HowToStopIt
        FROM sys.dm_exec_query_stats
        ORDER BY creation_time
    END;


    /* Query Problems - Sleeping Query with Open Transactions - CheckID 8 */
    IF @Seconds &gt; 0
    INSERT INTO #BlitzFirstResults (CheckID, Priority, FindingsGroup, Finding, URL, Details, HowToStopIt, StartTime, LoginName, NTUserName, ProgramName, HostName, DatabaseID, DatabaseName, QueryText, OpenTransactionCount)
    SELECT 8 AS CheckID,
        50 AS Priority,
        'Query Problems' AS FindingGroup,
        'Sleeping Query with Open Transactions' AS Finding,
        'http://www.brentozar.com/askbrent/sleeping-query-with-open-transactions/' AS URL,
        'Database: ' + DB_NAME(db.resource_database_id) + @LineFeed + 'Host: ' + s.[host_name] + @LineFeed + 'Program: ' + s.[program_name] + @LineFeed + 'Asleep with open transactions and locks since ' + CAST(s.last_request_end_time AS NVARCHAR(100)) + '. ' AS Details,
        'KILL ' + CAST(s.session_id AS NVARCHAR(100)) + ';' AS HowToStopIt,
        s.last_request_start_time AS StartTime,
        s.login_name AS LoginName,
        s.nt_user_name AS NTUserName,
        s.[program_name] AS ProgramName,
        s.[host_name] AS HostName,
        db.[resource_database_id] AS DatabaseID,
        DB_NAME(db.resource_database_id) AS DatabaseName,
        (SELECT TOP 1 [text] FROM sys.dm_exec_sql_text(c.most_recent_sql_handle)) AS QueryText,
        sessions_with_transactions.open_transaction_count AS OpenTransactionCount
    FROM (SELECT session_id, SUM(open_transaction_count) AS open_transaction_count FROM sys.dm_exec_requests WHERE open_transaction_count &gt; 0 GROUP BY session_id) AS sessions_with_transactions
    INNER JOIN sys.dm_exec_sessions s ON sessions_with_transactions.session_id = s.session_id
    INNER JOIN sys.dm_exec_connections c ON s.session_id = c.session_id
    INNER JOIN (
    SELECT DISTINCT request_session_id, resource_database_id
    FROM    sys.dm_tran_locks
    WHERE resource_type = N'DATABASE'
    AND     request_mode = N'S'
    AND     request_status = N'GRANT'
    AND     request_owner_type = N'SHARED_TRANSACTION_WORKSPACE') AS db ON s.session_id = db.request_session_id
    WHERE s.status = 'sleeping'
    AND s.last_request_end_time &lt; DATEADD(ss, -10, SYSDATETIMEOFFSET())
    AND EXISTS(SELECT * FROM sys.dm_tran_locks WHERE request_session_id = s.session_id
    AND NOT (resource_type = N'DATABASE' AND request_mode = N'S' AND request_status = N'GRANT' AND request_owner_type = N'SHARED_TRANSACTION_WORKSPACE'))


    /* Query Problems - Query Rolling Back - CheckID 9 */
    IF @Seconds &gt; 0
    INSERT INTO #BlitzFirstResults (CheckID, Priority, FindingsGroup, Finding, URL, Details, HowToStopIt, StartTime, LoginName, NTUserName, ProgramName, HostName, DatabaseID, DatabaseName, QueryText)
    SELECT 9 AS CheckID,
        1 AS Priority,
        'Query Problems' AS FindingGroup,
        'Query Rolling Back' AS Finding,
        'http://www.BrentOzar.com/askbrent/rollback/' AS URL,
        'Rollback started at ' + CAST(r.start_time AS NVARCHAR(100)) + ', is ' + CAST(r.percent_complete AS NVARCHAR(100)) + '% complete.' AS Details,
        'Unfortunately, you can''t stop this. Whatever you do, don''t restart the server in an attempt to fix it - SQL Server will keep rolling back.' AS HowToStopIt,
        r.start_time AS StartTime,
        s.login_name AS LoginName,
        s.nt_user_name AS NTUserName,
        s.[program_name] AS ProgramName,
        s.[host_name] AS HostName,
        db.[resource_database_id] AS DatabaseID,
        DB_NAME(db.resource_database_id) AS DatabaseName,
        (SELECT TOP 1 [text] FROM sys.dm_exec_sql_text(c.most_recent_sql_handle)) AS QueryText
    FROM sys.dm_exec_sessions s
    INNER JOIN sys.dm_exec_connections c ON s.session_id = c.session_id
    INNER JOIN sys.dm_exec_requests r ON s.session_id = r.session_id
    LEFT OUTER JOIN (
        SELECT DISTINCT request_session_id, resource_database_id
        FROM    sys.dm_tran_locks
        WHERE resource_type = N'DATABASE'
        AND     request_mode = N'S'
        AND     request_status = N'GRANT'
        AND     request_owner_type = N'SHARED_TRANSACTION_WORKSPACE') AS db ON s.session_id = db.request_session_id
    WHERE r.status = 'rollback'


    /* Server Performance - Page Life Expectancy Low - CheckID 10 */
    INSERT INTO #BlitzFirstResults (CheckID, Priority, FindingsGroup, Finding, URL, Details, HowToStopIt)
    SELECT 10 AS CheckID,
        50 AS Priority,
        'Server Performance' AS FindingGroup,
        'Page Life Expectancy Low' AS Finding,
        'http://www.BrentOzar.com/askbrent/page-life-expectancy/' AS URL,
        'SQL Server Buffer Manager:Page life expectancy is ' + CAST(c.cntr_value AS NVARCHAR(10)) + ' seconds.' + @LineFeed
            + 'This means SQL Server can only keep data pages in memory for that many seconds after reading those pages in from storage.' + @LineFeed
            + 'This is a symptom, not a cause - it indicates very read-intensive queries that need an index, or insufficient server memory.' AS Details,
        'Add more memory to the server, or find the queries reading a lot of data, and make them more efficient (or fix them with indexes).' AS HowToStopIt
    FROM sys.dm_os_performance_counters c
    WHERE object_name LIKE 'SQLServer:Buffer Manager%'
    AND counter_name LIKE 'Page life expectancy%'
    AND cntr_value &lt; 300

    /* Server Info - Database Size, Total GB - CheckID 21 */
    INSERT INTO #BlitzFirstResults (CheckID, Priority, FindingsGroup, Finding, Details, DetailsInt, URL)
    SELECT 21 AS CheckID,
        251 AS Priority,
        'Server Info' AS FindingGroup,
        'Database Size, Total GB' AS Finding,
        CAST(SUM (CAST(size AS BIGINT)*8./1024./1024.) AS VARCHAR(100)) AS Details,
        SUM (CAST(size AS BIGINT))*8./1024./1024. AS DetailsInt,
        'http://www.BrentOzar.com/askbrent/' AS URL
    FROM #MasterFiles
    WHERE database_id &gt; 4

    /* Server Info - Database Count - CheckID 22 */
    INSERT INTO #BlitzFirstResults (CheckID, Priority, FindingsGroup, Finding, Details, DetailsInt, URL)
    SELECT 22 AS CheckID,
        251 AS Priority,
        'Server Info' AS FindingGroup,
        'Database Count' AS Finding,
        CAST(SUM(1) AS VARCHAR(100)) AS Details,
        SUM (1) AS DetailsInt,
        'http://www.BrentOzar.com/askbrent/' AS URL
    FROM sys.databases
    WHERE database_id &gt; 4

    /* Server Performance - High CPU Utilization CheckID 24 */
    IF @Seconds &lt; 30
        BEGIN
        /* If we're waiting less than 30 seconds, run this check now rather than wait til the end.
           We get this data from the ring buffers, and it's only updated once per minute, so might
           as well get it now - whereas if we're checking 30+ seconds, it might get updated by the
           end of our sp_BlitzFirst session. */
        INSERT INTO #BlitzFirstResults (CheckID, Priority, FindingsGroup, Finding, Details, DetailsInt, URL)
        SELECT 24, 50, 'Server Performance', 'High CPU Utilization', CAST(100 - SystemIdle AS NVARCHAR(20)) + N'%. Ring buffer details: ' + CAST(record AS NVARCHAR(4000)), 100 - SystemIdle, 'http://www.BrentOzar.com/go/cpu'
            FROM (
                SELECT record,
                    record.value('(./Record/SchedulerMonitorEvent/SystemHealth/SystemIdle)[1]', 'int') AS SystemIdle
                FROM (
                    SELECT TOP 1 CONVERT(XML, record) AS record
                    FROM sys.dm_os_ring_buffers
                    WHERE ring_buffer_type = N'RING_BUFFER_SCHEDULER_MONITOR'
                    AND record LIKE '%&lt;SystemHealth&gt;%'
                    ORDER BY timestamp DESC) AS rb
            ) AS y
            WHERE 100 - SystemIdle &gt;= 50

        INSERT INTO #BlitzFirstResults (CheckID, Priority, FindingsGroup, Finding, Details, DetailsInt, URL)
        SELECT 23, 250, 'Server Info', 'CPU Utilization', CAST(100 - SystemIdle AS NVARCHAR(20)) + N'%. Ring buffer details: ' + CAST(record AS NVARCHAR(4000)), 100 - SystemIdle, 'http://www.BrentOzar.com/go/cpu'
            FROM (
                SELECT record,
                    record.value('(./Record/SchedulerMonitorEvent/SystemHealth/SystemIdle)[1]', 'int') AS SystemIdle
                FROM (
                    SELECT TOP 1 CONVERT(XML, record) AS record
                    FROM sys.dm_os_ring_buffers
                    WHERE ring_buffer_type = N'RING_BUFFER_SCHEDULER_MONITOR'
                    AND record LIKE '%&lt;SystemHealth&gt;%'
                    ORDER BY timestamp DESC) AS rb
            ) AS y

        END /* IF @Seconds &lt; 30 */

	RAISERROR('Finished running investigatory queries',10,1) WITH NOWAIT;


    /* End of checks. If we haven't waited @Seconds seconds, wait. */
    IF SYSDATETIMEOFFSET() &lt; @FinishSampleTime
		BEGIN
		RAISERROR('Waiting to match @Seconds parameter',10,1) WITH NOWAIT;
        WAITFOR TIME @FinishSampleTimeWaitFor;
		END

	RAISERROR('Capturing second pass of wait stats, perfmon counters, file stats',10,1) WITH NOWAIT;
    /* Populate #FileStats, #PerfmonStats, #WaitStats with DMV data. In a second, we'll compare these. */
    INSERT #WaitStats(Pass, SampleTime, wait_type, wait_time_ms, signal_wait_time_ms, waiting_tasks_count)
		SELECT 
		x.Pass, 
		x.SampleTime, 
		x.wait_type, 
		SUM(x.sum_wait_time_ms) AS sum_wait_time_ms, 
		SUM(x.sum_signal_wait_time_ms) AS sum_signal_wait_time_ms, 
		SUM(x.sum_waiting_tasks) AS sum_waiting_tasks
		FROM (
		SELECT  
				2 AS Pass,
				SYSDATETIMEOFFSET() AS SampleTime,
				owt.wait_type,
		        SUM(owt.wait_duration_ms) OVER (PARTITION BY owt.wait_type, owt.session_id)
					 - CASE WHEN @Seconds = 0 THEN 0 ELSE (@Seconds * 1000) END AS sum_wait_time_ms,
				0 AS sum_signal_wait_time_ms,
				CASE @Seconds WHEN 0 THEN 0 ELSE 1 END AS sum_waiting_tasks
			FROM    sys.dm_os_waiting_tasks owt
			WHERE owt.session_id &gt; 50
			AND owt.wait_duration_ms &gt;= CASE @Seconds WHEN 0 THEN 0 ELSE @Seconds * 1000 END
		UNION ALL
		SELECT
		       2 AS Pass,
		       SYSDATETIMEOFFSET() AS SampleTime,
		       os.wait_type,
			   SUM(os.wait_time_ms) OVER (PARTITION BY os.wait_type) AS sum_wait_time_ms,
			   SUM(os.signal_wait_time_ms) OVER (PARTITION BY os.wait_type ) AS sum_signal_wait_time_ms,
			   SUM(os.waiting_tasks_count) OVER (PARTITION BY os.wait_type) AS sum_waiting_tasks
		   FROM sys.dm_os_wait_stats os
		) x
		   WHERE x.wait_type NOT IN (
		       'REQUEST_FOR_DEADLOCK_SEARCH',
		       'SQLTRACE_INCREMENTAL_FLUSH_SLEEP',
		       'SQLTRACE_BUFFER_FLUSH',
		       'LAZYWRITER_SLEEP',
		       'XE_TIMER_EVENT',
		       'XE_DISPATCHER_WAIT',
		       'FT_IFTS_SCHEDULER_IDLE_WAIT',
		       'LOGMGR_QUEUE',
		       'CHECKPOINT_QUEUE',
		       'BROKER_TO_FLUSH',
		       'BROKER_TASK_STOP',
		       'BROKER_EVENTHANDLER',
		       'SLEEP_TASK',
		       'WAITFOR',
		       'DBMIRROR_DBM_MUTEX',
		       'DBMIRROR_EVENTS_QUEUE',
		       'DBMIRRORING_CMD',
		       'DISPATCHER_QUEUE_SEMAPHORE',
		       'BROKER_RECEIVE_WAITFOR',
		       'CLR_AUTO_EVENT',
		       'DIRTY_PAGE_POLL',
		       'HADR_FILESTREAM_IOMGR_IOCOMPLETION',
		       'ONDEMAND_TASK_QUEUE',
		       'FT_IFTSHC_MUTEX',
		       'CLR_MANUAL_EVENT',
		       'CLR_SEMAPHORE',
		       'DBMIRROR_WORKER_QUEUE',
		       'DBMIRROR_DBM_EVENT',
		       'SP_SERVER_DIAGNOSTICS_SLEEP',
		       'HADR_CLUSAPI_CALL',
		       'HADR_LOGCAPTURE_WAIT',
		       'HADR_NOTIFICATION_DEQUEUE',
		       'HADR_TIMER_TASK',
		       'HADR_WORK_QUEUE',
		       'QDS_PERSIST_TASK_MAIN_LOOP_SLEEP',
		       'QDS_CLEANUP_STALE_QUERIES_TASK_MAIN_LOOP_SLEEP',
		       'RESOURCE_GOVERNOR_IDLE',
		       'QDS_ASYNC_QUEUE',
		       'QDS_SHUTDOWN_QUEUE',
		       'SLEEP_SYSTEMTASK',
		       'BROKER_TRANSMITTER',
		       'REDO_THREAD_PENDING_WORK',
		       'UCS_SESSION_REGISTRATION'
		   )
		GROUP BY x.Pass, x.SampleTime, x.wait_type
		ORDER BY sum_wait_time_ms DESC;

    INSERT INTO #FileStats (Pass, SampleTime, DatabaseID, FileID, DatabaseName, FileLogicalName, SizeOnDiskMB, io_stall_read_ms ,
        num_of_reads, [bytes_read] , io_stall_write_ms,num_of_writes, [bytes_written], PhysicalName, TypeDesc, avg_stall_read_ms, avg_stall_write_ms)
    SELECT         2 AS Pass,
        SYSDATETIMEOFFSET() AS SampleTime,
        mf.[database_id],
        mf.[file_id],
        DB_NAME(vfs.database_id) AS [db_name],
        mf.name + N' [' + mf.type_desc COLLATE SQL_Latin1_General_CP1_CI_AS + N']' AS file_logical_name ,
        CAST(( ( vfs.size_on_disk_bytes / 1024.0 ) / 1024.0 ) AS INT) AS size_on_disk_mb ,
        vfs.io_stall_read_ms ,
        vfs.num_of_reads ,
        vfs.[num_of_bytes_read],
        vfs.io_stall_write_ms ,
        vfs.num_of_writes ,
        vfs.[num_of_bytes_written],
        mf.physical_name,
        mf.type_desc,
        0,
        0
    FROM sys.dm_io_virtual_file_stats (NULL, NULL) AS vfs
    INNER JOIN #MasterFiles AS mf ON vfs.file_id = mf.file_id
        AND vfs.database_id = mf.database_id
    WHERE vfs.num_of_reads &gt; 0
        OR vfs.num_of_writes &gt; 0;

    INSERT INTO #PerfmonStats (Pass, SampleTime, [object_name],[counter_name],[instance_name],[cntr_value],[cntr_type])
    SELECT         2 AS Pass,
        SYSDATETIMEOFFSET() AS SampleTime,
        RTRIM(dmv.object_name), RTRIM(dmv.counter_name), RTRIM(dmv.instance_name), dmv.cntr_value, dmv.cntr_type
        FROM #PerfmonCounters counters
        INNER JOIN sys.dm_os_performance_counters dmv ON counters.counter_name COLLATE SQL_Latin1_General_CP1_CI_AS = RTRIM(dmv.counter_name) COLLATE SQL_Latin1_General_CP1_CI_AS
            AND counters.[object_name] COLLATE SQL_Latin1_General_CP1_CI_AS = RTRIM(dmv.[object_name]) COLLATE SQL_Latin1_General_CP1_CI_AS
            AND (counters.[instance_name] IS NULL OR counters.[instance_name] COLLATE SQL_Latin1_General_CP1_CI_AS = RTRIM(dmv.[instance_name]) COLLATE SQL_Latin1_General_CP1_CI_AS)

    /* Set the latencies and averages. We could do this with a CTE, but we're not ambitious today. */
    UPDATE fNow
    SET avg_stall_read_ms = ((fNow.io_stall_read_ms - fBase.io_stall_read_ms) / (fNow.num_of_reads - fBase.num_of_reads))
    FROM #FileStats fNow
    INNER JOIN #FileStats fBase ON fNow.DatabaseID = fBase.DatabaseID AND fNow.FileID = fBase.FileID AND fNow.SampleTime &gt; fBase.SampleTime AND fNow.num_of_reads &gt; fBase.num_of_reads AND fNow.io_stall_read_ms &gt; fBase.io_stall_read_ms
    WHERE (fNow.num_of_reads - fBase.num_of_reads) &gt; 0

    UPDATE fNow
    SET avg_stall_write_ms = ((fNow.io_stall_write_ms - fBase.io_stall_write_ms) / (fNow.num_of_writes - fBase.num_of_writes))
    FROM #FileStats fNow
    INNER JOIN #FileStats fBase ON fNow.DatabaseID = fBase.DatabaseID AND fNow.FileID = fBase.FileID AND fNow.SampleTime &gt; fBase.SampleTime AND fNow.num_of_writes &gt; fBase.num_of_writes AND fNow.io_stall_write_ms &gt; fBase.io_stall_write_ms
    WHERE (fNow.num_of_writes - fBase.num_of_writes) &gt; 0

    UPDATE pNow
        SET [value_delta] = pNow.cntr_value - pFirst.cntr_value,
            [value_per_second] = ((1.0 * pNow.cntr_value - pFirst.cntr_value) / DATEDIFF(ss, pFirst.SampleTime, pNow.SampleTime))
        FROM #PerfmonStats pNow
            INNER JOIN #PerfmonStats pFirst ON pFirst.[object_name] = pNow.[object_name] AND pFirst.counter_name = pNow.counter_name AND (pFirst.instance_name = pNow.instance_name OR (pFirst.instance_name IS NULL AND pNow.instance_name IS NULL))
                AND pNow.ID &gt; pFirst.ID
        WHERE  DATEDIFF(ss, pFirst.SampleTime, pNow.SampleTime) &gt; 0;


    /* If we're within 10 seconds of our projected finish time, do the plan cache analysis. */
    IF DATEDIFF(ss, @FinishSampleTime, SYSDATETIMEOFFSET()) &gt; 10 AND @CheckProcedureCache = 1
        BEGIN

            INSERT INTO #BlitzFirstResults (CheckID, Priority, FindingsGroup, Finding, URL, Details)
            VALUES (18, 210, 'Query Stats', 'Plan Cache Analysis Skipped', 'http://www.BrentOzar.com/go/topqueries',
                'Due to excessive load, the plan cache analysis was skipped. To override this, use @ExpertMode = 1.')

        END
    ELSE IF @CheckProcedureCache = 1
        BEGIN


		RAISERROR('@CheckProcedureCache = 1, capturing second pass of plan cache',10,1) WITH NOWAIT;

        /* Populate #QueryStats. SQL 2005 doesn't have query hash or query plan hash. */
		IF @@VERSION LIKE 'Microsoft SQL Server 2005%'
			BEGIN
			IF @FilterPlansByDatabase IS NULL
				BEGIN
				SET @StringToExecute = N'INSERT INTO #QueryStats ([sql_handle], Pass, SampleTime, statement_start_offset, statement_end_offset, plan_generation_num, plan_handle, execution_count, total_worker_time, total_physical_reads, total_logical_writes, total_logical_reads, total_clr_time, total_elapsed_time, creation_time, query_hash, query_plan_hash, Points)
											SELECT [sql_handle], 2 AS Pass, SYSDATETIMEOFFSET(), statement_start_offset, statement_end_offset, plan_generation_num, plan_handle, execution_count, total_worker_time, total_physical_reads, total_logical_writes, total_logical_reads, total_clr_time, total_elapsed_time, creation_time, NULL AS query_hash, NULL AS query_plan_hash, 0
											FROM sys.dm_exec_query_stats qs
											WHERE qs.last_execution_time &gt;= @StartSampleTimeText;';
				END
			ELSE
				BEGIN
				SET @StringToExecute = N'INSERT INTO #QueryStats ([sql_handle], Pass, SampleTime, statement_start_offset, statement_end_offset, plan_generation_num, plan_handle, execution_count, total_worker_time, total_physical_reads, total_logical_writes, total_logical_reads, total_clr_time, total_elapsed_time, creation_time, query_hash, query_plan_hash, Points)
											SELECT [sql_handle], 2 AS Pass, SYSDATETIMEOFFSET(), statement_start_offset, statement_end_offset, plan_generation_num, plan_handle, execution_count, total_worker_time, total_physical_reads, total_logical_writes, total_logical_reads, total_clr_time, total_elapsed_time, creation_time, NULL AS query_hash, NULL AS query_plan_hash, 0
											FROM sys.dm_exec_query_stats qs
												CROSS APPLY sys.dm_exec_plan_attributes(qs.plan_handle) AS attr
												INNER JOIN #FilterPlansByDatabase dbs ON CAST(attr.value AS INT) = dbs.DatabaseID
											WHERE qs.last_execution_time &gt;= @StartSampleTimeText
												AND attr.attribute = ''dbid'';';
				END
			END
		ELSE
			BEGIN
			IF @FilterPlansByDatabase IS NULL
				BEGIN
				SET @StringToExecute = N'INSERT INTO #QueryStats ([sql_handle], Pass, SampleTime, statement_start_offset, statement_end_offset, plan_generation_num, plan_handle, execution_count, total_worker_time, total_physical_reads, total_logical_writes, total_logical_reads, total_clr_time, total_elapsed_time, creation_time, query_hash, query_plan_hash, Points)
											SELECT [sql_handle], 2 AS Pass, SYSDATETIMEOFFSET(), statement_start_offset, statement_end_offset, plan_generation_num, plan_handle, execution_count, total_worker_time, total_physical_reads, total_logical_writes, total_logical_reads, total_clr_time, total_elapsed_time, creation_time, query_hash, query_plan_hash, 0
											FROM sys.dm_exec_query_stats qs
											WHERE qs.last_execution_time &gt;= @StartSampleTimeText';
				END
			ELSE
				BEGIN
				SET @StringToExecute = N'INSERT INTO #QueryStats ([sql_handle], Pass, SampleTime, statement_start_offset, statement_end_offset, plan_generation_num, plan_handle, execution_count, total_worker_time, total_physical_reads, total_logical_writes, total_logical_reads, total_clr_time, total_elapsed_time, creation_time, query_hash, query_plan_hash, Points)
											SELECT [sql_handle], 2 AS Pass, SYSDATETIMEOFFSET(), statement_start_offset, statement_end_offset, plan_generation_num, plan_handle, execution_count, total_worker_time, total_physical_reads, total_logical_writes, total_logical_reads, total_clr_time, total_elapsed_time, creation_time, query_hash, query_plan_hash, 0
											FROM sys.dm_exec_query_stats qs
											CROSS APPLY sys.dm_exec_plan_attributes(qs.plan_handle) AS attr
											INNER JOIN #FilterPlansByDatabase dbs ON CAST(attr.value AS INT) = dbs.DatabaseID
											WHERE qs.last_execution_time &gt;= @StartSampleTimeText
												AND attr.attribute = ''dbid'';';
				END
			END
		/* Old version pre-2016/06/13:
        IF @@VERSION LIKE 'Microsoft SQL Server 2005%'
            SET @StringToExecute = N'INSERT INTO #QueryStats ([sql_handle], Pass, SampleTime, statement_start_offset, statement_end_offset, plan_generation_num, plan_handle, execution_count, total_worker_time, total_physical_reads, total_logical_writes, total_logical_reads, total_clr_time, total_elapsed_time, creation_time, query_hash, query_plan_hash, Points)
                                        SELECT [sql_handle], 2 AS Pass, SYSDATETIMEOFFSET(), statement_start_offset, statement_end_offset, plan_generation_num, plan_handle, execution_count, total_worker_time, total_physical_reads, total_logical_writes, total_logical_reads, total_clr_time, total_elapsed_time, creation_time, NULL AS query_hash, NULL AS query_plan_hash, 0
                                        FROM sys.dm_exec_query_stats qs
                                        WHERE qs.last_execution_time &gt;= @StartSampleTimeText;';
        ELSE
            SET @StringToExecute = N'INSERT INTO #QueryStats ([sql_handle], Pass, SampleTime, statement_start_offset, statement_end_offset, plan_generation_num, plan_handle, execution_count, total_worker_time, total_physical_reads, total_logical_writes, total_logical_reads, total_clr_time, total_elapsed_time, creation_time, query_hash, query_plan_hash, Points)
                                        SELECT [sql_handle], 2 AS Pass, SYSDATETIMEOFFSET(), statement_start_offset, statement_end_offset, plan_generation_num, plan_handle, execution_count, total_worker_time, total_physical_reads, total_logical_writes, total_logical_reads, total_clr_time, total_elapsed_time, creation_time, query_hash, query_plan_hash, 0
                                        FROM sys.dm_exec_query_stats qs
                                        WHERE qs.last_execution_time &gt;= @StartSampleTimeText;';
		*/
        SET @ParmDefinitions = N'@StartSampleTimeText NVARCHAR(100)';
        SET @Parm1 = CONVERT(NVARCHAR(100), CAST(@StartSampleTime AS DATETIME), 127);

        EXECUTE sp_executesql @StringToExecute, @ParmDefinitions, @StartSampleTimeText = @Parm1;

		RAISERROR('@CheckProcedureCache = 1, totaling up plan cache metrics',10,1) WITH NOWAIT;

        /* Get the totals for the entire plan cache */
        INSERT INTO #QueryStats (Pass, SampleTime, execution_count, total_worker_time, total_physical_reads, total_logical_writes, total_logical_reads, total_clr_time, total_elapsed_time, creation_time)
        SELECT 0 AS Pass, SYSDATETIMEOFFSET(), SUM(execution_count), SUM(total_worker_time), SUM(total_physical_reads), SUM(total_logical_writes), SUM(total_logical_reads), SUM(total_clr_time), SUM(total_elapsed_time), MIN(creation_time)
            FROM sys.dm_exec_query_stats qs;


		RAISERROR('@CheckProcedureCache = 1, so analyzing execution plans',10,1) WITH NOWAIT;
        /*
        Pick the most resource-intensive queries to review. Update the Points field
        in #QueryStats - if a query is in the top 10 for logical reads, CPU time,
        duration, or execution, add 1 to its points.
        */
        WITH qsTop AS (
        SELECT TOP 10 qsNow.ID
        FROM #QueryStats qsNow
          INNER JOIN #QueryStats qsFirst ON qsNow.[sql_handle] = qsFirst.[sql_handle] AND qsNow.statement_start_offset = qsFirst.statement_start_offset AND qsNow.statement_end_offset = qsFirst.statement_end_offset AND qsNow.plan_generation_num = qsFirst.plan_generation_num AND qsNow.plan_handle = qsFirst.plan_handle AND qsFirst.Pass = 1
        WHERE qsNow.total_elapsed_time &gt; qsFirst.total_elapsed_time
            AND qsNow.Pass = 2
            AND qsNow.total_elapsed_time - qsFirst.total_elapsed_time &gt; 1000000 /* Only queries with over 1 second of runtime */
        ORDER BY (qsNow.total_elapsed_time - COALESCE(qsFirst.total_elapsed_time, 0)) DESC)
        UPDATE #QueryStats
            SET Points = Points + 1
            FROM #QueryStats qs
            INNER JOIN qsTop ON qs.ID = qsTop.ID;

        WITH qsTop AS (
        SELECT TOP 10 qsNow.ID
        FROM #QueryStats qsNow
          INNER JOIN #QueryStats qsFirst ON qsNow.[sql_handle] = qsFirst.[sql_handle] AND qsNow.statement_start_offset = qsFirst.statement_start_offset AND qsNow.statement_end_offset = qsFirst.statement_end_offset AND qsNow.plan_generation_num = qsFirst.plan_generation_num AND qsNow.plan_handle = qsFirst.plan_handle AND qsFirst.Pass = 1
        WHERE qsNow.total_logical_reads &gt; qsFirst.total_logical_reads
            AND qsNow.Pass = 2
            AND qsNow.total_logical_reads - qsFirst.total_logical_reads &gt; 1000 /* Only queries with over 1000 reads */
        ORDER BY (qsNow.total_logical_reads - COALESCE(qsFirst.total_logical_reads, 0)) DESC)
        UPDATE #QueryStats
            SET Points = Points + 1
            FROM #QueryStats qs
            INNER JOIN qsTop ON qs.ID = qsTop.ID;

        WITH qsTop AS (
        SELECT TOP 10 qsNow.ID
        FROM #QueryStats qsNow
          INNER JOIN #QueryStats qsFirst ON qsNow.[sql_handle] = qsFirst.[sql_handle] AND qsNow.statement_start_offset = qsFirst.statement_start_offset AND qsNow.statement_end_offset = qsFirst.statement_end_offset AND qsNow.plan_generation_num = qsFirst.plan_generation_num AND qsNow.plan_handle = qsFirst.plan_handle AND qsFirst.Pass = 1
        WHERE qsNow.total_worker_time &gt; qsFirst.total_worker_time
            AND qsNow.Pass = 2
            AND qsNow.total_worker_time - qsFirst.total_worker_time &gt; 1000000 /* Only queries with over 1 second of worker time */
        ORDER BY (qsNow.total_worker_time - COALESCE(qsFirst.total_worker_time, 0)) DESC)
        UPDATE #QueryStats
            SET Points = Points + 1
            FROM #QueryStats qs
            INNER JOIN qsTop ON qs.ID = qsTop.ID;

        WITH qsTop AS (
        SELECT TOP 10 qsNow.ID
        FROM #QueryStats qsNow
          INNER JOIN #QueryStats qsFirst ON qsNow.[sql_handle] = qsFirst.[sql_handle] AND qsNow.statement_start_offset = qsFirst.statement_start_offset AND qsNow.statement_end_offset = qsFirst.statement_end_offset AND qsNow.plan_generation_num = qsFirst.plan_generation_num AND qsNow.plan_handle = qsFirst.plan_handle AND qsFirst.Pass = 1
        WHERE qsNow.execution_count &gt; qsFirst.execution_count
            AND qsNow.Pass = 2
            AND (qsNow.total_elapsed_time - qsFirst.total_elapsed_time &gt; 1000000 /* Only queries with over 1 second of runtime */
                OR qsNow.total_logical_reads - qsFirst.total_logical_reads &gt; 1000 /* Only queries with over 1000 reads */
                OR qsNow.total_worker_time - qsFirst.total_worker_time &gt; 1000000 /* Only queries with over 1 second of worker time */)
        ORDER BY (qsNow.execution_count - COALESCE(qsFirst.execution_count, 0)) DESC)
        UPDATE #QueryStats
            SET Points = Points + 1
            FROM #QueryStats qs
            INNER JOIN qsTop ON qs.ID = qsTop.ID;

        /* Query Stats - CheckID 17 - Most Resource-Intensive Queries */
        INSERT INTO #BlitzFirstResults (CheckID, Priority, FindingsGroup, Finding, URL, Details, HowToStopIt, QueryPlan, QueryText, QueryStatsNowID, QueryStatsFirstID, PlanHandle)
        SELECT 17, 210, 'Query Stats', 'Most Resource-Intensive Queries', 'http://www.BrentOzar.com/go/topqueries',
            'Query stats during the sample:' + @LineFeed +
            'Executions: ' + CAST(qsNow.execution_count - (COALESCE(qsFirst.execution_count, 0)) AS NVARCHAR(100)) + @LineFeed +
            'Elapsed Time: ' + CAST(qsNow.total_elapsed_time - (COALESCE(qsFirst.total_elapsed_time, 0)) AS NVARCHAR(100)) + @LineFeed +
            'CPU Time: ' + CAST(qsNow.total_worker_time - (COALESCE(qsFirst.total_worker_time, 0)) AS NVARCHAR(100)) + @LineFeed +
            'Logical Reads: ' + CAST(qsNow.total_logical_reads - (COALESCE(qsFirst.total_logical_reads, 0)) AS NVARCHAR(100)) + @LineFeed +
            'Logical Writes: ' + CAST(qsNow.total_logical_writes - (COALESCE(qsFirst.total_logical_writes, 0)) AS NVARCHAR(100)) + @LineFeed +
            'CLR Time: ' + CAST(qsNow.total_clr_time - (COALESCE(qsFirst.total_clr_time, 0)) AS NVARCHAR(100)) + @LineFeed +
            @LineFeed + @LineFeed + 'Query stats since ' + CONVERT(NVARCHAR(100), qsNow.creation_time ,121) + @LineFeed +
            'Executions: ' + CAST(qsNow.execution_count AS NVARCHAR(100)) +
                    CASE qsTotal.execution_count WHEN 0 THEN '' ELSE (' - Percent of Server Total: ' + CAST(CAST(100.0 * qsNow.execution_count / qsTotal.execution_count AS DECIMAL(6,2)) AS NVARCHAR(100)) + '%') END + @LineFeed +
            'Elapsed Time: ' + CAST(qsNow.total_elapsed_time AS NVARCHAR(100)) +
                    CASE qsTotal.total_elapsed_time WHEN 0 THEN '' ELSE (' - Percent of Server Total: ' + CAST(CAST(100.0 * qsNow.total_elapsed_time / qsTotal.total_elapsed_time AS DECIMAL(6,2)) AS NVARCHAR(100)) + '%') END + @LineFeed +
            'CPU Time: ' + CAST(qsNow.total_worker_time AS NVARCHAR(100)) +
                    CASE qsTotal.total_worker_time WHEN 0 THEN '' ELSE (' - Percent of Server Total: ' + CAST(CAST(100.0 * qsNow.total_worker_time / qsTotal.total_worker_time AS DECIMAL(6,2)) AS NVARCHAR(100)) + '%') END + @LineFeed +
            'Logical Reads: ' + CAST(qsNow.total_logical_reads AS NVARCHAR(100)) +
                    CASE qsTotal.total_logical_reads WHEN 0 THEN '' ELSE (' - Percent of Server Total: ' + CAST(CAST(100.0 * qsNow.total_logical_reads / qsTotal.total_logical_reads AS DECIMAL(6,2)) AS NVARCHAR(100)) + '%') END + @LineFeed +
            'Logical Writes: ' + CAST(qsNow.total_logical_writes AS NVARCHAR(100)) +
                    CASE qsTotal.total_logical_writes WHEN 0 THEN '' ELSE (' - Percent of Server Total: ' + CAST(CAST(100.0 * qsNow.total_logical_writes / qsTotal.total_logical_writes AS DECIMAL(6,2)) AS NVARCHAR(100)) + '%') END + @LineFeed +
            'CLR Time: ' + CAST(qsNow.total_clr_time AS NVARCHAR(100)) +
                    CASE qsTotal.total_clr_time WHEN 0 THEN '' ELSE (' - Percent of Server Total: ' + CAST(CAST(100.0 * qsNow.total_clr_time / qsTotal.total_clr_time AS DECIMAL(6,2)) AS NVARCHAR(100)) + '%') END + @LineFeed +
            --@LineFeed + @LineFeed + 'Query hash: ' + CAST(qsNow.query_hash AS NVARCHAR(100)) + @LineFeed +
            --@LineFeed + @LineFeed + 'Query plan hash: ' + CAST(qsNow.query_plan_hash AS NVARCHAR(100)) +
            @LineFeed AS Details,
            'See the URL for tuning tips on why this query may be consuming resources.' AS HowToStopIt,
            qp.query_plan,
            QueryText = SUBSTRING(st.text,
                 (qsNow.statement_start_offset / 2) + 1,
                 ((CASE qsNow.statement_end_offset
                   WHEN -1 THEN DATALENGTH(st.text)
                   ELSE qsNow.statement_end_offset
                   END - qsNow.statement_start_offset) / 2) + 1),
            qsNow.ID AS QueryStatsNowID,
            qsFirst.ID AS QueryStatsFirstID,
            qsNow.plan_handle AS PlanHandle
            FROM #QueryStats qsNow
                INNER JOIN #QueryStats qsTotal ON qsTotal.Pass = 0
                LEFT OUTER JOIN #QueryStats qsFirst ON qsNow.[sql_handle] = qsFirst.[sql_handle] AND qsNow.statement_start_offset = qsFirst.statement_start_offset AND qsNow.statement_end_offset = qsFirst.statement_end_offset AND qsNow.plan_generation_num = qsFirst.plan_generation_num AND qsNow.plan_handle = qsFirst.plan_handle AND qsFirst.Pass = 1
                CROSS APPLY sys.dm_exec_sql_text(qsNow.sql_handle) AS st
                CROSS APPLY sys.dm_exec_query_plan(qsNow.plan_handle) AS qp
            WHERE qsNow.Points &gt; 0 AND st.text IS NOT NULL AND qp.query_plan IS NOT NULL

            UPDATE #BlitzFirstResults
                SET DatabaseID = CAST(attr.value AS INT),
                DatabaseName = DB_NAME(CAST(attr.value AS INT))
            FROM #BlitzFirstResults
                CROSS APPLY sys.dm_exec_plan_attributes(#BlitzFirstResults.PlanHandle) AS attr
            WHERE attr.attribute = 'dbid'


        END /* IF DATEDIFF(ss, @FinishSampleTime, SYSDATETIMEOFFSET()) &gt; 10 AND @CheckProcedureCache = 1 */


	RAISERROR('Analyzing changes between first and second passes of DMVs',10,1) WITH NOWAIT;

    /* Wait Stats - CheckID 6 */
    /* Compare the current wait stats to the sample we took at the start, and insert the top 10 waits. */
    INSERT INTO #BlitzFirstResults (CheckID, Priority, FindingsGroup, Finding, URL, Details, HowToStopIt, DetailsInt)
    SELECT TOP 10 6 AS CheckID,
        200 AS Priority,
        'Wait Stats' AS FindingGroup,
        wNow.wait_type AS Finding,
        N'http://www.brentozar.com/sql/wait-stats/#' + wNow.wait_type AS URL,
        'For ' + CAST(((wNow.wait_time_ms - COALESCE(wBase.wait_time_ms,0)) / 1000) AS NVARCHAR(100)) + ' seconds over the last ' + CASE @Seconds WHEN 0 THEN (CAST(DATEDIFF(dd,@StartSampleTime,@FinishSampleTime) AS NVARCHAR(10)) + ' days') ELSE (CAST(@Seconds AS NVARCHAR(10)) + ' seconds') END + ', SQL Server was waiting on this particular bottleneck.' + @LineFeed + @LineFeed AS Details,
        'See the URL for more details on how to mitigate this wait type.' AS HowToStopIt,
        ((wNow.wait_time_ms - COALESCE(wBase.wait_time_ms,0)) / 1000) AS DetailsInt
    FROM #WaitStats wNow
    LEFT OUTER JOIN #WaitStats wBase ON wNow.wait_type = wBase.wait_type AND wNow.SampleTime &gt; wBase.SampleTime
    WHERE wNow.wait_time_ms &gt; (wBase.wait_time_ms + (.5 * (DATEDIFF(ss,@StartSampleTime,@FinishSampleTime)) * 1000)) /* Only look for things we've actually waited on for half of the time or more */
    ORDER BY (wNow.wait_time_ms - COALESCE(wBase.wait_time_ms,0)) DESC;

    /* Server Performance - Slow Data File Reads - CheckID 11 */
    INSERT INTO #BlitzFirstResults (CheckID, Priority, FindingsGroup, Finding, URL, Details, HowToStopIt, DatabaseID, DatabaseName)
    SELECT TOP 10 11 AS CheckID,
        50 AS Priority,
        'Server Performance' AS FindingGroup,
        'Slow Data File Reads' AS Finding,
        'http://www.BrentOzar.com/go/slow/' AS URL,
        'File: ' + fNow.PhysicalName + @LineFeed
            + 'Number of reads during the sample: ' + CAST((fNow.num_of_reads - fBase.num_of_reads) AS NVARCHAR(20)) + @LineFeed
            + 'Seconds spent waiting on storage for these reads: ' + CAST(((fNow.io_stall_read_ms - fBase.io_stall_read_ms) / 1000.0) AS NVARCHAR(20)) + @LineFeed
            + 'Average read latency during the sample: ' + CAST(((fNow.io_stall_read_ms - fBase.io_stall_read_ms) / (fNow.num_of_reads - fBase.num_of_reads) ) AS NVARCHAR(20)) + ' milliseconds' + @LineFeed
            + 'Microsoft guidance for data file read speed: 20ms or less.' + @LineFeed + @LineFeed AS Details,
        'See the URL for more details on how to mitigate this wait type.' AS HowToStopIt,
        fNow.DatabaseID,
        fNow.DatabaseName
    FROM #FileStats fNow
    INNER JOIN #FileStats fBase ON fNow.DatabaseID = fBase.DatabaseID AND fNow.FileID = fBase.FileID AND fNow.SampleTime &gt; fBase.SampleTime AND fNow.num_of_reads &gt; fBase.num_of_reads AND fNow.io_stall_read_ms &gt; (fBase.io_stall_read_ms + 1000)
    WHERE (fNow.io_stall_read_ms - fBase.io_stall_read_ms) / (fNow.num_of_reads - fBase.num_of_reads) &gt;= @FileLatencyThresholdMS
        AND fNow.TypeDesc = 'ROWS'
    ORDER BY (fNow.io_stall_read_ms - fBase.io_stall_read_ms) / (fNow.num_of_reads - fBase.num_of_reads) DESC;

    /* Server Performance - Slow Log File Writes - CheckID 12 */
    INSERT INTO #BlitzFirstResults (CheckID, Priority, FindingsGroup, Finding, URL, Details, HowToStopIt, DatabaseID, DatabaseName)
    SELECT TOP 10 12 AS CheckID,
        50 AS Priority,
        'Server Performance' AS FindingGroup,
        'Slow Log File Writes' AS Finding,
        'http://www.BrentOzar.com/go/slow/' AS URL,
        'File: ' + fNow.PhysicalName + @LineFeed
            + 'Number of writes during the sample: ' + CAST((fNow.num_of_writes - fBase.num_of_writes) AS NVARCHAR(20)) + @LineFeed
            + 'Seconds spent waiting on storage for these writes: ' + CAST(((fNow.io_stall_write_ms - fBase.io_stall_write_ms) / 1000.0) AS NVARCHAR(20)) + @LineFeed
            + 'Average write latency during the sample: ' + CAST(((fNow.io_stall_write_ms - fBase.io_stall_write_ms) / (fNow.num_of_writes - fBase.num_of_writes) ) AS NVARCHAR(20)) + ' milliseconds' + @LineFeed
            + 'Microsoft guidance for log file write speed: 3ms or less.' + @LineFeed + @LineFeed AS Details,
        'See the URL for more details on how to mitigate this wait type.' AS HowToStopIt,
        fNow.DatabaseID,
        fNow.DatabaseName
    FROM #FileStats fNow
    INNER JOIN #FileStats fBase ON fNow.DatabaseID = fBase.DatabaseID AND fNow.FileID = fBase.FileID AND fNow.SampleTime &gt; fBase.SampleTime AND fNow.num_of_writes &gt; fBase.num_of_writes AND fNow.io_stall_write_ms &gt; (fBase.io_stall_write_ms + 1000)
    WHERE (fNow.io_stall_write_ms - fBase.io_stall_write_ms) / (fNow.num_of_writes - fBase.num_of_writes) &gt;= @FileLatencyThresholdMS
        AND fNow.TypeDesc = 'LOG'
    ORDER BY (fNow.io_stall_write_ms - fBase.io_stall_write_ms) / (fNow.num_of_writes - fBase.num_of_writes) DESC;


    /* SQL Server Internal Maintenance - Log File Growing - CheckID 13 */
    INSERT INTO #BlitzFirstResults (CheckID, Priority, FindingsGroup, Finding, URL, Details, HowToStopIt)
    SELECT 13 AS CheckID,
        1 AS Priority,
        'SQL Server Internal Maintenance' AS FindingGroup,
        'Log File Growing' AS Finding,
        'http://www.BrentOzar.com/askbrent/file-growing/' AS URL,
        'Number of growths during the sample: ' + CAST(ps.value_delta AS NVARCHAR(20)) + @LineFeed
            + 'Determined by sampling Perfmon counter ' + ps.object_name + ' - ' + ps.counter_name + @LineFeed AS Details,
        'Pre-grow data and log files during maintenance windows so that they do not grow during production loads. See the URL for more details.'  AS HowToStopIt
    FROM #PerfmonStats ps
    WHERE ps.Pass = 2
        AND object_name = @ServiceName + ':Databases'
        AND counter_name = 'Log Growths'
        AND value_delta &gt; 0


    /* SQL Server Internal Maintenance - Log File Shrinking - CheckID 14 */
    INSERT INTO #BlitzFirstResults (CheckID, Priority, FindingsGroup, Finding, URL, Details, HowToStopIt)
    SELECT 14 AS CheckID,
        1 AS Priority,
        'SQL Server Internal Maintenance' AS FindingGroup,
        'Log File Shrinking' AS Finding,
        'http://www.BrentOzar.com/askbrent/file-shrinking/' AS URL,
        'Number of shrinks during the sample: ' + CAST(ps.value_delta AS NVARCHAR(20)) + @LineFeed
            + 'Determined by sampling Perfmon counter ' + ps.object_name + ' - ' + ps.counter_name + @LineFeed AS Details,
        'Pre-grow data and log files during maintenance windows so that they do not grow during production loads. See the URL for more details.' AS HowToStopIt
    FROM #PerfmonStats ps
    WHERE ps.Pass = 2
        AND object_name = @ServiceName + ':Databases'
        AND counter_name = 'Log Shrinks'
        AND value_delta &gt; 0

    /* Query Problems - Compilations/Sec High - CheckID 15 */
    INSERT INTO #BlitzFirstResults (CheckID, Priority, FindingsGroup, Finding, URL, Details, HowToStopIt)
    SELECT 15 AS CheckID,
        50 AS Priority,
        'Query Problems' AS FindingGroup,
        'Compilations/Sec High' AS Finding,
        'http://www.BrentOzar.com/askbrent/compilations/' AS URL,
        'Number of batch requests during the sample: ' + CAST(ps.value_delta AS NVARCHAR(20)) + @LineFeed
            + 'Number of compilations during the sample: ' + CAST(psComp.value_delta AS NVARCHAR(20)) + @LineFeed
            + 'For OLTP environments, Microsoft recommends that 90% of batch requests should hit the plan cache, and not be compiled from scratch. We are exceeding that threshold.' + @LineFeed AS Details,
        'Find out why plans are not being reused, and consider enabling Forced Parameterization. See the URL for more details.' AS HowToStopIt
    FROM #PerfmonStats ps
        INNER JOIN #PerfmonStats psComp ON psComp.Pass = 2 AND psComp.object_name = @ServiceName + ':SQL Statistics' AND psComp.counter_name = 'SQL Compilations/sec' AND psComp.value_delta &gt; 0
    WHERE ps.Pass = 2
        AND ps.object_name = @ServiceName + ':SQL Statistics'
        AND ps.counter_name = 'Batch Requests/sec'
        AND ps.value_delta &gt; (1000 * @Seconds) /* Ignore servers sitting idle */
        AND (psComp.value_delta * 10) &gt; ps.value_delta /* Compilations are more than 10% of batch requests per second */

    /* Query Problems - Re-Compilations/Sec High - CheckID 16 */
    INSERT INTO #BlitzFirstResults (CheckID, Priority, FindingsGroup, Finding, URL, Details, HowToStopIt)
    SELECT 16 AS CheckID,
        50 AS Priority,
        'Query Problems' AS FindingGroup,
        'Re-Compilations/Sec High' AS Finding,
        'http://www.BrentOzar.com/askbrent/recompilations/' AS URL,
        'Number of batch requests during the sample: ' + CAST(ps.value_delta AS NVARCHAR(20)) + @LineFeed
            + 'Number of recompilations during the sample: ' + CAST(psComp.value_delta AS NVARCHAR(20)) + @LineFeed
            + 'More than 10% of our queries are being recompiled. This is typically due to statistics changing on objects.' + @LineFeed AS Details,
        'Find out which objects are changing so quickly that they hit the stats update threshold. See the URL for more details.' AS HowToStopIt
    FROM #PerfmonStats ps
        INNER JOIN #PerfmonStats psComp ON psComp.Pass = 2 AND psComp.object_name = @ServiceName + ':SQL Statistics' AND psComp.counter_name = 'SQL Re-Compilations/sec' AND psComp.value_delta &gt; 0
    WHERE ps.Pass = 2
        AND ps.object_name = @ServiceName + ':SQL Statistics'
        AND ps.counter_name = 'Batch Requests/sec'
        AND ps.value_delta &gt; (1000 * @Seconds) /* Ignore servers sitting idle */
        AND (psComp.value_delta * 10) &gt; ps.value_delta /* Recompilations are more than 10% of batch requests per second */

    /* Server Info - Batch Requests per Sec - CheckID 19 */
    INSERT INTO #BlitzFirstResults (CheckID, Priority, FindingsGroup, Finding, URL, Details, DetailsInt)
    SELECT 19 AS CheckID,
        250 AS Priority,
        'Server Info' AS FindingGroup,
        'Batch Requests per Sec' AS Finding,
        'http://www.BrentOzar.com/go/measure' AS URL,
        CAST(ps.value_delta / (DATEDIFF(ss, ps1.SampleTime, ps.SampleTime)) AS NVARCHAR(20)) AS Details,
        ps.value_delta / (DATEDIFF(ss, ps1.SampleTime, ps.SampleTime)) AS DetailsInt
    FROM #PerfmonStats ps
        INNER JOIN #PerfmonStats ps1 ON ps.object_name = ps1.object_name AND ps.counter_name = ps1.counter_name AND ps1.Pass = 1
    WHERE ps.Pass = 2
        AND ps.object_name = @ServiceName + ':SQL Statistics'
        AND ps.counter_name = 'Batch Requests/sec';


        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':SQL Statistics','SQL Compilations/sec', NULL)
        INSERT INTO #PerfmonCounters ([object_name],[counter_name],[instance_name]) VALUES (@ServiceName + ':SQL Statistics','SQL Re-Compilations/sec', NULL)

    /* Server Info - SQL Compilations/sec - CheckID 25 */
    IF @ExpertMode = 1
    INSERT INTO #BlitzFirstResults (CheckID, Priority, FindingsGroup, Finding, URL, Details, DetailsInt)
    SELECT 25 AS CheckID,
        250 AS Priority,
        'Server Info' AS FindingGroup,
        'SQL Compilations per Sec' AS Finding,
        'http://www.BrentOzar.com/go/measure' AS URL,
        CAST(ps.value_delta / (DATEDIFF(ss, ps1.SampleTime, ps.SampleTime)) AS NVARCHAR(20)) AS Details,
        ps.value_delta / (DATEDIFF(ss, ps1.SampleTime, ps.SampleTime)) AS DetailsInt
    FROM #PerfmonStats ps
        INNER JOIN #PerfmonStats ps1 ON ps.object_name = ps1.object_name AND ps.counter_name = ps1.counter_name AND ps1.Pass = 1
    WHERE ps.Pass = 2
        AND ps.object_name = @ServiceName + ':SQL Statistics'
        AND ps.counter_name = 'SQL Compilations/sec';

    /* Server Info - SQL Re-Compilations/sec - CheckID 26 */
    IF @ExpertMode = 1
    INSERT INTO #BlitzFirstResults (CheckID, Priority, FindingsGroup, Finding, URL, Details, DetailsInt)
    SELECT 26 AS CheckID,
        250 AS Priority,
        'Server Info' AS FindingGroup,
        'SQL Re-Compilations per Sec' AS Finding,
        'http://www.BrentOzar.com/go/measure' AS URL,
        CAST(ps.value_delta / (DATEDIFF(ss, ps1.SampleTime, ps.SampleTime)) AS NVARCHAR(20)) AS Details,
        ps.value_delta / (DATEDIFF(ss, ps1.SampleTime, ps.SampleTime)) AS DetailsInt
    FROM #PerfmonStats ps
        INNER JOIN #PerfmonStats ps1 ON ps.object_name = ps1.object_name AND ps.counter_name = ps1.counter_name AND ps1.Pass = 1
    WHERE ps.Pass = 2
        AND ps.object_name = @ServiceName + ':SQL Statistics'
        AND ps.counter_name = 'SQL Re-Compilations/sec';

    /* Server Info - Wait Time per Core per Sec - CheckID 20 */
    IF @Seconds &gt; 0
    BEGIN
        WITH waits1(SampleTime, waits_ms) AS (SELECT SampleTime, SUM(ws1.wait_time_ms) FROM #WaitStats ws1 WHERE ws1.Pass = 1 GROUP BY SampleTime),
        waits2(SampleTime, waits_ms) AS (SELECT SampleTime, SUM(ws2.wait_time_ms) FROM #WaitStats ws2 WHERE ws2.Pass = 2 GROUP BY SampleTime),
        cores(cpu_count) AS (SELECT SUM(1) FROM sys.dm_os_schedulers WHERE status = 'VISIBLE ONLINE' AND is_online = 1)
        INSERT INTO #BlitzFirstResults (CheckID, Priority, FindingsGroup, Finding, URL, Details, DetailsInt)
        SELECT 19 AS CheckID,
            250 AS Priority,
            'Server Info' AS FindingGroup,
            'Wait Time per Core per Sec' AS Finding,
            'http://www.BrentOzar.com/go/measure' AS URL,
            CAST((waits2.waits_ms - waits1.waits_ms) / 1000 / i.cpu_count / DATEDIFF(ss, waits1.SampleTime, waits2.SampleTime) AS NVARCHAR(20)) AS Details,
            (waits2.waits_ms - waits1.waits_ms) / 1000 / i.cpu_count / DATEDIFF(ss, waits1.SampleTime, waits2.SampleTime) AS DetailsInt
        FROM cores i
          CROSS JOIN waits1
          CROSS JOIN waits2;
    END

    /* Server Performance - High CPU Utilization CheckID 24 */
    IF @Seconds &gt;= 30
        BEGIN
        /* If we're waiting 30+ seconds, run this check at the end.
           We get this data from the ring buffers, and it's only updated once per minute, so might
           as well get it now - whereas if we're checking 30+ seconds, it might get updated by the
           end of our sp_BlitzFirst session. */
        INSERT INTO #BlitzFirstResults (CheckID, Priority, FindingsGroup, Finding, Details, DetailsInt, URL)
        SELECT 24, 50, 'Server Performance', 'High CPU Utilization', CAST(100 - SystemIdle AS NVARCHAR(20)) + N'%. Ring buffer details: ' + CAST(record AS NVARCHAR(4000)), 100 - SystemIdle, 'http://www.BrentOzar.com/go/cpu'
            FROM (
                SELECT record,
                    record.value('(./Record/SchedulerMonitorEvent/SystemHealth/SystemIdle)[1]', 'int') AS SystemIdle
                FROM (
                    SELECT TOP 1 CONVERT(XML, record) AS record
                    FROM sys.dm_os_ring_buffers
                    WHERE ring_buffer_type = N'RING_BUFFER_SCHEDULER_MONITOR'
                    AND record LIKE '%&lt;SystemHealth&gt;%'
                    ORDER BY timestamp DESC) AS rb
            ) AS y
            WHERE 100 - SystemIdle &gt;= 50

        INSERT INTO #BlitzFirstResults (CheckID, Priority, FindingsGroup, Finding, Details, DetailsInt, URL)
        SELECT 23, 250, 'Server Info', 'CPU Utilization', CAST(100 - SystemIdle AS NVARCHAR(20)) + N'%. Ring buffer details: ' + CAST(record AS NVARCHAR(4000)), 100 - SystemIdle, 'http://www.BrentOzar.com/go/cpu'
            FROM (
                SELECT record,
                    record.value('(./Record/SchedulerMonitorEvent/SystemHealth/SystemIdle)[1]', 'int') AS SystemIdle
                FROM (
                    SELECT TOP 1 CONVERT(XML, record) AS record
                    FROM sys.dm_os_ring_buffers
                    WHERE ring_buffer_type = N'RING_BUFFER_SCHEDULER_MONITOR'
                    AND record LIKE '%&lt;SystemHealth&gt;%'
                    ORDER BY timestamp DESC) AS rb
            ) AS y

        END /* IF @Seconds &lt; 30 */

	RAISERROR('Analysis finished, outputting results',10,1) WITH NOWAIT;


    /* If we didn't find anything, apologize. */
    IF NOT EXISTS (SELECT * FROM #BlitzFirstResults WHERE Priority &lt; 250)
    BEGIN

        INSERT  INTO #BlitzFirstResults
                ( CheckID ,
                  Priority ,
                  FindingsGroup ,
                  Finding ,
                  URL ,
                  Details
                )
        VALUES  ( -1 ,
                  1 ,
                  'No Problems Found' ,
                  'From Your Community Volunteers' ,
                  'http://FirstResponderKit.org/' ,
                  'Try running our more in-depth checks with sp_Blitz, or there may not be an unusual SQL Server performance problem. '
                );

    END /*IF NOT EXISTS (SELECT * FROM #BlitzFirstResults) */

        /* Add credits for the nice folks who put so much time into building and maintaining this for free: */
        INSERT  INTO #BlitzFirstResults
                ( CheckID ,
                  Priority ,
                  FindingsGroup ,
                  Finding ,
                  URL ,
                  Details
                )
        VALUES  ( -1 ,
                  255 ,
                  'Thanks!' ,
                  'From Your Community Volunteers' ,
                  'http://FirstResponderKit.org/' ,
                  'To get help or add your own contributions, join us at http://FirstResponderKit.org.'
                );

        INSERT  INTO #BlitzFirstResults
                ( CheckID ,
                  Priority ,
                  FindingsGroup ,
                  Finding ,
                  URL ,
                  Details

                )
        VALUES  ( -1 ,
                  0 ,
                  'sp_BlitzFirst ' + CAST(CONVERT(DATETIMEOFFSET, @VersionDate, 102) AS VARCHAR(100)),
                  'From Your Community Volunteers' ,
                  'http://FirstResponderKit.org/' ,
                  'We hope you found this tool useful.'
                );

                /* Outdated sp_BlitzFirst - sp_BlitzFirst is Over 6 Months Old */
                IF DATEDIFF(MM, @VersionDate, SYSDATETIMEOFFSET()) &gt; 6
                    BEGIN
                        INSERT  INTO #BlitzFirstResults
                                ( CheckID ,
                                    Priority ,
                                    FindingsGroup ,
                                    Finding ,
                                    URL ,
                                    Details
                                )
                                SELECT 27 AS CheckID ,
                                        0 AS Priority ,
                                        'Outdated sp_BlitzFirst' AS FindingsGroup ,
                                        'sp_BlitzFirst is Over 6 Months Old' AS Finding ,
                                        'http://FirstResponderKit.org/' AS URL ,
                                        'Some things get better with age, like fine wine and your T-SQL. However, sp_BlitzFirst is not one of those things - time to go download the current one.' AS Details
                    END



    /* @OutputTableName lets us export the results to a permanent table */
    IF @OutputDatabaseName IS NOT NULL
        AND @OutputSchemaName IS NOT NULL
        AND @OutputTableName IS NOT NULL
        AND @OutputTableName NOT LIKE '#%'
        AND EXISTS ( SELECT *
                     FROM   sys.databases
                     WHERE  QUOTENAME([name]) = @OutputDatabaseName)
    BEGIN
        SET @StringToExecute = 'USE '
            + @OutputDatabaseName
            + '; IF EXISTS(SELECT * FROM '
            + @OutputDatabaseName
            + '.INFORMATION_SCHEMA.SCHEMATA WHERE QUOTENAME(SCHEMA_NAME) = '''
            + @OutputSchemaName
            + ''') AND NOT EXISTS (SELECT * FROM '
            + @OutputDatabaseName
            + '.INFORMATION_SCHEMA.TABLES WHERE QUOTENAME(TABLE_SCHEMA) = '''
            + @OutputSchemaName + ''' AND QUOTENAME(TABLE_NAME) = '''
            + @OutputTableName + ''') CREATE TABLE '
            + @OutputSchemaName + '.'
            + @OutputTableName
            + ' (ID INT IDENTITY(1,1) NOT NULL,
                ServerName NVARCHAR(128),
                CheckDate DATETIMEOFFSET,
                CheckID INT NOT NULL,
                Priority TINYINT NOT NULL,
                FindingsGroup VARCHAR(50) NOT NULL,
                Finding VARCHAR(200) NOT NULL,
                URL VARCHAR(200) NOT NULL,
                Details NVARCHAR(4000) NULL,
                HowToStopIt [XML] NULL,
                QueryPlan [XML] NULL,
                QueryText NVARCHAR(MAX) NULL,
                StartTime DATETIMEOFFSET NULL,
                LoginName NVARCHAR(128) NULL,
                NTUserName NVARCHAR(128) NULL,
                OriginalLoginName NVARCHAR(128) NULL,
                ProgramName NVARCHAR(128) NULL,
                HostName NVARCHAR(128) NULL,
                DatabaseID INT NULL,
                DatabaseName NVARCHAR(128) NULL,
                OpenTransactionCount INT NULL,
                DetailsInt INT NULL,
                CONSTRAINT [PK_' + CAST(NEWID() AS CHAR(36)) + '] PRIMARY KEY CLUSTERED (ID ASC));'

        EXEC(@StringToExecute);
        SET @StringToExecute = N' IF EXISTS(SELECT * FROM '
            + @OutputDatabaseName
            + '.INFORMATION_SCHEMA.SCHEMATA WHERE QUOTENAME(SCHEMA_NAME) = '''
            + @OutputSchemaName + ''') INSERT '
            + @OutputDatabaseName + '.'
            + @OutputSchemaName + '.'
            + @OutputTableName
            + ' (ServerName, CheckDate, CheckID, Priority, FindingsGroup, Finding, URL, Details, HowToStopIt, QueryPlan, QueryText, StartTime, LoginName, NTUserName, OriginalLoginName, ProgramName, HostName, DatabaseID, DatabaseName, OpenTransactionCount, DetailsInt) SELECT '''
            + CAST(SERVERPROPERTY('ServerName') AS NVARCHAR(128))
            + ''', ''' + (CONVERT(NVARCHAR(100), @StartSampleTime, 127)) + ''', CheckID, Priority, FindingsGroup, Finding, URL, Details, HowToStopIt, QueryPlan, QueryText, StartTime, LoginName, NTUserName, OriginalLoginName, ProgramName, HostName, DatabaseID, DatabaseName, OpenTransactionCount, DetailsInt FROM #BlitzFirstResults ORDER BY Priority , FindingsGroup , Finding , Details';
        EXEC(@StringToExecute);
    END
    ELSE IF (SUBSTRING(@OutputTableName, 2, 2) = '##')
    BEGIN
        SET @StringToExecute = N' IF (OBJECT_ID(''tempdb..'
            + @OutputTableName
            + ''') IS NULL) CREATE TABLE '
            + @OutputTableName
            + ' (ID INT IDENTITY(1,1) NOT NULL,
                ServerName NVARCHAR(128),
                CheckDate DATETIMEOFFSET,
                CheckID INT NOT NULL,
                Priority TINYINT NOT NULL,
                FindingsGroup VARCHAR(50) NOT NULL,
                Finding VARCHAR(200) NOT NULL,
                URL VARCHAR(200) NOT NULL,
                Details NVARCHAR(4000) NULL,
                HowToStopIt [XML] NULL,
                QueryPlan [XML] NULL,
                QueryText NVARCHAR(MAX) NULL,
                StartTime DATETIMEOFFSET NULL,
                LoginName NVARCHAR(128) NULL,
                NTUserName NVARCHAR(128) NULL,
                OriginalLoginName NVARCHAR(128) NULL,
                ProgramName NVARCHAR(128) NULL,
                HostName NVARCHAR(128) NULL,
                DatabaseID INT NULL,
                DatabaseName NVARCHAR(128) NULL,
                OpenTransactionCount INT NULL,
                DetailsInt INT NULL,
                CONSTRAINT [PK_' + CAST(NEWID() AS CHAR(36)) + '] PRIMARY KEY CLUSTERED (ID ASC));'
            + ' INSERT '
            + @OutputTableName
            + ' (ServerName, CheckDate, CheckID, Priority, FindingsGroup, Finding, URL, Details, HowToStopIt, QueryPlan, QueryText, StartTime, LoginName, NTUserName, OriginalLoginName, ProgramName, HostName, DatabaseID, DatabaseName, OpenTransactionCount, DetailsInt) SELECT '''
            + CAST(SERVERPROPERTY('ServerName') AS NVARCHAR(128))
            + ''', ''' + CONVERT(NVARCHAR(100), @StartSampleTime, 127) + ''', CheckID, Priority, FindingsGroup, Finding, URL, Details, HowToStopIt, QueryPlan, QueryText, StartTime, LoginName, NTUserName, OriginalLoginName, ProgramName, HostName, DatabaseID, DatabaseName, OpenTransactionCount, DetailsInt FROM #BlitzFirstResults ORDER BY Priority , FindingsGroup , Finding , Details';
        EXEC(@StringToExecute);
    END
    ELSE IF (SUBSTRING(@OutputTableName, 2, 1) = '#')
    BEGIN
        RAISERROR('Due to the nature of Dymamic SQL, only global (i.e. double pound (##)) temp tables are supported for @OutputTableName', 16, 0)
    END

    /* @OutputTableNameFileStats lets us export the results to a permanent table */
    IF @OutputDatabaseName IS NOT NULL
        AND @OutputSchemaName IS NOT NULL
        AND @OutputTableNameFileStats IS NOT NULL
        AND @OutputTableNameFileStats NOT LIKE '#%'
        AND EXISTS ( SELECT *
                     FROM   sys.databases
                     WHERE  QUOTENAME([name]) = @OutputDatabaseName)
    BEGIN
        /* Create the table */
        SET @StringToExecute = 'USE '
            + @OutputDatabaseName
            + '; IF EXISTS(SELECT * FROM '
            + @OutputDatabaseName
            + '.INFORMATION_SCHEMA.SCHEMATA WHERE QUOTENAME(SCHEMA_NAME) = '''
            + @OutputSchemaName
            + ''') AND NOT EXISTS (SELECT * FROM '
            + @OutputDatabaseName
            + '.INFORMATION_SCHEMA.TABLES WHERE QUOTENAME(TABLE_SCHEMA) = '''
            + @OutputSchemaName + ''' AND QUOTENAME(TABLE_NAME) = '''
            + @OutputTableNameFileStats + ''') CREATE TABLE '
            + @OutputSchemaName + '.'
            + @OutputTableNameFileStats
            + ' (ID INT IDENTITY(1,1) NOT NULL,
                ServerName NVARCHAR(128),
                CheckDate DATETIMEOFFSET,
                DatabaseID INT NOT NULL,
                FileID INT NOT NULL,
                DatabaseName NVARCHAR(256) ,
                FileLogicalName NVARCHAR(256) ,
                TypeDesc NVARCHAR(60) ,
                SizeOnDiskMB BIGINT ,
                io_stall_read_ms BIGINT ,
                num_of_reads BIGINT ,
                bytes_read BIGINT ,
                io_stall_write_ms BIGINT ,
                num_of_writes BIGINT ,
                bytes_written BIGINT,
                PhysicalName NVARCHAR(520) ,
                CONSTRAINT [PK_' + CAST(NEWID() AS CHAR(36)) + '] PRIMARY KEY CLUSTERED (ID ASC));'
        EXEC(@StringToExecute);

        /* Create the view */
        SET @ObjectFullName = @OutputDatabaseName + N'.' + @OutputSchemaName + N'.' +  @OutputTableNameFileStats_View;
        IF OBJECT_ID(@ObjectFullName) IS NULL
            BEGIN
            SET @StringToExecute = 'USE '
                + @OutputDatabaseName
                + '; EXEC (''CREATE VIEW '
                + @OutputSchemaName + '.'
                + @OutputTableNameFileStats_View + ' AS ' + @LineFeed
                + 'SELECT f.ServerName, f.CheckDate, f.DatabaseID, f.DatabaseName, f.FileID, f.FileLogicalName, f.TypeDesc, f.PhysicalName, f.SizeOnDiskMB' + @LineFeed
                + ', DATEDIFF(ss, fPrior.CheckDate, f.CheckDate) AS ElapsedSeconds' + @LineFeed
                + ', (f.SizeOnDiskMB - fPrior.SizeOnDiskMB) AS SizeOnDiskMBgrowth' + @LineFeed
                + ', (f.io_stall_read_ms - fPrior.io_stall_read_ms) AS io_stall_read_ms' + @LineFeed
                + ', io_stall_read_ms_average = CASE WHEN (f.num_of_reads - fPrior.num_of_reads) = 0 THEN 0 ELSE (f.io_stall_read_ms - fPrior.io_stall_read_ms) / (f.num_of_reads - fPrior.num_of_reads) END' + @LineFeed
                + ', (f.num_of_reads - fPrior.num_of_reads) AS num_of_reads' + @LineFeed
                + ', (f.bytes_read - fPrior.bytes_read) / 1024.0 / 1024.0 AS megabytes_read' + @LineFeed
                + ', (f.io_stall_write_ms - fPrior.io_stall_write_ms) AS io_stall_write_ms' + @LineFeed
                + ', io_stall_write_ms_average = CASE WHEN (f.num_of_writes - fPrior.num_of_writes) = 0 THEN 0 ELSE (f.io_stall_write_ms - fPrior.io_stall_write_ms) / (f.num_of_writes - fPrior.num_of_writes) END' + @LineFeed
                + ', (f.num_of_writes - fPrior.num_of_writes) AS num_of_writes' + @LineFeed
                + ', (f.bytes_written - fPrior.bytes_written) / 1024.0 / 1024.0 AS megabytes_written' + @LineFeed
                + 'FROM ' + @OutputSchemaName + '.' + @OutputTableNameFileStats + ' f' + @LineFeed
                + 'INNER JOIN ' + @OutputSchemaName + '.' + @OutputTableNameFileStats + ' fPrior ON f.ServerName = fPrior.ServerName AND f.DatabaseID = fPrior.DatabaseID AND f.FileID = fPrior.FileID AND f.CheckDate &gt; fPrior.CheckDate' + @LineFeed
                + 'LEFT OUTER JOIN ' + @OutputSchemaName + '.' + @OutputTableNameFileStats + ' fMiddle ON f.ServerName = fMiddle.ServerName AND f.DatabaseID = fMiddle.DatabaseID AND f.FileID = fMiddle.FileID AND f.CheckDate &gt; fMiddle.CheckDate AND fMiddle.CheckDate &gt; fPrior.CheckDate' + @LineFeed
                + 'WHERE fMiddle.ID IS NULL;'')'
            EXEC(@StringToExecute);
            END

        SET @StringToExecute = N' IF EXISTS(SELECT * FROM '
            + @OutputDatabaseName
            + '.INFORMATION_SCHEMA.SCHEMATA WHERE QUOTENAME(SCHEMA_NAME) = '''
            + @OutputSchemaName + ''') INSERT '
            + @OutputDatabaseName + '.'
            + @OutputSchemaName + '.'
            + @OutputTableNameFileStats
            + ' (ServerName, CheckDate, DatabaseID, FileID, DatabaseName, FileLogicalName, TypeDesc, SizeOnDiskMB, io_stall_read_ms, num_of_reads, bytes_read, io_stall_write_ms, num_of_writes, bytes_written, PhysicalName) SELECT '''
            + CAST(SERVERPROPERTY('ServerName') AS NVARCHAR(128))
            + ''', ''' + CONVERT(NVARCHAR(100), @StartSampleTime, 127) + ''', '
            + 'DatabaseID, FileID, DatabaseName, FileLogicalName, TypeDesc, SizeOnDiskMB, io_stall_read_ms, num_of_reads, bytes_read, io_stall_write_ms, num_of_writes, bytes_written, PhysicalName FROM #FileStats WHERE Pass = 2';
        EXEC(@StringToExecute);
    END
    ELSE IF (SUBSTRING(@OutputTableNameFileStats, 2, 2) = '##')
    BEGIN
        SET @StringToExecute = N' IF (OBJECT_ID(''tempdb..'
            + @OutputTableNameFileStats
            + ''') IS NULL) CREATE TABLE '
            + @OutputTableNameFileStats
            + ' (ID INT IDENTITY(1,1) NOT NULL,
                ServerName NVARCHAR(128),
                CheckDate DATETIMEOFFSET,
                DatabaseID INT NOT NULL,
                FileID INT NOT NULL,
                DatabaseName NVARCHAR(256) ,
                FileLogicalName NVARCHAR(256) ,
                TypeDesc NVARCHAR(60) ,
                SizeOnDiskMB BIGINT ,
                io_stall_read_ms BIGINT ,
                num_of_reads BIGINT ,
                bytes_read BIGINT ,
                io_stall_write_ms BIGINT ,
                num_of_writes BIGINT ,
                bytes_written BIGINT,
                PhysicalName NVARCHAR(520) ,
                DetailsInt INT NULL,
                CONSTRAINT [PK_' + CAST(NEWID() AS CHAR(36)) + '] PRIMARY KEY CLUSTERED (ID ASC));'
            + ' INSERT '
            + @OutputTableNameFileStats
            + ' (ServerName, CheckDate, DatabaseID, FileID, DatabaseName, FileLogicalName, TypeDesc, SizeOnDiskMB, io_stall_read_ms, num_of_reads, bytes_read, io_stall_write_ms, num_of_writes, bytes_written, PhysicalName) SELECT '''
            + CAST(SERVERPROPERTY('ServerName') AS NVARCHAR(128))
            + ''', ''' + CONVERT(NVARCHAR(100), @StartSampleTime, 127) + ''', '
            + 'DatabaseID, FileID, DatabaseName, FileLogicalName, TypeDesc, SizeOnDiskMB, io_stall_read_ms, num_of_reads, bytes_read, io_stall_write_ms, num_of_writes, bytes_written, PhysicalName FROM #FileStats WHERE Pass = 2';
        EXEC(@StringToExecute);
    END
    ELSE IF (SUBSTRING(@OutputTableNameFileStats, 2, 1) = '#')
    BEGIN
        RAISERROR('Due to the nature of Dymamic SQL, only global (i.e. double pound (##)) temp tables are supported for @OutputTableName', 16, 0)
    END


    /* @OutputTableNamePerfmonStats lets us export the results to a permanent table */
    IF @OutputDatabaseName IS NOT NULL
        AND @OutputSchemaName IS NOT NULL
        AND @OutputTableNamePerfmonStats IS NOT NULL
        AND @OutputTableNamePerfmonStats NOT LIKE '#%'
        AND EXISTS ( SELECT *
                     FROM   sys.databases
                     WHERE  QUOTENAME([name]) = @OutputDatabaseName)
    BEGIN
        /* Create the table */
        SET @StringToExecute = 'USE '
            + @OutputDatabaseName
            + '; IF EXISTS(SELECT * FROM '
            + @OutputDatabaseName
            + '.INFORMATION_SCHEMA.SCHEMATA WHERE QUOTENAME(SCHEMA_NAME) = '''
            + @OutputSchemaName
            + ''') AND NOT EXISTS (SELECT * FROM '
            + @OutputDatabaseName
            + '.INFORMATION_SCHEMA.TABLES WHERE QUOTENAME(TABLE_SCHEMA) = '''
            + @OutputSchemaName + ''' AND QUOTENAME(TABLE_NAME) = '''
            + @OutputTableNamePerfmonStats + ''') CREATE TABLE '
            + @OutputSchemaName + '.'
            + @OutputTableNamePerfmonStats
            + ' (ID INT IDENTITY(1,1) NOT NULL,
                ServerName NVARCHAR(128),
                CheckDate DATETIMEOFFSET,
                [object_name] NVARCHAR(128) NOT NULL,
                [counter_name] NVARCHAR(128) NOT NULL,
                [instance_name] NVARCHAR(128) NULL,
                [cntr_value] BIGINT NULL,
                [cntr_type] INT NOT NULL,
                [value_delta] BIGINT NULL,
                [value_per_second] DECIMAL(18,2) NULL,
                CONSTRAINT [PK_' + CAST(NEWID() AS CHAR(36)) + '] PRIMARY KEY CLUSTERED (ID ASC));'
        EXEC(@StringToExecute);

        /* Create the view */
        SET @ObjectFullName = @OutputDatabaseName + N'.' + @OutputSchemaName + N'.' +  @OutputTableNamePerfmonStats_View;
        IF OBJECT_ID(@ObjectFullName) IS NULL
            BEGIN
            SET @StringToExecute = 'USE '
                + @OutputDatabaseName
                + '; EXEC (''CREATE VIEW '
                + @OutputSchemaName + '.'
                + @OutputTableNamePerfmonStats_View + ' AS ' + @LineFeed
                + 'SELECT p.ServerName, p.CheckDate, p.object_name, p.counter_name, p.instance_name' + @LineFeed
                + ', DATEDIFF(ss, pPrior.CheckDate, p.CheckDate) AS ElapsedSeconds' + @LineFeed
                + ', p.cntr_value' + @LineFeed
                + ', p.cntr_type' + @LineFeed
                + ', (p.cntr_value - pPrior.cntr_value) AS cntr_delta' + @LineFeed
                + 'FROM ' + @OutputSchemaName + '.' + @OutputTableNamePerfmonStats + ' p' + @LineFeed
                + 'INNER JOIN ' + @OutputSchemaName + '.' + @OutputTableNamePerfmonStats + ' pPrior ON p.ServerName = pPrior.ServerName AND p.object_name = pPrior.object_name AND p.counter_name = pPrior.counter_name AND p.instance_name = pPrior.instance_name AND p.CheckDate &gt; pPrior.CheckDate' + @LineFeed
                + 'LEFT OUTER JOIN ' + @OutputSchemaName + '.' + @OutputTableNamePerfmonStats + ' pMiddle ON p.ServerName = pMiddle.ServerName AND p.object_name = pMiddle.object_name AND p.counter_name = pMiddle.counter_name AND p.instance_name = pMiddle.instance_name AND p.CheckDate &gt; pMiddle.CheckDate AND pMiddle.CheckDate &gt; pPrior.CheckDate' + @LineFeed
                + 'WHERE pMiddle.ID IS NULL;'')'
            EXEC(@StringToExecute);
            END;

        SET @StringToExecute = N' IF EXISTS(SELECT * FROM '
            + @OutputDatabaseName
            + '.INFORMATION_SCHEMA.SCHEMATA WHERE QUOTENAME(SCHEMA_NAME) = '''
            + @OutputSchemaName + ''') INSERT '
            + @OutputDatabaseName + '.'
            + @OutputSchemaName + '.'
            + @OutputTableNamePerfmonStats
            + ' (ServerName, CheckDate, object_name, counter_name, instance_name, cntr_value, cntr_type, value_delta, value_per_second) SELECT '''
            + CAST(SERVERPROPERTY('ServerName') AS NVARCHAR(128))
            + ''', ''' + CONVERT(NVARCHAR(100), @StartSampleTime, 127) + ''', '
            + 'object_name, counter_name, instance_name, cntr_value, cntr_type, value_delta, value_per_second FROM #PerfmonStats WHERE Pass = 2';
        EXEC(@StringToExecute);

    END
    ELSE IF (SUBSTRING(@OutputTableNamePerfmonStats, 2, 2) = '##')
    BEGIN
        SET @StringToExecute = N' IF (OBJECT_ID(''tempdb..'
            + @OutputTableNamePerfmonStats
            + ''') IS NULL) CREATE TABLE '
            + @OutputTableNamePerfmonStats
            + ' (ID INT IDENTITY(1,1) NOT NULL,
                ServerName NVARCHAR(128),
                CheckDate DATETIMEOFFSET,
                [object_name] NVARCHAR(128) NOT NULL,
                [counter_name] NVARCHAR(128) NOT NULL,
                [instance_name] NVARCHAR(128) NULL,
                [cntr_value] BIGINT NULL,
                [cntr_type] INT NOT NULL,
                [value_delta] BIGINT NULL,
                [value_per_second] DECIMAL(18,2) NULL,
                CONSTRAINT [PK_' + CAST(NEWID() AS CHAR(36)) + '] PRIMARY KEY CLUSTERED (ID ASC));'
            + ' INSERT '
            + @OutputTableNamePerfmonStats
            + ' (ServerName, CheckDate, object_name, counter_name, instance_name, cntr_value, cntr_type, value_delta, value_per_second) SELECT '''
            + CAST(SERVERPROPERTY('ServerName') AS NVARCHAR(128))
            + ''', ''' + CONVERT(NVARCHAR(100), @StartSampleTime, 127) + ''', '
            + 'object_name, counter_name, instance_name, cntr_value, cntr_type, value_delta, value_per_second FROM #PerfmonStats WHERE Pass = 2';
        EXEC(@StringToExecute);
    END
    ELSE IF (SUBSTRING(@OutputTableNamePerfmonStats, 2, 1) = '#')
    BEGIN
        RAISERROR('Due to the nature of Dymamic SQL, only global (i.e. double pound (##)) temp tables are supported for @OutputTableName', 16, 0)
    END


    /* @OutputTableNameWaitStats lets us export the results to a permanent table */
    IF @OutputDatabaseName IS NOT NULL
        AND @OutputSchemaName IS NOT NULL
        AND @OutputTableNameWaitStats IS NOT NULL
        AND @OutputTableNameWaitStats NOT LIKE '#%'
        AND EXISTS ( SELECT *
                     FROM   sys.databases
                     WHERE  QUOTENAME([name]) = @OutputDatabaseName)
    BEGIN
        /* Create the table */
        SET @StringToExecute = 'USE '
            + @OutputDatabaseName
            + '; IF EXISTS(SELECT * FROM '
            + @OutputDatabaseName
            + '.INFORMATION_SCHEMA.SCHEMATA WHERE QUOTENAME(SCHEMA_NAME) = '''
            + @OutputSchemaName
            + ''') AND NOT EXISTS (SELECT * FROM '
            + @OutputDatabaseName
            + '.INFORMATION_SCHEMA.TABLES WHERE QUOTENAME(TABLE_SCHEMA) = '''
            + @OutputSchemaName + ''' AND QUOTENAME(TABLE_NAME) = '''
            + @OutputTableNameWaitStats + ''') CREATE TABLE '
            + @OutputSchemaName + '.'
            + @OutputTableNameWaitStats
            + ' (ID INT IDENTITY(1,1) NOT NULL,
                ServerName NVARCHAR(128),
                CheckDate DATETIMEOFFSET,
                wait_type NVARCHAR(60),
                wait_time_ms BIGINT,
                signal_wait_time_ms BIGINT,
                waiting_tasks_count BIGINT ,
                CONSTRAINT [PK_' + CAST(NEWID() AS CHAR(36)) + '] PRIMARY KEY CLUSTERED (ID ASC));'
        EXEC(@StringToExecute);

        /* Create the view */
        SET @ObjectFullName = @OutputDatabaseName + N'.' + @OutputSchemaName + N'.' +  @OutputTableNameWaitStats_View;
        IF OBJECT_ID(@ObjectFullName) IS NULL
            BEGIN
            SET @StringToExecute = 'USE '
                + @OutputDatabaseName
                + '; EXEC (''CREATE VIEW '
                + @OutputSchemaName + '.'
                + @OutputTableNameWaitStats_View + ' AS ' + @LineFeed
                + 'SELECT w.ServerName, w.CheckDate, w.wait_type' + @LineFeed
                + ', DATEDIFF(ss, wPrior.CheckDate, w.CheckDate) AS ElapsedSeconds' + @LineFeed
                + ', (w.wait_time_ms - wPrior.wait_time_ms) AS wait_time_ms_delta' + @LineFeed
                + ', (w.signal_wait_time_ms - wPrior.signal_wait_time_ms) AS signal_wait_time_ms_delta' + @LineFeed
                + ', (w.waiting_tasks_count - wPrior.waiting_tasks_count) AS waiting_tasks_count_delta' + @LineFeed
                + 'FROM ' + @OutputSchemaName + '.' + @OutputTableNameWaitStats + ' w' + @LineFeed
                + 'INNER JOIN ' + @OutputSchemaName + '.' + @OutputTableNameWaitStats + ' wPrior ON w.ServerName = wPrior.ServerName AND w.wait_type = wPrior.wait_type AND w.CheckDate &gt; wPrior.CheckDate' + @LineFeed
                + 'LEFT OUTER JOIN ' + @OutputSchemaName + '.' + @OutputTableNameWaitStats + ' wMiddle ON w.ServerName = wMiddle.ServerName AND w.wait_type = wMiddle.wait_type AND w.CheckDate &gt; wMiddle.CheckDate AND wMiddle.CheckDate &gt; wPrior.CheckDate' + @LineFeed
                + 'WHERE wMiddle.ID IS NULL;'')'
            EXEC(@StringToExecute);
            END


        SET @StringToExecute = N' IF EXISTS(SELECT * FROM '
            + @OutputDatabaseName
            + '.INFORMATION_SCHEMA.SCHEMATA WHERE QUOTENAME(SCHEMA_NAME) = '''
            + @OutputSchemaName + ''') INSERT '
            + @OutputDatabaseName + '.'
            + @OutputSchemaName + '.'
            + @OutputTableNameWaitStats
            + ' (ServerName, CheckDate, wait_type, wait_time_ms, signal_wait_time_ms, waiting_tasks_count) SELECT '''
            + CAST(SERVERPROPERTY('ServerName') AS NVARCHAR(128))
            + ''', ''' + CONVERT(NVARCHAR(100), @StartSampleTime, 127) + ''', '
            + 'wait_type, wait_time_ms, signal_wait_time_ms, waiting_tasks_count FROM #WaitStats WHERE Pass = 2 AND wait_time_ms &gt; 0 AND waiting_tasks_count &gt; 0';
        EXEC(@StringToExecute);
    END
    ELSE IF (SUBSTRING(@OutputTableNameWaitStats, 2, 2) = '##')
    BEGIN
        SET @StringToExecute = N' IF (OBJECT_ID(''tempdb..'
            + @OutputTableNameWaitStats
            + ''') IS NULL) CREATE TABLE '
            + @OutputTableNameWaitStats
            + ' (ID INT IDENTITY(1,1) NOT NULL,
                ServerName NVARCHAR(128),
                CheckDate DATETIMEOFFSET,
                wait_type NVARCHAR(60),
                wait_time_ms BIGINT,
                signal_wait_time_ms BIGINT,
                waiting_tasks_count BIGINT ,
                CONSTRAINT [PK_' + CAST(NEWID() AS CHAR(36)) + '] PRIMARY KEY CLUSTERED (ID ASC));'
            + ' INSERT '
            + @OutputTableNameWaitStats
            + ' (ServerName, CheckDate, wait_type, wait_time_ms, signal_wait_time_ms, waiting_tasks_count) SELECT '''
            + CAST(SERVERPROPERTY('ServerName') AS NVARCHAR(128))
            + ''', ''' + CONVERT(NVARCHAR(100), @StartSampleTime, 127) + ''', '
            + 'wait_type, wait_time_ms, signal_wait_time_ms, waiting_tasks_count FROM #WaitStats WHERE Pass = 2 AND wait_time_ms &gt; 0 AND waiting_tasks_count &gt; 0';
        EXEC(@StringToExecute);
    END
    ELSE IF (SUBSTRING(@OutputTableNameWaitStats, 2, 1) = '#')
    BEGIN
        RAISERROR('Due to the nature of Dymamic SQL, only global (i.e. double pound (##)) temp tables are supported for @OutputTableName', 16, 0)
    END




    DECLARE @separator AS VARCHAR(1);
    IF @OutputType = 'RSV'
        SET @separator = CHAR(31);
    ELSE
        SET @separator = ',';

    IF @OutputType = 'COUNT' AND @SinceStartup = 0
    BEGIN
        SELECT  COUNT(*) AS Warnings
        FROM    #BlitzFirstResults
    END
    ELSE
        IF @OutputType = 'Opserver1' AND @SinceStartup = 0
        BEGIN

            SELECT  r.[Priority] ,
                    r.[FindingsGroup] ,
                    r.[Finding] ,
                    r.[URL] ,
                    r.[Details],
                    r.[HowToStopIt] ,
                    r.[CheckID] ,
                    r.[StartTime],
                    r.[LoginName],
                    r.[NTUserName],
                    r.[OriginalLoginName],
                    r.[ProgramName],
                    r.[HostName],
                    r.[DatabaseID],
                    r.[DatabaseName],
                    r.[OpenTransactionCount],
                    r.[QueryPlan],
                    r.[QueryText],
                    qsNow.plan_handle AS PlanHandle,
                    qsNow.sql_handle AS SqlHandle,
                    qsNow.statement_start_offset AS StatementStartOffset,
                    qsNow.statement_end_offset AS StatementEndOffset,
                    [Executions] = qsNow.execution_count - (COALESCE(qsFirst.execution_count, 0)),
                    [ExecutionsPercent] = CAST(100.0 * (qsNow.execution_count - (COALESCE(qsFirst.execution_count, 0))) / (qsTotal.execution_count - qsTotalFirst.execution_count) AS DECIMAL(6,2)),
                    [Duration] = qsNow.total_elapsed_time - (COALESCE(qsFirst.total_elapsed_time, 0)),
                    [DurationPercent] = CAST(100.0 * (qsNow.total_elapsed_time - (COALESCE(qsFirst.total_elapsed_time, 0))) / (qsTotal.total_elapsed_time - qsTotalFirst.total_elapsed_time) AS DECIMAL(6,2)),
                    [CPU] = qsNow.total_worker_time - (COALESCE(qsFirst.total_worker_time, 0)),
                    [CPUPercent] = CAST(100.0 * (qsNow.total_worker_time - (COALESCE(qsFirst.total_worker_time, 0))) / (qsTotal.total_worker_time - qsTotalFirst.total_worker_time) AS DECIMAL(6,2)),
                    [Reads] = qsNow.total_logical_reads - (COALESCE(qsFirst.total_logical_reads, 0)),
                    [ReadsPercent] = CAST(100.0 * (qsNow.total_logical_reads - (COALESCE(qsFirst.total_logical_reads, 0))) / (qsTotal.total_logical_reads - qsTotalFirst.total_logical_reads) AS DECIMAL(6,2)),
                    [PlanCreationTime] = CONVERT(NVARCHAR(100), qsNow.creation_time ,121),
                    [TotalExecutions] = qsNow.execution_count,
                    [TotalExecutionsPercent] = CAST(100.0 * qsNow.execution_count / qsTotal.execution_count AS DECIMAL(6,2)),
                    [TotalDuration] = qsNow.total_elapsed_time,
                    [TotalDurationPercent] = CAST(100.0 * qsNow.total_elapsed_time / qsTotal.total_elapsed_time AS DECIMAL(6,2)),
                    [TotalCPU] = qsNow.total_worker_time,
                    [TotalCPUPercent] = CAST(100.0 * qsNow.total_worker_time / qsTotal.total_worker_time AS DECIMAL(6,2)),
                    [TotalReads] = qsNow.total_logical_reads,
                    [TotalReadsPercent] = CAST(100.0 * qsNow.total_logical_reads / qsTotal.total_logical_reads AS DECIMAL(6,2)),
                    r.[DetailsInt]
            FROM    #BlitzFirstResults r
                LEFT OUTER JOIN #QueryStats qsTotal ON qsTotal.Pass = 0
                LEFT OUTER JOIN #QueryStats qsTotalFirst ON qsTotalFirst.Pass = -1
                LEFT OUTER JOIN #QueryStats qsNow ON r.QueryStatsNowID = qsNow.ID
                LEFT OUTER JOIN #QueryStats qsFirst ON r.QueryStatsFirstID = qsFirst.ID
            ORDER BY r.Priority ,
                    r.FindingsGroup ,
                    CASE
                        WHEN r.CheckID = 6 THEN DetailsInt
                        ELSE 0
                    END DESC,
                    r.Finding,
                    r.ID;
        END
        ELSE IF @OutputType IN ( 'CSV', 'RSV' ) AND @SinceStartup = 0
        BEGIN

            SELECT  Result = CAST([Priority] AS NVARCHAR(100))
                    + @separator + CAST(CheckID AS NVARCHAR(100))
                    + @separator + COALESCE([FindingsGroup],
                                            '(N/A)') + @separator
                    + COALESCE([Finding], '(N/A)') + @separator
                    + COALESCE(DatabaseName, '(N/A)') + @separator
                    + COALESCE([URL], '(N/A)') + @separator
                    + COALESCE([Details], '(N/A)')
            FROM    #BlitzFirstResults
            ORDER BY Priority ,
                    FindingsGroup ,
                    CASE
                        WHEN CheckID = 6 THEN DetailsInt
                        ELSE 0
                    END DESC,
                    Finding,
                    Details;
        END
        ELSE IF @ExpertMode = 0 AND @OutputXMLasNVARCHAR = 0 AND @SinceStartup = 0
        BEGIN
            SELECT  [Priority] ,
                    [FindingsGroup] ,
                    [Finding] ,
                    [URL] ,
                    CAST(@StockDetailsHeader + [Details] + @StockDetailsFooter AS XML) AS Details,
                    CAST(@StockWarningHeader + HowToStopIt + @StockWarningFooter AS XML) AS HowToStopIt,
                    [QueryText],
                    [QueryPlan]
            FROM    #BlitzFirstResults
            WHERE (@Seconds &gt; 0 OR (Priority IN (0, 250, 251, 255))) /* For @Seconds = 0, filter out broken checks for now */
            ORDER BY Priority ,
                    FindingsGroup ,
                    CASE
                        WHEN CheckID = 6 THEN DetailsInt
                        ELSE 0
                    END DESC,
                    Finding,
                    ID;
        END
        ELSE IF @ExpertMode = 0 AND @OutputXMLasNVARCHAR = 1 AND @SinceStartup = 0
        BEGIN
            SELECT  [Priority] ,
                    [FindingsGroup] ,
                    [Finding] ,
                    [URL] ,
                    CAST(@StockDetailsHeader + [Details] + @StockDetailsFooter AS NVARCHAR(MAX)) AS Details,
                    CAST([HowToStopIt] AS NVARCHAR(MAX)) AS HowToStopIt,
                    CAST([QueryText] AS NVARCHAR(MAX)) AS QueryText,
                    CAST([QueryPlan] AS NVARCHAR(MAX)) AS QueryPlan
            FROM    #BlitzFirstResults
            WHERE (@Seconds &gt; 0 OR (Priority IN (0, 250, 251, 255))) /* For @Seconds = 0, filter out broken checks for now */
            ORDER BY Priority ,
                    FindingsGroup ,
                    CASE
                        WHEN CheckID = 6 THEN DetailsInt
                        ELSE 0
                    END DESC,
                    Finding,
                    ID;
        END
        ELSE IF @ExpertMode = 1
        BEGIN
            IF @SinceStartup = 0
                SELECT  r.[Priority] ,
                        r.[FindingsGroup] ,
                        r.[Finding] ,
                        r.[URL] ,
                        CAST(@StockDetailsHeader + r.[Details] + @StockDetailsFooter AS XML) AS Details,
                        CAST(@StockWarningHeader + r.HowToStopIt + @StockWarningFooter AS XML) AS HowToStopIt,
                        r.[CheckID] ,
                        r.[StartTime],
                        r.[LoginName],
                        r.[NTUserName],
                        r.[OriginalLoginName],
                        r.[ProgramName],
                        r.[HostName],
                        r.[DatabaseID],
                        r.[DatabaseName],
                        r.[OpenTransactionCount],
                        r.[QueryPlan],
                        r.[QueryText],
                        qsNow.plan_handle AS PlanHandle,
                        qsNow.sql_handle AS SqlHandle,
                        qsNow.statement_start_offset AS StatementStartOffset,
                        qsNow.statement_end_offset AS StatementEndOffset,
                        [Executions] = qsNow.execution_count - (COALESCE(qsFirst.execution_count, 0)),
                        [ExecutionsPercent] = CAST(100.0 * (qsNow.execution_count - (COALESCE(qsFirst.execution_count, 0))) / (qsTotal.execution_count - qsTotalFirst.execution_count) AS DECIMAL(6,2)),
                        [Duration] = qsNow.total_elapsed_time - (COALESCE(qsFirst.total_elapsed_time, 0)),
                        [DurationPercent] = CAST(100.0 * (qsNow.total_elapsed_time - (COALESCE(qsFirst.total_elapsed_time, 0))) / (qsTotal.total_elapsed_time - qsTotalFirst.total_elapsed_time) AS DECIMAL(6,2)),
                        [CPU] = qsNow.total_worker_time - (COALESCE(qsFirst.total_worker_time, 0)),
                        [CPUPercent] = CAST(100.0 * (qsNow.total_worker_time - (COALESCE(qsFirst.total_worker_time, 0))) / (qsTotal.total_worker_time - qsTotalFirst.total_worker_time) AS DECIMAL(6,2)),
                        [Reads] = qsNow.total_logical_reads - (COALESCE(qsFirst.total_logical_reads, 0)),
                        [ReadsPercent] = CAST(100.0 * (qsNow.total_logical_reads - (COALESCE(qsFirst.total_logical_reads, 0))) / (qsTotal.total_logical_reads - qsTotalFirst.total_logical_reads) AS DECIMAL(6,2)),
                        [PlanCreationTime] = CONVERT(NVARCHAR(100), qsNow.creation_time ,121),
                        [TotalExecutions] = qsNow.execution_count,
                        [TotalExecutionsPercent] = CAST(100.0 * qsNow.execution_count / qsTotal.execution_count AS DECIMAL(6,2)),
                        [TotalDuration] = qsNow.total_elapsed_time,
                        [TotalDurationPercent] = CAST(100.0 * qsNow.total_elapsed_time / qsTotal.total_elapsed_time AS DECIMAL(6,2)),
                        [TotalCPU] = qsNow.total_worker_time,
                        [TotalCPUPercent] = CAST(100.0 * qsNow.total_worker_time / qsTotal.total_worker_time AS DECIMAL(6,2)),
                        [TotalReads] = qsNow.total_logical_reads,
                        [TotalReadsPercent] = CAST(100.0 * qsNow.total_logical_reads / qsTotal.total_logical_reads AS DECIMAL(6,2)),
                        r.[DetailsInt]
                FROM    #BlitzFirstResults r
                    LEFT OUTER JOIN #QueryStats qsTotal ON qsTotal.Pass = 0
                    LEFT OUTER JOIN #QueryStats qsTotalFirst ON qsTotalFirst.Pass = -1
                    LEFT OUTER JOIN #QueryStats qsNow ON r.QueryStatsNowID = qsNow.ID
                    LEFT OUTER JOIN #QueryStats qsFirst ON r.QueryStatsFirstID = qsFirst.ID
                WHERE (@Seconds &gt; 0 OR (Priority IN (0, 250, 251, 255))) /* For @Seconds = 0, filter out broken checks for now */
                ORDER BY r.Priority ,
                        r.FindingsGroup ,
                        CASE
                            WHEN r.CheckID = 6 THEN DetailsInt
                            ELSE 0
                        END DESC,
                        r.Finding,
                        r.ID;

            -------------------------
            --What happened: #WaitStats
            -------------------------
            IF @Seconds = 0
                BEGIN
                /* Measure waits in hours */
                ;WITH max_batch AS (
                    SELECT MAX(SampleTime) AS SampleTime
                    FROM #WaitStats
                )
                SELECT
                    'WAIT STATS' AS Pattern,
                    b.SampleTime AS [Sample Ended],
                    CAST(DATEDIFF(mi,wd1.SampleTime, wd2.SampleTime) / 60.0 AS DECIMAL(18,1)) AS [Hours Sample],
                    wd1.wait_type,
                    CAST(c.[Wait Time (Seconds)] / 60.0 / 60 AS DECIMAL(18,1)) AS [Wait Time (Hours)],
                    CAST((wd2.wait_time_ms - wd1.wait_time_ms) / 1000.0 / 60 / 60 / cores.cpu_count / DATEDIFF(ss, wd1.SampleTime, wd2.SampleTime) AS DECIMAL(18,1)) AS [Per Core Per Hour],
                    CAST(c.[Signal Wait Time (Seconds)] / 60.0 / 60 AS DECIMAL(18,1)) AS [Signal Wait Time (Hours)],
                    CASE WHEN c.[Wait Time (Seconds)] &gt; 0
                     THEN CAST(100.*(c.[Signal Wait Time (Seconds)]/c.[Wait Time (Seconds)]) AS NUMERIC(4,1))
                    ELSE 0 END AS [Percent Signal Waits],
                    (wd2.waiting_tasks_count - wd1.waiting_tasks_count) AS [Number of Waits],
                    CASE WHEN (wd2.waiting_tasks_count - wd1.waiting_tasks_count) &gt; 0
                    THEN
                        CAST((wd2.wait_time_ms-wd1.wait_time_ms)/
                            (1.0*(wd2.waiting_tasks_count - wd1.waiting_tasks_count)) AS NUMERIC(12,1))
                    ELSE 0 END AS [Avg ms Per Wait],
                    N'http://www.brentozar.com/sql/wait-stats/#' + wd1.wait_type AS URL
                FROM  max_batch b
                JOIN #WaitStats wd2 ON
                    wd2.SampleTime =b.SampleTime
                JOIN #WaitStats wd1 ON
                    wd1.wait_type=wd2.wait_type AND
                    wd2.SampleTime &gt; wd1.SampleTime
                CROSS APPLY (SELECT SUM(1) AS cpu_count FROM sys.dm_os_schedulers WHERE status = 'VISIBLE ONLINE' AND is_online = 1) AS cores
                CROSS APPLY (SELECT
                    CAST((wd2.wait_time_ms-wd1.wait_time_ms)/1000. AS NUMERIC(12,1)) AS [Wait Time (Seconds)],
                    CAST((wd2.signal_wait_time_ms - wd1.signal_wait_time_ms)/1000. AS NUMERIC(12,1)) AS [Signal Wait Time (Seconds)]) AS c
                WHERE (wd2.waiting_tasks_count - wd1.waiting_tasks_count) &gt; 0
                    AND wd2.wait_time_ms-wd1.wait_time_ms &gt; 0
                ORDER BY [Wait Time (Seconds)] DESC;
                END
            ELSE
                BEGIN
                /* Measure waits in seconds */
                ;WITH max_batch AS (
                    SELECT MAX(SampleTime) AS SampleTime
                    FROM #WaitStats
                )
                SELECT
                    'WAIT STATS' AS Pattern,
                    b.SampleTime AS [Sample Ended],
                    DATEDIFF(ss,wd1.SampleTime, wd2.SampleTime) AS [Seconds Sample],
                    wd1.wait_type,
                    c.[Wait Time (Seconds)],
                    CAST((wd2.wait_time_ms - wd1.wait_time_ms) / 1000.0 / cores.cpu_count / DATEDIFF(ss, wd1.SampleTime, wd2.SampleTime) AS DECIMAL(18,1)) AS [Per Core Per Second],
                    c.[Signal Wait Time (Seconds)],
                    CASE WHEN c.[Wait Time (Seconds)] &gt; 0
                     THEN CAST(100.*(c.[Signal Wait Time (Seconds)]/c.[Wait Time (Seconds)]) AS NUMERIC(4,1))
                    ELSE 0 END AS [Percent Signal Waits],
                    (wd2.waiting_tasks_count - wd1.waiting_tasks_count) AS [Number of Waits],
                    CASE WHEN (wd2.waiting_tasks_count - wd1.waiting_tasks_count) &gt; 0
                    THEN
                        CAST((wd2.wait_time_ms-wd1.wait_time_ms)/
                            (1.0*(wd2.waiting_tasks_count - wd1.waiting_tasks_count)) AS NUMERIC(12,1))
                    ELSE 0 END AS [Avg ms Per Wait],
                    N'http://www.brentozar.com/sql/wait-stats/#' + wd1.wait_type AS URL
                FROM  max_batch b
                JOIN #WaitStats wd2 ON
                    wd2.SampleTime =b.SampleTime
                JOIN #WaitStats wd1 ON
                    wd1.wait_type=wd2.wait_type AND
                    wd2.SampleTime &gt; wd1.SampleTime
                CROSS APPLY (SELECT SUM(1) AS cpu_count FROM sys.dm_os_schedulers WHERE status = 'VISIBLE ONLINE' AND is_online = 1) AS cores
                CROSS APPLY (SELECT
                    CAST((wd2.wait_time_ms-wd1.wait_time_ms)/1000. AS NUMERIC(12,1)) AS [Wait Time (Seconds)],
                    CAST((wd2.signal_wait_time_ms - wd1.signal_wait_time_ms)/1000. AS NUMERIC(12,1)) AS [Signal Wait Time (Seconds)]) AS c
                WHERE (wd2.waiting_tasks_count - wd1.waiting_tasks_count) &gt; 0
                    AND wd2.wait_time_ms-wd1.wait_time_ms &gt; 0
                ORDER BY [Wait Time (Seconds)] DESC;
                END;

            -------------------------
            --What happened: #FileStats
            -------------------------
            WITH readstats AS (
                SELECT 'PHYSICAL READS' AS Pattern,
                ROW_NUMBER() OVER (ORDER BY wd2.avg_stall_read_ms DESC) AS StallRank,
                wd2.SampleTime AS [Sample Time],
                DATEDIFF(ss,wd1.SampleTime, wd2.SampleTime) AS [Sample (seconds)],
                wd1.DatabaseName ,
                wd1.FileLogicalName AS [File Name],
                UPPER(SUBSTRING(wd1.PhysicalName, 1, 2)) AS [Drive] ,
                wd1.SizeOnDiskMB ,
                ( wd2.num_of_reads - wd1.num_of_reads ) AS [# Reads/Writes],
                CASE WHEN wd2.num_of_reads - wd1.num_of_reads &gt; 0
                  THEN CAST(( wd2.bytes_read - wd1.bytes_read)/1024./1024. AS NUMERIC(21,1))
                  ELSE 0
                END AS [MB Read/Written],
                wd2.avg_stall_read_ms AS [Avg Stall (ms)],
                wd1.PhysicalName AS [file physical name]
            FROM #FileStats wd2
                JOIN #FileStats wd1 ON wd2.SampleTime &gt; wd1.SampleTime
                  AND wd1.DatabaseID = wd2.DatabaseID
                  AND wd1.FileID = wd2.FileID
            ),
            writestats AS (
                SELECT
                'PHYSICAL WRITES' AS Pattern,
                ROW_NUMBER() OVER (ORDER BY wd2.avg_stall_write_ms DESC) AS StallRank,
                wd2.SampleTime AS [Sample Time],
                DATEDIFF(ss,wd1.SampleTime, wd2.SampleTime) AS [Sample (seconds)],
                wd1.DatabaseName ,
                wd1.FileLogicalName AS [File Name],
                UPPER(SUBSTRING(wd1.PhysicalName, 1, 2)) AS [Drive] ,
                wd1.SizeOnDiskMB ,
                ( wd2.num_of_writes - wd1.num_of_writes ) AS [# Reads/Writes],
                CASE WHEN wd2.num_of_writes - wd1.num_of_writes &gt; 0
                  THEN CAST(( wd2.bytes_written - wd1.bytes_written)/1024./1024. AS NUMERIC(21,1))
                  ELSE 0
                END AS [MB Read/Written],
                wd2.avg_stall_write_ms AS [Avg Stall (ms)],
                wd1.PhysicalName AS [file physical name]
            FROM #FileStats wd2
                JOIN #FileStats wd1 ON wd2.SampleTime &gt; wd1.SampleTime
                  AND wd1.DatabaseID = wd2.DatabaseID
                  AND wd1.FileID = wd2.FileID
            )
            SELECT
                Pattern, [Sample Time], [Sample (seconds)], [File Name], [Drive],  [# Reads/Writes],[MB Read/Written],[Avg Stall (ms)], [file physical name]
            FROM readstats
            WHERE StallRank &lt;=5 AND [MB Read/Written] &gt; 0
            UNION ALL
            SELECT Pattern, [Sample Time], [Sample (seconds)], [File Name], [Drive],  [# Reads/Writes],[MB Read/Written],[Avg Stall (ms)], [file physical name]
            FROM writestats
            WHERE StallRank &lt;=5 AND [MB Read/Written] &gt; 0;


            -------------------------
            --What happened: #PerfmonStats
            -------------------------

            SELECT 'PERFMON' AS Pattern, pLast.[object_name], pLast.counter_name, pLast.instance_name,
                pFirst.SampleTime AS FirstSampleTime, pFirst.cntr_value AS FirstSampleValue,
                pLast.SampleTime AS LastSampleTime, pLast.cntr_value AS LastSampleValue,
                pLast.cntr_value - pFirst.cntr_value AS ValueDelta,
                ((1.0 * pLast.cntr_value - pFirst.cntr_value) / DATEDIFF(ss, pFirst.SampleTime, pLast.SampleTime)) AS ValuePerSecond
                FROM #PerfmonStats pLast
                    INNER JOIN #PerfmonStats pFirst ON pFirst.[object_name] = pLast.[object_name] AND pFirst.counter_name = pLast.counter_name AND (pFirst.instance_name = pLast.instance_name OR (pFirst.instance_name IS NULL AND pLast.instance_name IS NULL))
                    AND pLast.ID &gt; pFirst.ID
				WHERE (pLast.cntr_value - pFirst.cntr_value) &gt; 0
                ORDER BY Pattern, pLast.[object_name], pLast.counter_name, pLast.instance_name


            -------------------------
            --What happened: #FileStats
            -------------------------
            SELECT
                [qsNow].[ID] AS [Now-ID],
                [qsNow].[Pass] AS [Now-Pass],
                [qsNow].[SampleTime] AS [Now-SampleTime],
                [qsNow].[sql_handle] AS [Now-sql_handle],
                [qsNow].[statement_start_offset] AS [Now-statement_start_offset],
                [qsNow].[statement_end_offset] AS [Now-statement_end_offset],
                [qsNow].[plan_generation_num] AS [Now-plan_generation_num],
                [qsNow].[plan_handle] AS [Now-plan_handle],
                [qsNow].[execution_count] AS [Now-execution_count],
                [qsNow].[total_worker_time] AS [Now-total_worker_time],
                [qsNow].[total_physical_reads] AS [Now-total_physical_reads],
                [qsNow].[total_logical_writes] AS [Now-total_logical_writes],
                [qsNow].[total_logical_reads] AS [Now-total_logical_reads],
                [qsNow].[total_clr_time] AS [Now-total_clr_time],
                [qsNow].[total_elapsed_time] AS [Now-total_elapsed_time],
                [qsNow].[creation_time] AS [Now-creation_time],
                [qsNow].[query_hash] AS [Now-query_hash],
                [qsNow].[query_plan_hash] AS [Now-query_plan_hash],
                [qsNow].[Points] AS [Now-Points],
                [qsFirst].[ID] AS [First-ID],
                [qsFirst].[Pass] AS [First-Pass],
                [qsFirst].[SampleTime] AS [First-SampleTime],
                [qsFirst].[sql_handle] AS [First-sql_handle],
                [qsFirst].[statement_start_offset] AS [First-statement_start_offset],
                [qsFirst].[statement_end_offset] AS [First-statement_end_offset],
                [qsFirst].[plan_generation_num] AS [First-plan_generation_num],
                [qsFirst].[plan_handle] AS [First-plan_handle],
                [qsFirst].[execution_count] AS [First-execution_count],
                [qsFirst].[total_worker_time] AS [First-total_worker_time],
                [qsFirst].[total_physical_reads] AS [First-total_physical_reads],
                [qsFirst].[total_logical_writes] AS [First-total_logical_writes],
                [qsFirst].[total_logical_reads] AS [First-total_logical_reads],
                [qsFirst].[total_clr_time] AS [First-total_clr_time],
                [qsFirst].[total_elapsed_time] AS [First-total_elapsed_time],
                [qsFirst].[creation_time] AS [First-creation_time],
                [qsFirst].[query_hash] AS [First-query_hash],
                [qsFirst].[query_plan_hash] AS [First-query_plan_hash],
                [qsFirst].[Points] AS [First-Points]
            FROM #QueryStats qsNow
              INNER JOIN #QueryStats qsFirst ON qsNow.[sql_handle] = qsFirst.[sql_handle] AND qsNow.statement_start_offset = qsFirst.statement_start_offset AND qsNow.statement_end_offset = qsFirst.statement_end_offset AND qsNow.plan_generation_num = qsFirst.plan_generation_num AND qsNow.plan_handle = qsFirst.plan_handle AND qsFirst.Pass = 1
            WHERE qsNow.Pass = 2
        END

    DROP TABLE #BlitzFirstResults;

    /* What's running right now? This is the first and last result set. */
    IF @SinceStartup = 0 AND @Seconds &gt; 0 AND @ExpertMode = 1 
    BEGIN
		IF OBJECT_ID('dbo.sp_BlitzWho') IS NOT NULL
		BEGIN
			EXEC [dbo].[sp_BlitzWho]
		END
    END /* IF @SinceStartup = 0 AND @Seconds &gt; 0 AND @ExpertMode = 1   -   What's running right now? This is the first and last result set. */

END /* IF @Question IS NULL */
ELSE IF @Question IS NOT NULL

/* We're playing Magic SQL 8 Ball, so give them an answer. */
BEGIN
    IF OBJECT_ID('tempdb..#BlitzFirstAnswers') IS NOT NULL
        DROP TABLE #BlitzFirstAnswers;
    CREATE TABLE #BlitzFirstAnswers(Answer VARCHAR(200) NOT NULL);
    INSERT INTO #BlitzFirstAnswers VALUES ('It sounds like a SAN problem.');
    INSERT INTO #BlitzFirstAnswers VALUES ('You know what you need? Bacon.');
    INSERT INTO #BlitzFirstAnswers VALUES ('Talk to the developers about that.');
    INSERT INTO #BlitzFirstAnswers VALUES ('Let''s post that on StackOverflow.com and find out.');
    INSERT INTO #BlitzFirstAnswers VALUES ('Have you tried adding an index?');
    INSERT INTO #BlitzFirstAnswers VALUES ('Have you tried dropping an index?');
    INSERT INTO #BlitzFirstAnswers VALUES ('You can''t prove anything.');
    INSERT INTO #BlitzFirstAnswers VALUES ('Please phrase the question in the form of an answer.');
    INSERT INTO #BlitzFirstAnswers VALUES ('Outlook not so good. Access even worse.');
    INSERT INTO #BlitzFirstAnswers VALUES ('Did you try asking the rubber duck? http://www.codinghorror.com/blog/2012/03/rubber-duck-problem-solving.html');
    INSERT INTO #BlitzFirstAnswers VALUES ('Oooo, I read about that once.');
    INSERT INTO #BlitzFirstAnswers VALUES ('I feel your pain.');
    INSERT INTO #BlitzFirstAnswers VALUES ('http://LMGTFY.com');
    INSERT INTO #BlitzFirstAnswers VALUES ('No comprende Ingles, senor.');
    INSERT INTO #BlitzFirstAnswers VALUES ('I don''t have that problem on my Mac.');
    INSERT INTO #BlitzFirstAnswers VALUES ('Is Priority Boost on?');
    INSERT INTO #BlitzFirstAnswers VALUES ('Have you tried rebooting your machine?');
    INSERT INTO #BlitzFirstAnswers VALUES ('Try defragging your cursors.');
    INSERT INTO #BlitzFirstAnswers VALUES ('Why are you wearing that? Do you have a job interview later or something?');
    INSERT INTO #BlitzFirstAnswers VALUES ('I''m ashamed that you don''t know the answer to that question.');
    INSERT INTO #BlitzFirstAnswers VALUES ('Duh, Debra.');
    INSERT INTO #BlitzFirstAnswers VALUES ('Have you tried restoring TempDB?');
    SELECT TOP 1 Answer FROM #BlitzFirstAnswers ORDER BY NEWID();
END

END /* ELSE IF @OutputType = 'SCHEMA' */

SET NOCOUNT OFF;
GO



/* How to run it:
EXEC dbo.sp_BlitzFirst

With extra diagnostic info:
EXEC dbo.sp_BlitzFirst @ExpertMode = 1;

In Ask a Question mode:
EXEC dbo.sp_BlitzFirst 'Is this cursor bad?';

Saving output to tables:
EXEC sp_BlitzFirst @Seconds = 60
, @OutputDatabaseName = 'DBAtools'
, @OutputSchemaName = 'dbo'
, @OutputTableName = 'BlitzFirstResults'
, @OutputTableNameFileStats = 'BlitzFirstResults_FileStats'
, @OutputTableNamePerfmonStats = 'BlitzFirstResults_PerfmonStats'
, @OutputTableNameWaitStats = 'BlitzFirstResults_WaitStats'
*/</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>TroubleShooting</Category>
        <Language>SQLSERVER2K SQL</Language>
        <Public>false</Public>
        <Name>sp_BlitzIndex____script</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>TroubleShooting</Category>
          <Language>SQLSERVER2K SQL</Language>
          <Public>false</Public>
          <Name>sp_BlitzIndex____script</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>SET ANSI_NULLS ON;
SET ANSI_PADDING ON;
SET ANSI_WARNINGS ON;
SET ARITHABORT ON;
SET CONCAT_NULL_YIELDS_NULL ON;
SET QUOTED_IDENTIFIER ON;
SET STATISTICS IO OFF;
SET STATISTICS TIME OFF;
GO

IF OBJECT_ID('dbo.sp_BlitzIndex') IS NULL
  EXEC ('CREATE PROCEDURE dbo.sp_BlitzIndex AS RETURN 0;')
GO

ALTER PROCEDURE dbo.sp_BlitzIndex
    @DatabaseName NVARCHAR(128) = NULL, /*Defaults to current DB if not specified*/
    @SchemaName NVARCHAR(128) = NULL, /*Requires table_name as well.*/
    @TableName NVARCHAR(128) = NULL,  /*Requires schema_name as well.*/
    @Mode TINYINT=0, /*0=Diagnose, 1=Summarize, 2=Index Usage Detail, 3=Missing Index Detail, 4=Diagnose Details*/
        /*Note:@Mode doesn't matter if you're specifying schema_name and @TableName.*/
    @Filter TINYINT = 0, /* 0=no filter (default). 1=No low-usage warnings for objects with 0 reads. 2=Only warn for objects &gt;= 500MB */
        /*Note:@Filter doesn't do anything unless @Mode=0*/
	@SkipPartitions BIT	= 0,
	@SkipStatistics BIT	= 1,
    @GetAllDatabases BIT = 0,
    @BringThePain BIT = 0,
    @ThresholdMB INT = 250 /* Number of megabytes that an object must be before we include it in basic results */,
    @OutputServerName NVARCHAR(256) = NULL ,
    @OutputDatabaseName NVARCHAR(256) = NULL ,
    @OutputSchemaName NVARCHAR(256) = NULL ,
    @OutputTableName NVARCHAR(256) = NULL ,
	@Help TINYINT = 0,
	@VersionDate DATETIME = NULL OUTPUT
WITH RECOMPILE
AS
SET NOCOUNT ON;
SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;
DECLARE @Version VARCHAR(30);
SET @Version = '4.6';
SET @VersionDate = '20161210';
IF @Help = 1 PRINT '
/*
sp_BlitzIndex from http://FirstResponderKit.org
	
This script analyzes the design and performance of your indexes.

To learn more, visit http://FirstResponderKit.org where you can download new
versions for free, watch training videos on how it works, get more info on
the findings, contribute your own code, and more.

Known limitations of this version:
 - Only Microsoft-supported versions of SQL Server. Sorry, 2005 and 2000.
 - The @OutputDatabaseName parameters are not functional yet. To check the
   status of this enhancement request, visit:
   https://github.com/BrentOzarULTD/SQL-Server-First-Responder-Kit/issues/221
 - Does not analyze columnstore, spatial, XML, or full text indexes. If you
   would like to contribute code to analyze those, head over to Github and
   check out the issues list: http://FirstResponderKit.org
 - Index create statements are just to give you a rough idea of the syntax. It includes filters and fillfactor.
 --        Example 1: index creates use ONLINE=? instead of ONLINE=ON / ONLINE=OFF. This is because it is important 
           for the user to understand if it is going to be offline and not just run a script.
 --        Example 2: they do not include all the options the index may have been created with (padding, compression
           filegroup/partition scheme etc.)
 --        (The compression and filegroup index create syntax is not trivial because it is set at the partition 
           level and is not trivial to code.)
 - Does not advise you about data modeling for clustered indexes and primary keys (primarily looks for signs of insanity.)

Unknown limitations of this version:
 - We knew them once, but we forgot.

Changes - for the full list of improvements and fixes in this version, see:
https://github.com/BrentOzarULTD/SQL-Server-First-Responder-Kit/milestone/4?closed=1


MIT License

Copyright (c) 2016 Brent Ozar Unlimited

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
'


DECLARE @ScriptVersionName NVARCHAR(50);
DECLARE @DaysUptime NUMERIC(23,2);
DECLARE @DatabaseID INT;
DECLARE @ObjectID INT;
DECLARE @dsql NVARCHAR(MAX);
DECLARE @params NVARCHAR(MAX);
DECLARE @msg NVARCHAR(4000);
DECLARE @ErrorSeverity INT;
DECLARE @ErrorState INT;
DECLARE @Rowcount BIGINT;
DECLARE @SQLServerProductVersion NVARCHAR(128);
DECLARE @SQLServerEdition INT;
DECLARE @FilterMB INT;
DECLARE @collation NVARCHAR(256);
DECLARE @NumDatabases INT;
DECLARE @LineFeed NVARCHAR(5);

SET @LineFeed = CHAR(13) + CHAR(10);
SELECT @SQLServerProductVersion = CAST(SERVERPROPERTY('ProductVersion') AS NVARCHAR(128));
SELECT @SQLServerEdition =CAST(SERVERPROPERTY('EngineEdition') AS INT); /* We default to online index creates where EngineEdition=3*/
SET @FilterMB=250;
SELECT @ScriptVersionName = 'sp_BlitzIndex(TM) v' + @Version + ' - ' + DATENAME(MM, @VersionDate) + ' ' + RIGHT('0'+DATENAME(DD, @VersionDate),2) + ', ' + DATENAME(YY, @VersionDate)

RAISERROR(N'Starting run. %s', 0,1, @ScriptVersionName) WITH NOWAIT;

IF OBJECT_ID('tempdb..#IndexSanity') IS NOT NULL 
    DROP TABLE #IndexSanity;

IF OBJECT_ID('tempdb..#IndexPartitionSanity') IS NOT NULL 
    DROP TABLE #IndexPartitionSanity;

IF OBJECT_ID('tempdb..#IndexSanitySize') IS NOT NULL 
    DROP TABLE #IndexSanitySize;

IF OBJECT_ID('tempdb..#IndexColumns') IS NOT NULL 
    DROP TABLE #IndexColumns;

IF OBJECT_ID('tempdb..#MissingIndexes') IS NOT NULL 
    DROP TABLE #MissingIndexes;

IF OBJECT_ID('tempdb..#ForeignKeys') IS NOT NULL 
    DROP TABLE #ForeignKeys;

IF OBJECT_ID('tempdb..#BlitzIndexResults') IS NOT NULL 
    DROP TABLE #BlitzIndexResults;
        
IF OBJECT_ID('tempdb..#IndexCreateTsql') IS NOT NULL    
    DROP TABLE #IndexCreateTsql;

IF OBJECT_ID('tempdb..#DatabaseList') IS NOT NULL 
    DROP TABLE #DatabaseList;

IF OBJECT_ID('tempdb..#Statistics') IS NOT NULL 
    DROP TABLE #Statistics;

IF OBJECT_ID('tempdb..#PartitionCompressionInfo') IS NOT NULL 
    DROP TABLE #PartitionCompressionInfo;

IF OBJECT_ID('tempdb..#ComputedColumns') IS NOT NULL 
    DROP TABLE #ComputedColumns;
	
IF OBJECT_ID('tempdb..#TraceStatus') IS NOT NULL
	DROP TABLE #TraceStatus;

        RAISERROR (N'Create temp tables.',0,1) WITH NOWAIT;
        CREATE TABLE #BlitzIndexResults
            (
              blitz_result_id INT IDENTITY PRIMARY KEY,
              check_id INT NOT NULL,
              index_sanity_id INT NULL,
              Priority INT NULL,
              findings_group VARCHAR(4000) NOT NULL,
              finding VARCHAR(200) NOT NULL,
              [database_name] VARCHAR(200) NULL,
              URL VARCHAR(200) NOT NULL,
              details NVARCHAR(4000) NOT NULL,
              index_definition NVARCHAR(MAX) NOT NULL,
              secret_columns NVARCHAR(MAX) NULL,
              index_usage_summary NVARCHAR(MAX) NULL,
              index_size_summary NVARCHAR(MAX) NULL,
              create_tsql NVARCHAR(MAX) NULL,
              more_info NVARCHAR(MAX)NULL
            );

        CREATE TABLE #IndexSanity
            (
              [index_sanity_id] INT IDENTITY PRIMARY KEY,
              [database_id] SMALLINT NOT NULL ,
              [object_id] INT NOT NULL ,
              [index_id] INT NOT NULL ,
              [index_type] TINYINT NOT NULL,
              [database_name] NVARCHAR(128) NOT NULL ,
              [schema_name] NVARCHAR(128) NOT NULL ,
              [object_name] NVARCHAR(128) NOT NULL ,
              index_name NVARCHAR(128) NULL ,
              key_column_names NVARCHAR(MAX) NULL ,
              key_column_names_with_sort_order NVARCHAR(MAX) NULL ,
              key_column_names_with_sort_order_no_types NVARCHAR(MAX) NULL ,
              count_key_columns INT NULL ,
              include_column_names NVARCHAR(MAX) NULL ,
              include_column_names_no_types NVARCHAR(MAX) NULL ,
              count_included_columns INT NULL ,
              partition_key_column_name NVARCHAR(MAX) NULL,
              filter_definition NVARCHAR(MAX) NOT NULL ,
              is_indexed_view BIT NOT NULL ,
              is_unique BIT NOT NULL ,
              is_primary_key BIT NOT NULL ,
              is_XML BIT NOT NULL,
              is_spatial BIT NOT NULL,
              is_NC_columnstore BIT NOT NULL,
              is_CX_columnstore BIT NOT NULL,
              is_disabled BIT NOT NULL ,
              is_hypothetical BIT NOT NULL ,
              is_padded BIT NOT NULL ,
              fill_factor SMALLINT NOT NULL ,
              user_seeks BIGINT NOT NULL ,
              user_scans BIGINT NOT NULL ,
              user_lookups BIGINT NOT  NULL ,
              user_updates BIGINT NULL ,
              last_user_seek DATETIME NULL ,
              last_user_scan DATETIME NULL ,
              last_user_lookup DATETIME NULL ,
              last_user_update DATETIME NULL ,
              is_referenced_by_foreign_key BIT DEFAULT(0),
              secret_columns NVARCHAR(MAX) NULL,
              count_secret_columns INT NULL,
              create_date DATETIME NOT NULL,
              modify_date DATETIME NOT NULL,
            [db_schema_object_name] AS [schema_name] + '.' + [object_name]  ,
            [db_schema_object_indexid] AS [schema_name] + '.' + [object_name]
                + CASE WHEN [index_name] IS NOT NULL THEN '.' + index_name
                ELSE ''
                END + ' (' + CAST(index_id AS NVARCHAR(20)) + ')' ,
            first_key_column_name AS CASE    WHEN count_key_columns &gt; 1
                THEN LEFT(key_column_names, CHARINDEX(',', key_column_names, 0) - 1)
                ELSE key_column_names
                END ,
            index_definition AS 
            CASE WHEN partition_key_column_name IS NOT NULL 
                THEN N'[PARTITIONED BY:' + partition_key_column_name +  N']' 
                ELSE '' 
                END +
                CASE index_id
                    WHEN 0 THEN N'[HEAP] '
                    WHEN 1 THEN N'[CX] '
                    ELSE N'' END + CASE WHEN is_indexed_view = 1 THEN '[VIEW] '
                    ELSE N'' END + CASE WHEN is_primary_key = 1 THEN N'[PK] '
                    ELSE N'' END + CASE WHEN is_XML = 1 THEN N'[XML] '
                    ELSE N'' END + CASE WHEN is_spatial = 1 THEN N'[SPATIAL] '
                    ELSE N'' END + CASE WHEN is_NC_columnstore = 1 THEN N'[COLUMNSTORE] '
                    ELSE N'' END + CASE WHEN is_disabled = 1 THEN N'[DISABLED] '
                    ELSE N'' END + CASE WHEN is_hypothetical = 1 THEN N'[HYPOTHETICAL] '
                    ELSE N'' END + CASE WHEN is_unique = 1 AND is_primary_key = 0 THEN N'[UNIQUE] '
                    ELSE N'' END + CASE WHEN count_key_columns &gt; 0 THEN 
                        N'[' + CAST(count_key_columns AS VARCHAR(10)) + N' KEY' 
                            + CASE WHEN count_key_columns &gt; 1 THEN  N'S' ELSE N'' END
                            + N'] ' + LTRIM(key_column_names_with_sort_order)
                    ELSE N'' END + CASE WHEN count_included_columns &gt; 0 THEN 
                        N' [' + CAST(count_included_columns AS VARCHAR(10))  + N' INCLUDE' + 
                            + CASE WHEN count_included_columns &gt; 1 THEN  N'S' ELSE N'' END                    
                            + N'] ' + include_column_names
                    ELSE N'' END + CASE WHEN filter_definition &lt;&gt; N'' THEN N' [FILTER] ' + filter_definition
                    ELSE N'' END ,
            [total_reads] AS user_seeks + user_scans + user_lookups,
            [reads_per_write] AS CAST(CASE WHEN user_updates &gt; 0
                THEN ( user_seeks + user_scans + user_lookups )  / (1.0 * user_updates)
                ELSE 0 END AS MONEY) ,
            [index_usage_summary] AS N'Reads: ' + 
                REPLACE(CONVERT(NVARCHAR(30),CAST((user_seeks + user_scans + user_lookups) AS MONEY), 1), '.00', '')
                + CASE WHEN user_seeks + user_scans + user_lookups &gt; 0 THEN
                    N' (' 
                        + RTRIM(
                        CASE WHEN user_seeks &gt; 0 THEN REPLACE(CONVERT(NVARCHAR(30),CAST((user_seeks) AS MONEY), 1), '.00', '') + N' seek ' ELSE N'' END
                        + CASE WHEN user_scans &gt; 0 THEN REPLACE(CONVERT(NVARCHAR(30),CAST((user_scans) AS MONEY), 1), '.00', '') + N' scan '  ELSE N'' END
                        + CASE WHEN user_lookups &gt; 0 THEN  REPLACE(CONVERT(NVARCHAR(30),CAST((user_lookups) AS MONEY), 1), '.00', '') + N' lookup' ELSE N'' END
                        )
                        + N') '
                    ELSE N' ' END 
                + N'Writes:' + 
                REPLACE(CONVERT(NVARCHAR(30),CAST(user_updates AS MONEY), 1), '.00', ''),
            [more_info] AS N'EXEC dbo.sp_BlitzIndex @DatabaseName=' + QUOTENAME([database_name],'''') + 
                N', @SchemaName=' + QUOTENAME([schema_name],'''') + N', @TableName=' + QUOTENAME([object_name],'''') + N';'
		);


        CREATE TABLE #IndexPartitionSanity
            (
              [index_partition_sanity_id] INT IDENTITY PRIMARY KEY ,
              [index_sanity_id] INT NULL ,
              [database_id] INT NOT NULL ,
              [object_id] INT NOT NULL ,
              [index_id] INT NOT NULL ,
              [partition_number] INT NOT NULL ,
              row_count BIGINT NOT NULL ,
              reserved_MB NUMERIC(29,2) NOT NULL ,
              reserved_LOB_MB NUMERIC(29,2) NOT NULL ,
              reserved_row_overflow_MB NUMERIC(29,2) NOT NULL ,
              leaf_insert_count BIGINT NULL ,
              leaf_delete_count BIGINT NULL ,
              leaf_update_count BIGINT NULL ,
              range_scan_count BIGINT NULL ,
              singleton_lookup_count BIGINT NULL , 
              forwarded_fetch_count BIGINT NULL ,
              lob_fetch_in_pages BIGINT NULL ,
              lob_fetch_in_bytes BIGINT NULL ,
              row_overflow_fetch_in_pages BIGINT NULL ,
              row_overflow_fetch_in_bytes BIGINT NULL ,
              row_lock_count BIGINT NULL ,
              row_lock_wait_count BIGINT NULL ,
              row_lock_wait_in_ms BIGINT NULL ,
              page_lock_count BIGINT NULL ,
              page_lock_wait_count BIGINT NULL ,
              page_lock_wait_in_ms BIGINT NULL ,
              index_lock_promotion_attempt_count BIGINT NULL ,
              index_lock_promotion_count BIGINT NULL,
              data_compression_desc VARCHAR(60) NULL
            );

        CREATE TABLE #IndexSanitySize
            (
              [index_sanity_size_id] INT IDENTITY NOT NULL ,
              [index_sanity_id] INT NULL ,
              [database_id] INT NOT NULL,
              partition_count INT NOT NULL ,
              total_rows BIGINT NOT NULL ,
              total_reserved_MB NUMERIC(29,2) NOT NULL ,
              total_reserved_LOB_MB NUMERIC(29,2) NOT NULL ,
              total_reserved_row_overflow_MB NUMERIC(29,2) NOT NULL ,
              total_leaf_delete_count BIGINT NULL,
              total_leaf_update_count BIGINT NULL,
              total_range_scan_count BIGINT NULL,
              total_singleton_lookup_count BIGINT NULL,
              total_forwarded_fetch_count BIGINT NULL,
              total_row_lock_count BIGINT NULL ,
              total_row_lock_wait_count BIGINT NULL ,
              total_row_lock_wait_in_ms BIGINT NULL ,
              avg_row_lock_wait_in_ms BIGINT NULL ,
              total_page_lock_count BIGINT NULL ,
              total_page_lock_wait_count BIGINT NULL ,
              total_page_lock_wait_in_ms BIGINT NULL ,
              avg_page_lock_wait_in_ms BIGINT NULL ,
               total_index_lock_promotion_attempt_count BIGINT NULL ,
              total_index_lock_promotion_count BIGINT NULL ,
              data_compression_desc VARCHAR(8000) NULL,
              index_size_summary AS ISNULL(
                CASE WHEN partition_count &gt; 1
                        THEN N'[' + CAST(partition_count AS NVARCHAR(10)) + N' PARTITIONS] '
                        ELSE N''
                END + REPLACE(CONVERT(NVARCHAR(30),CAST([total_rows] AS MONEY), 1), N'.00', N'') + N' rows; '
                + CASE WHEN total_reserved_MB &gt; 1024 THEN 
                    CAST(CAST(total_reserved_MB/1024. AS NUMERIC(29,1)) AS NVARCHAR(30)) + N'GB'
                ELSE 
                    CAST(CAST(total_reserved_MB AS NUMERIC(29,1)) AS NVARCHAR(30)) + N'MB'
                END
                + CASE WHEN total_reserved_LOB_MB &gt; 1024 THEN 
                    N'; ' + CAST(CAST(total_reserved_LOB_MB/1024. AS NUMERIC(29,1)) AS NVARCHAR(30)) + N'GB LOB'
                WHEN total_reserved_LOB_MB &gt; 0 THEN
                    N'; ' + CAST(CAST(total_reserved_LOB_MB AS NUMERIC(29,1)) AS NVARCHAR(30)) + N'MB LOB'
                ELSE ''
                END
                 + CASE WHEN total_reserved_row_overflow_MB &gt; 1024 THEN
                    N'; ' + CAST(CAST(total_reserved_row_overflow_MB/1024. AS NUMERIC(29,1)) AS NVARCHAR(30)) + N'GB Row Overflow'
                WHEN total_reserved_row_overflow_MB &gt; 0 THEN
                    N'; ' + CAST(CAST(total_reserved_row_overflow_MB AS NUMERIC(29,1)) AS NVARCHAR(30)) + N'MB Row Overflow'
                ELSE ''
                END ,
                    N'Error- NULL in computed column'),
            index_op_stats AS ISNULL(
                (
                    REPLACE(CONVERT(NVARCHAR(30),CAST(total_singleton_lookup_count AS MONEY), 1),N'.00',N'') + N' singleton lookups; '
                    + REPLACE(CONVERT(NVARCHAR(30),CAST(total_range_scan_count AS MONEY), 1),N'.00',N'') + N' scans/seeks; '
                    + REPLACE(CONVERT(NVARCHAR(30),CAST(total_leaf_delete_count AS MONEY), 1),N'.00',N'') + N' deletes; '
                    + REPLACE(CONVERT(NVARCHAR(30),CAST(total_leaf_update_count AS MONEY), 1),N'.00',N'') + N' updates; '
                    + CASE WHEN ISNULL(total_forwarded_fetch_count,0) &gt;0 THEN
                        REPLACE(CONVERT(NVARCHAR(30),CAST(total_forwarded_fetch_count AS MONEY), 1),N'.00',N'') + N' forward records fetched; '
                    ELSE N'' END

                    /* rows will only be in this dmv when data is in memory for the table */
                ), N'Table metadata not in memory'),
            index_lock_wait_summary AS ISNULL(
                CASE WHEN total_row_lock_wait_count = 0 AND  total_page_lock_wait_count = 0 AND
                    total_index_lock_promotion_attempt_count = 0 THEN N'0 lock waits.'
                ELSE
                    CASE WHEN total_row_lock_wait_count &gt; 0 THEN
                        N'Row lock waits: ' + REPLACE(CONVERT(NVARCHAR(30),CAST(total_row_lock_wait_count AS MONEY), 1), N'.00', N'')
                        + N'; total duration: ' + 
                            CASE WHEN total_row_lock_wait_in_ms &gt;= 60000 THEN /*More than 1 min*/
                                REPLACE(CONVERT(NVARCHAR(30),CAST((total_row_lock_wait_in_ms/60000) AS MONEY), 1), N'.00', N'') + N' minutes; '
                            ELSE                         
                                REPLACE(CONVERT(NVARCHAR(30),CAST(ISNULL(total_row_lock_wait_in_ms/1000,0) AS MONEY), 1), N'.00', N'') + N' seconds; '
                            END
                        + N'avg duration: ' + 
                            CASE WHEN avg_row_lock_wait_in_ms &gt;= 60000 THEN /*More than 1 min*/
                                REPLACE(CONVERT(NVARCHAR(30),CAST((avg_row_lock_wait_in_ms/60000) AS MONEY), 1), N'.00', N'') + N' minutes; '
                            ELSE                         
                                REPLACE(CONVERT(NVARCHAR(30),CAST(ISNULL(avg_row_lock_wait_in_ms/1000,0) AS MONEY), 1), N'.00', N'') + N' seconds; '
                            END
                    ELSE N''
                    END +
                    CASE WHEN total_page_lock_wait_count &gt; 0 THEN
                        N'Page lock waits: ' + REPLACE(CONVERT(NVARCHAR(30),CAST(total_page_lock_wait_count AS MONEY), 1), N'.00', N'')
                        + N'; total duration: ' + 
                            CASE WHEN total_page_lock_wait_in_ms &gt;= 60000 THEN /*More than 1 min*/
                                REPLACE(CONVERT(NVARCHAR(30),CAST((total_page_lock_wait_in_ms/60000) AS MONEY), 1), N'.00', N'') + N' minutes; '
                            ELSE                         
                                REPLACE(CONVERT(NVARCHAR(30),CAST(ISNULL(total_page_lock_wait_in_ms/1000,0) AS MONEY), 1), N'.00', N'') + N' seconds; '
                            END
                        + N'avg duration: ' + 
                            CASE WHEN avg_page_lock_wait_in_ms &gt;= 60000 THEN /*More than 1 min*/
                                REPLACE(CONVERT(NVARCHAR(30),CAST((avg_page_lock_wait_in_ms/60000) AS MONEY), 1), N'.00', N'') + N' minutes; '
                            ELSE                         
                                REPLACE(CONVERT(NVARCHAR(30),CAST(ISNULL(avg_page_lock_wait_in_ms/1000,0) AS MONEY), 1), N'.00', N'') + N' seconds; '
                            END
                    ELSE N''
                    END +
                    CASE WHEN total_index_lock_promotion_attempt_count &gt; 0 THEN
                        N'Lock escalation attempts: ' + REPLACE(CONVERT(NVARCHAR(30),CAST(total_index_lock_promotion_attempt_count AS MONEY), 1), N'.00', N'')
                        + N'; Actual Escalations: ' + REPLACE(CONVERT(NVARCHAR(30),CAST(ISNULL(total_index_lock_promotion_count,0) AS MONEY), 1), N'.00', N'') + N'.'
                    ELSE N''
                    END
                END                  
                    ,'Error- NULL in computed column')
            );

        CREATE TABLE #IndexColumns
            (
              [database_id] INT NOT NULL,
              [object_id] INT NOT NULL ,
              [index_id] INT NOT NULL ,
              [key_ordinal] INT NULL ,
              is_included_column BIT NULL ,
              is_descending_key BIT NULL ,
              [partition_ordinal] INT NULL ,
              column_name NVARCHAR(256) NOT NULL ,
              system_type_name NVARCHAR(256) NOT NULL,
              max_length SMALLINT NOT NULL,
              [precision] TINYINT NOT NULL,
              [scale] TINYINT NOT NULL,
              collation_name NVARCHAR(256) NULL,
              is_nullable BIT NULL,
              is_identity BIT NULL,
              is_computed BIT NULL,
              is_replicated BIT NULL,
              is_sparse BIT NULL,
              is_filestream BIT NULL,
              seed_value BIGINT NULL,
              increment_value INT NULL ,
              last_value BIGINT NULL,
              is_not_for_replication BIT NULL
            );

        CREATE TABLE #MissingIndexes
            ([object_id] INT NOT NULL,
            [database_name] NVARCHAR(128) NOT NULL ,
            [schema_name] NVARCHAR(128) NOT NULL ,
            [table_name] NVARCHAR(128),
            [statement] NVARCHAR(512) NOT NULL,
            magic_benefit_number AS (( user_seeks + user_scans ) * avg_total_user_cost * avg_user_impact),
            avg_total_user_cost NUMERIC(29,4) NOT NULL,
            avg_user_impact NUMERIC(29,1) NOT NULL,
            user_seeks BIGINT NOT NULL,
            user_scans BIGINT NOT NULL,
            unique_compiles BIGINT NULL,
            equality_columns NVARCHAR(4000), 
            inequality_columns NVARCHAR(4000),
            included_columns NVARCHAR(4000),
                [index_estimated_impact] AS 
                    REPLACE(CONVERT(NVARCHAR(256),CAST(CAST(
                                    (user_seeks + user_scans)
                                     AS BIGINT) AS MONEY), 1), '.00', '') + N' use' 
                        + CASE WHEN (user_seeks + user_scans) &gt; 1 THEN N's' ELSE N'' END
                         +N'; Impact: ' + CAST(avg_user_impact AS NVARCHAR(30))
                        + N'%; Avg query cost: '
                        + CAST(avg_total_user_cost AS NVARCHAR(30)),
                [missing_index_details] AS
                    CASE WHEN equality_columns IS NOT NULL THEN N'EQUALITY: ' + equality_columns + N' '
                         ELSE N''
                    END + CASE WHEN inequality_columns IS NOT NULL THEN N'INEQUALITY: ' + inequality_columns + N' '
                       ELSE N''
                    END + CASE WHEN included_columns IS NOT NULL THEN N'INCLUDES: ' + included_columns + N' '
                        ELSE N''
                    END,
                [create_tsql] AS N'CREATE INDEX [ix_' + table_name + N'_' 
                    + REPLACE(REPLACE(REPLACE(REPLACE(
                        ISNULL(equality_columns,N'')+ 
                        CASE WHEN equality_columns IS NOT NULL AND inequality_columns IS NOT NULL THEN N'_' ELSE N'' END
                        + ISNULL(inequality_columns,''),',','')
                        ,'[',''),']',''),' ','_') 
                    + CASE WHEN included_columns IS NOT NULL THEN N'_includes' ELSE N'' END + N'] ON ' 
                    + [statement] + N' (' + ISNULL(equality_columns,N'')
                    + CASE WHEN equality_columns IS NOT NULL AND inequality_columns IS NOT NULL THEN N', ' ELSE N'' END
                    + CASE WHEN inequality_columns IS NOT NULL THEN inequality_columns ELSE N'' END + 
                    ') ' + CASE WHEN included_columns IS NOT NULL THEN N' INCLUDE (' + included_columns + N')' ELSE N'' END
                    + N' WITH (' 
                        + N'FILLFACTOR=100, ONLINE=?, SORT_IN_TEMPDB=?' 
                    + N')'
                    + N';'
                    ,
                [more_info] AS N'EXEC dbo.sp_BlitzIndex @DatabaseName=' + QUOTENAME([database_name],'''') + 
                    N', @SchemaName=' + QUOTENAME([schema_name],'''') + N', @TableName=' + QUOTENAME([table_name],'''') + N';'
            );

        CREATE TABLE #ForeignKeys (
            [database_name] NVARCHAR(128) NOT NULL ,
            foreign_key_name NVARCHAR(256),
            parent_object_id INT,
            parent_object_name NVARCHAR(256),
            referenced_object_id INT,
            referenced_object_name NVARCHAR(256),
            is_disabled BIT,
            is_not_trusted BIT,
            is_not_for_replication BIT,
            parent_fk_columns NVARCHAR(MAX),
            referenced_fk_columns NVARCHAR(MAX),
            update_referential_action_desc NVARCHAR(16),
            delete_referential_action_desc NVARCHAR(60)
        )
        
        CREATE TABLE #IndexCreateTsql (
            index_sanity_id INT NOT NULL,
            create_tsql NVARCHAR(MAX) NOT NULL
        )

        CREATE TABLE #DatabaseList (
			DatabaseName NVARCHAR(256)
        )

		CREATE TABLE #PartitionCompressionInfo (
			[index_sanity_id] INT NULL,
			[partition_compression_detail] VARCHAR(8000) NULL
        )

		CREATE TABLE #Statistics (
		  database_name NVARCHAR(256) NOT NULL,
		  table_name NVARCHAR(128) NULL,
		  schema_name NVARCHAR(128) NULL,
		  index_name  NVARCHAR(128) NULL,
		  column_name  NVARCHAR(128) NULL,
		  statistics_name NVARCHAR(128) NULL,
		  last_statistics_update DATETIME NULL,
		  days_since_last_stats_update INT NULL,
		  rows BIGINT NULL,
		  rows_sampled BIGINT NULL,
		  percent_sampled DECIMAL(18, 1) NULL,
		  histogram_steps INT NULL,
		  modification_counter BIGINT NULL,
		  percent_modifications DECIMAL(18, 1) NULL,
		  modifications_before_auto_update INT NULL,
		  index_type_desc NVARCHAR(128) NULL,
		  table_create_date DATETIME NULL,
		  table_modify_date DATETIME NULL,
		  no_recompute BIT NULL,
		  has_filter BIT NULL,
		  filter_definition NVARCHAR(MAX) NULL
		); 

		CREATE TABLE #ComputedColumns
		(
		  index_sanity_id INT IDENTITY(1, 1) NOT NULL,
		  database_name NVARCHAR(128) NULL,
		  table_name NVARCHAR(128) NOT NULL,
		  schema_name NVARCHAR(128) NOT NULL,
		  column_name NVARCHAR(128) NULL,
		  is_nullable BIT NULL,
		  definition NVARCHAR(MAX) NULL,
		  uses_database_collation BIT NOT NULL,
		  is_persisted BIT NOT NULL,
		  is_computed BIT NOT NULL,
		  is_function INT NOT NULL,
		  column_definition NVARCHAR(MAX) NULL
		);
		
		CREATE TABLE #TraceStatus
		(
		 TraceFlag VARCHAR(10) ,
		 status BIT ,
		 Global BIT ,
		 Session BIT
		);


IF @GetAllDatabases = 1
    BEGIN
        INSERT INTO #DatabaseList (DatabaseName)
        SELECT  DB_NAME(database_id)
        FROM    sys.databases
        WHERE user_access_desc='MULTI_USER'
        AND state_desc = 'ONLINE'
        AND database_id &gt; 4
        AND DB_NAME(database_id) NOT IN ('ReportServer','ReportServerTempDB')
        AND is_distributor = 0;
    END
ELSE
    BEGIN
        INSERT INTO #DatabaseList
                ( DatabaseName )
        SELECT CASE WHEN @DatabaseName IS NULL OR @DatabaseName = N'' THEN DB_NAME()
                    ELSE @DatabaseName END
    END

SET @NumDatabases = @@ROWCOUNT;

/* Running on 50+ databases can take a reaaallly long time, so we want explicit permission to do so (and only after warning about it) */

BEGIN TRY
        IF @NumDatabases &gt;= 50 AND @BringThePain != 1
        BEGIN

            INSERT    #BlitzIndexResults ( Priority, check_id, findings_group, finding, URL, details, index_definition,
                                            index_usage_summary, index_size_summary )
            VALUES  ( -1, 0 , 
		            @ScriptVersionName,
                    CASE WHEN @GetAllDatabases = 1 THEN N'All Databases' ELSE N'Database ' + QUOTENAME(@DatabaseName) + N' as of ' + CONVERT(NVARCHAR(16),GETDATE(),121) END, 
                    N'From Your Community Volunteers' ,   N'http://www.BrentOzar.com/BlitzIndex' ,
                    N''
                    , N'',N''
                    );
            INSERT    #BlitzIndexResults ( Priority, check_id, findings_group, finding, database_name, URL, details, index_definition,
                                            index_usage_summary, index_size_summary )
            VALUES  ( 1, 0 , 
		           N'You''re trying to run sp_BlitzIndex on a server with ' + CAST(@NumDatabases AS NVARCHAR(8)) + N' databases. ',
                   N'Running sp_BlitzIndex on a server with 50+ databases may cause temporary insanity for the server and/or user.',
				   N'If you''re sure you want to do this, run again with the parameter @BringThePain = 1.',
                   'http://FirstResponderKit.org', '', '', '', ''
                    );        
		
		SELECT bir.blitz_result_id,
               bir.check_id,
               bir.index_sanity_id,
               bir.Priority,
               bir.findings_group,
               bir.finding,
               bir.database_name,
               bir.URL,
               bir.details,
               bir.index_definition,
               bir.secret_columns,
               bir.index_usage_summary,
               bir.index_size_summary,
               bir.create_tsql,
               bir.more_info 
			   FROM #BlitzIndexResults AS bir

		RETURN;

		END
END TRY
BEGIN CATCH
        RAISERROR (N'Failure to execute due to number of databases.', 0,1) WITH NOWAIT;

        SELECT    @msg = ERROR_MESSAGE(), @ErrorSeverity = ERROR_SEVERITY(), @ErrorState = ERROR_STATE();

        RAISERROR (@msg, 
               @ErrorSeverity, 
               @ErrorState 
               );
        
        WHILE @@trancount &gt; 0 
            ROLLBACK;

        RETURN;
    END CATCH;

/* Permission granted or unnecessary? Ok, let's go! */

DECLARE c1 CURSOR 
LOCAL FAST_FORWARD 
FOR 
SELECT DatabaseName FROM #DatabaseList ORDER BY DatabaseName

OPEN c1
FETCH NEXT FROM c1 INTO @DatabaseName
                WHILE @@FETCH_STATUS = 0
BEGIN
    
    RAISERROR (@LineFeed, 0, 1) WITH NOWAIT;
    RAISERROR (@LineFeed, 0, 1) WITH NOWAIT;
    RAISERROR (@DatabaseName, 0, 1) WITH NOWAIT;

SELECT   @DatabaseID = [database_id]
FROM     sys.databases
         WHERE [name] = @DatabaseName
         AND user_access_desc='MULTI_USER'
         AND state_desc = 'ONLINE';

/* Last startup */
SELECT @DaysUptime = CAST(DATEDIFF(hh,create_date,GETDATE())/24. AS NUMERIC (23,2))
FROM    sys.databases
WHERE   database_id = 2;

IF @DaysUptime = 0 SET @DaysUptime = .01;

----------------------------------------
--STEP 1: OBSERVE THE PATIENT
--This step puts index information into temp tables.
----------------------------------------
BEGIN TRY
    BEGIN

        --Validate SQL Server Verson

        IF (SELECT LEFT(@SQLServerProductVersion,
              CHARINDEX('.',@SQLServerProductVersion,0)-1
              )) &lt;= 9
        BEGIN
            SET @msg=N'sp_BlitzIndex is only supported on SQL Server 2008 and higher. The version of this instance is: ' + @SQLServerProductVersion;
            RAISERROR(@msg,16,1);
        END

        --Short circuit here if database name does not exist.
        IF @DatabaseName IS NULL OR @DatabaseID IS NULL
        BEGIN
            SET @msg='Database does not exist or is not online/multi-user: cannot proceed.'
            RAISERROR(@msg,16,1);
        END    

        --Validate parameters.
        IF (@Mode NOT IN (0,1,2,3,4))
        BEGIN
            SET @msg=N'Invalid @Mode parameter. 0=diagnose, 1=summarize, 2=index detail, 3=missing index detail, 4=diagnose detail';
            RAISERROR(@msg,16,1);
        END

        IF (@Mode &lt;&gt; 0 AND @TableName IS NOT NULL)
        BEGIN
            SET @msg=N'Setting the @Mode doesn''t change behavior if you supply @TableName. Use default @Mode=0 to see table detail.';
            RAISERROR(@msg,16,1);
        END

        IF ((@Mode &lt;&gt; 0 OR @TableName IS NOT NULL) AND @Filter &lt;&gt; 0)
        BEGIN
            SET @msg=N'@Filter only appies when @Mode=0 and @TableName is not specified. Please try again.';
            RAISERROR(@msg,16,1);
        END

        IF (@SchemaName IS NOT NULL AND @TableName IS NULL) 
        BEGIN
            SET @msg='We can''t run against a whole schema! Specify a @TableName, or leave both NULL for diagnosis.'
            RAISERROR(@msg,16,1);
        END


        IF  (@TableName IS NOT NULL AND @SchemaName IS NULL)
        BEGIN
            SET @SchemaName=N'dbo'
            SET @msg='@SchemaName wasn''t specified-- assuming schema=dbo.'
            RAISERROR(@msg,1,1) WITH NOWAIT;
        END

        --If a table is specified, grab the object id.
        --Short circuit if it doesn't exist.
        IF @TableName IS NOT NULL
        BEGIN
            SET @dsql = N'
                    SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;
                    SELECT    @ObjectID= OBJECT_ID
                    FROM    ' + QUOTENAME(@DatabaseName) + N'.sys.objects AS so
                    JOIN    ' + QUOTENAME(@DatabaseName) + N'.sys.schemas AS sc on 
                        so.schema_id=sc.schema_id
                    where so.type in (''U'', ''V'')
                    and so.name=' + QUOTENAME(@TableName,'''')+ N'
                    and sc.name=' + QUOTENAME(@SchemaName,'''')+ N'
                    /*Has a row in sys.indexes. This lets us get indexed views.*/
                    and exists (
                        SELECT si.name
                        FROM ' + QUOTENAME(@DatabaseName) + '.sys.indexes AS si 
                        WHERE so.object_id=si.object_id)
                    OPTION (RECOMPILE);';

            SET @params='@ObjectID INT OUTPUT'                

            IF @dsql IS NULL 
                RAISERROR('@dsql is null',16,1);

            EXEC sp_executesql @dsql, @params, @ObjectID=@ObjectID OUTPUT;
            
            IF @ObjectID IS NULL
                    BEGIN
                        SET @msg=N'Oh, this is awkward. I can''t find the table or indexed view you''re looking for in that database.' + CHAR(10) +
                            N'Please check your parameters.'
                        RAISERROR(@msg,1,1);
                        RETURN;
                    END
        END

        --set @collation
        SELECT @collation=collation_name
        FROM sys.databases
        WHERE database_id=@DatabaseID;

        --insert columns for clustered indexes and heaps
        --collect info on identity columns for this one
        SET @dsql = N'SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;
                SELECT ' + CAST(@DatabaseID AS NVARCHAR(16)) + ',    
                    si.object_id, 
                    si.index_id, 
                    sc.key_ordinal, 
                    sc.is_included_column, 
                    sc.is_descending_key,
                    sc.partition_ordinal,
                    c.name as column_name, 
                    st.name as system_type_name,
                    c.max_length,
                    c.[precision],
                    c.[scale],
                    c.collation_name,
                    c.is_nullable,
                    c.is_identity,
                    c.is_computed,
                    c.is_replicated,
                    ' + CASE WHEN @SQLServerProductVersion NOT LIKE '9%' THEN N'c.is_sparse' ELSE N'NULL as is_sparse' END + N',
                    ' + CASE WHEN @SQLServerProductVersion NOT LIKE '9%' THEN N'c.is_filestream' ELSE N'NULL as is_filestream' END + N',
                    CAST(ic.seed_value AS BIGINT),
                    CAST(ic.increment_value AS INT),
                    CAST(ic.last_value AS BIGINT),
                    ic.is_not_for_replication
                FROM    ' + QUOTENAME(@DatabaseName) + N'.sys.indexes si
                JOIN    ' + QUOTENAME(@DatabaseName) + N'.sys.columns c ON
                    si.object_id=c.object_id
                LEFT JOIN ' + QUOTENAME(@DatabaseName) + N'.sys.index_columns sc ON 
                    sc.object_id = si.object_id
                    and sc.index_id=si.index_id
                    AND sc.column_id=c.column_id
                LEFT JOIN ' + QUOTENAME(@DatabaseName) + N'.sys.identity_columns ic ON
                    c.object_id=ic.object_id and
                    c.column_id=ic.column_id
                JOIN ' + QUOTENAME(@DatabaseName) + N'.sys.types st ON 
                    c.system_type_id=st.system_type_id
                    AND c.user_type_id=st.user_type_id
                WHERE si.index_id in (0,1) ' 
                    + CASE WHEN @ObjectID IS NOT NULL 
                        THEN N' AND si.object_id=' + CAST(@ObjectID AS NVARCHAR(30)) 
                    ELSE N'' END 
                + N'OPTION (RECOMPILE);';

        IF @dsql IS NULL 
            RAISERROR('@dsql is null',16,1);

        RAISERROR (N'Inserting data into #IndexColumns for clustered indexes and heaps',0,1) WITH NOWAIT;
        INSERT    #IndexColumns ( database_id, object_id, index_id, key_ordinal, is_included_column, is_descending_key, partition_ordinal,
            column_name, system_type_name, max_length, precision, scale, collation_name, is_nullable, is_identity, is_computed,
            is_replicated, is_sparse, is_filestream, seed_value, increment_value, last_value, is_not_for_replication )
                EXEC sp_executesql @dsql;

        --insert columns for nonclustered indexes
        --this uses a full join to sys.index_columns
        --We don't collect info on identity columns here. They may be in NC indexes, but we just analyze identities in the base table.
        SET @dsql = N'SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;
                SELECT ' + CAST(@DatabaseID AS NVARCHAR(16)) + ',    
                    si.object_id, 
                    si.index_id, 
                    sc.key_ordinal, 
                    sc.is_included_column, 
                    sc.is_descending_key,
                    sc.partition_ordinal,
                    c.name as column_name, 
                    st.name as system_type_name,
                    c.max_length,
                    c.[precision],
                    c.[scale],
                    c.collation_name,
                    c.is_nullable,
                    c.is_identity,
                    c.is_computed,
                    c.is_replicated,
                    ' + CASE WHEN @SQLServerProductVersion NOT LIKE '9%' THEN N'c.is_sparse' ELSE N'NULL AS is_sparse' END + N',
                    ' + CASE WHEN @SQLServerProductVersion NOT LIKE '9%' THEN N'c.is_filestream' ELSE N'NULL AS is_filestream' END + N'                
                FROM    ' + QUOTENAME(@DatabaseName) + N'.sys.indexes AS si
                JOIN    ' + QUOTENAME(@DatabaseName) + N'.sys.columns AS c ON
                    si.object_id=c.object_id
                JOIN ' + QUOTENAME(@DatabaseName) + N'.sys.index_columns AS sc ON 
                    sc.object_id = si.object_id
                    and sc.index_id=si.index_id
                    AND sc.column_id=c.column_id
                JOIN ' + QUOTENAME(@DatabaseName) + N'.sys.types AS st ON 
                    c.system_type_id=st.system_type_id
                    AND c.user_type_id=st.user_type_id
                WHERE si.index_id not in (0,1) ' 
                    + CASE WHEN @ObjectID IS NOT NULL 
                        THEN N' AND si.object_id=' + CAST(@ObjectID AS NVARCHAR(30)) 
                    ELSE N'' END 
                + N'OPTION (RECOMPILE);';

        IF @dsql IS NULL 
            RAISERROR('@dsql is null',16,1);

        RAISERROR (N'Inserting data into #IndexColumns for nonclustered indexes',0,1) WITH NOWAIT;
        INSERT    #IndexColumns ( database_id, object_id, index_id, key_ordinal, is_included_column, is_descending_key, partition_ordinal,
            column_name, system_type_name, max_length, precision, scale, collation_name, is_nullable, is_identity, is_computed,
            is_replicated, is_sparse, is_filestream )
                EXEC sp_executesql @dsql;
                    
        SET @dsql = N'SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;
                SELECT    ' + CAST(@DatabaseID AS NVARCHAR(10)) + ' AS database_id, 
                        so.object_id, 
                        si.index_id, 
                        si.type,
                        ' + QUOTENAME(@DatabaseName, '''') + ' AS database_name, 
                        COALESCE(sc.NAME, ''Unknown'') AS [schema_name],
                        COALESCE(so.name, ''Unknown'') AS [object_name], 
                        COALESCE(si.name, ''Unknown'') AS [index_name],
                        CASE    WHEN so.[type] = CAST(''V'' AS CHAR(2)) THEN 1 ELSE 0 END, 
                        si.is_unique, 
                        si.is_primary_key, 
                        CASE when si.type = 3 THEN 1 ELSE 0 END AS is_XML,
                        CASE when si.type = 4 THEN 1 ELSE 0 END AS is_spatial,
                        CASE when si.type = 6 THEN 1 ELSE 0 END AS is_NC_columnstore,
                        CASE when si.type = 5 then 1 else 0 end as is_CX_columnstore,
                        si.is_disabled,
                        si.is_hypothetical, 
                        si.is_padded, 
                        si.fill_factor,'
                        + CASE WHEN @SQLServerProductVersion NOT LIKE '9%' THEN '
                        CASE WHEN si.filter_definition IS NOT NULL THEN si.filter_definition
                             ELSE ''''
                        END AS filter_definition' ELSE ''''' AS filter_definition' END + '
                        , ISNULL(us.user_seeks, 0), ISNULL(us.user_scans, 0),
                        ISNULL(us.user_lookups, 0), ISNULL(us.user_updates, 0), us.last_user_seek, us.last_user_scan,
                        us.last_user_lookup, us.last_user_update,
                        so.create_date, so.modify_date
                FROM    ' + QUOTENAME(@DatabaseName) + '.sys.indexes AS si WITH (NOLOCK)
                        JOIN ' + QUOTENAME(@DatabaseName) + '.sys.objects AS so WITH (NOLOCK) ON si.object_id = so.object_id
                                               AND so.is_ms_shipped = 0 /*Exclude objects shipped by Microsoft*/
                                               AND so.type &lt;&gt; ''TF'' /*Exclude table valued functions*/
                        JOIN ' + QUOTENAME(@DatabaseName) + '.sys.schemas sc ON so.schema_id = sc.schema_id
                        LEFT JOIN sys.dm_db_index_usage_stats AS us WITH (NOLOCK) ON si.[object_id] = us.[object_id]
                                                                       AND si.index_id = us.index_id
                                                                       AND us.database_id = '+ CAST(@DatabaseID AS NVARCHAR(10)) + '
                WHERE    si.[type] IN ( 0, 1, 2, 3, 4, 5, 6 ) 
                /* Heaps, clustered, nonclustered, XML, spatial, Cluster Columnstore, NC Columnstore */ ' +
                CASE WHEN @TableName IS NOT NULL THEN ' and so.name=' + QUOTENAME(@TableName,'''') + ' ' ELSE '' END + 
        'OPTION    ( RECOMPILE );
        ';
        IF @dsql IS NULL 
            RAISERROR('@dsql is null',16,1);

        RAISERROR (N'Inserting data into #IndexSanity',0,1) WITH NOWAIT;
        INSERT    #IndexSanity ( [database_id], [object_id], [index_id], [index_type], [database_name], [schema_name], [object_name],
                                index_name, is_indexed_view, is_unique, is_primary_key, is_XML, is_spatial, is_NC_columnstore, is_CX_columnstore,
                                is_disabled, is_hypothetical, is_padded, fill_factor, filter_definition, user_seeks, user_scans, 
                                user_lookups, user_updates, last_user_seek, last_user_scan, last_user_lookup, last_user_update,
                                create_date, modify_date )
                EXEC sp_executesql @dsql;

        RAISERROR (N'Updating #IndexSanity.key_column_names',0,1) WITH NOWAIT;
        UPDATE    #IndexSanity
        SET        key_column_names = D1.key_column_names
        FROM    #IndexSanity si
                CROSS APPLY ( SELECT    RTRIM(STUFF( (SELECT    N', ' + c.column_name 
                                    + N' {' + system_type_name + N' ' + CAST(max_length AS NVARCHAR(50)) +  N'}'
                                        AS col_definition
                                    FROM    #IndexColumns c
                                    WHERE    c.database_id= si.database_id
                                            AND c.object_id = si.object_id
                                            AND c.index_id = si.index_id
                                            AND c.is_included_column = 0 /*Just Keys*/
                                            AND c.key_ordinal &gt; 0 /*Ignore non-key columns, such as partitioning keys*/
                                    ORDER BY c.object_id, c.index_id, c.key_ordinal    
                            FOR      XML PATH('') ,TYPE).value('.', 'varchar(max)'), 1, 1, ''))
                                        ) D1 ( key_column_names )

        RAISERROR (N'Updating #IndexSanity.partition_key_column_name',0,1) WITH NOWAIT;
        UPDATE    #IndexSanity
        SET        partition_key_column_name = D1.partition_key_column_name
        FROM    #IndexSanity si
                CROSS APPLY ( SELECT    RTRIM(STUFF( (SELECT    N', ' + c.column_name AS col_definition
                                    FROM    #IndexColumns c
                                    WHERE    c.database_id= si.database_id
                                            AND c.object_id = si.object_id
                                            AND c.index_id = si.index_id
                                            AND c.partition_ordinal &lt;&gt; 0 /*Just Partitioned Keys*/
                                    ORDER BY c.object_id, c.index_id, c.key_ordinal    
                            FOR      XML PATH('') , TYPE).value('.', 'varchar(max)'), 1, 1,''))) D1 
                                        ( partition_key_column_name )

        RAISERROR (N'Updating #IndexSanity.key_column_names_with_sort_order',0,1) WITH NOWAIT;
        UPDATE    #IndexSanity
        SET        key_column_names_with_sort_order = D2.key_column_names_with_sort_order
        FROM    #IndexSanity si
                CROSS APPLY ( SELECT    RTRIM(STUFF( (SELECT    N', ' + c.column_name + CASE c.is_descending_key
                                    WHEN 1 THEN N' DESC'
                                    ELSE N''
                                + N' {' + system_type_name + N' ' + CAST(max_length AS NVARCHAR(50)) +  N'}'
                                END AS col_definition
                            FROM    #IndexColumns c
                            WHERE    c.database_id= si.database_id
                                    AND c.object_id = si.object_id
                                    AND c.index_id = si.index_id
                                    AND c.is_included_column = 0 /*Just Keys*/
                                    AND c.key_ordinal &gt; 0 /*Ignore non-key columns, such as partitioning keys*/
                            ORDER BY c.object_id, c.index_id, c.key_ordinal    
                    FOR      XML PATH('') , TYPE).value('.', 'varchar(max)'), 1, 1, ''))
                    ) D2 ( key_column_names_with_sort_order )

        RAISERROR (N'Updating #IndexSanity.key_column_names_with_sort_order_no_types (for create tsql)',0,1) WITH NOWAIT;
        UPDATE    #IndexSanity
        SET        key_column_names_with_sort_order_no_types = D2.key_column_names_with_sort_order_no_types
        FROM    #IndexSanity si
                CROSS APPLY ( SELECT    RTRIM(STUFF( (SELECT    N', ' + QUOTENAME(c.column_name) + CASE c.is_descending_key
                                    WHEN 1 THEN N' [DESC]'
                                    ELSE N''
                                END AS col_definition
                            FROM    #IndexColumns c
                            WHERE    c.database_id= si.database_id
                                    AND c.object_id = si.object_id
                                    AND c.index_id = si.index_id
                                    AND c.is_included_column = 0 /*Just Keys*/
                                    AND c.key_ordinal &gt; 0 /*Ignore non-key columns, such as partitioning keys*/
                            ORDER BY c.object_id, c.index_id, c.key_ordinal    
                    FOR      XML PATH('') , TYPE).value('.', 'varchar(max)'), 1, 1, ''))
                    ) D2 ( key_column_names_with_sort_order_no_types )

        RAISERROR (N'Updating #IndexSanity.include_column_names',0,1) WITH NOWAIT;
        UPDATE    #IndexSanity
        SET        include_column_names = D3.include_column_names
        FROM    #IndexSanity si
                CROSS APPLY ( SELECT    RTRIM(STUFF( (SELECT    N', ' + c.column_name
                                + N' {' + system_type_name + N' ' + CAST(max_length AS NVARCHAR(50)) +  N'}'
                                FROM    #IndexColumns c
                                WHERE    c.database_id= si.database_id
                                        AND c.object_id = si.object_id
                                        AND c.index_id = si.index_id
                                        AND c.is_included_column = 1 /*Just includes*/
                                ORDER BY c.column_name /*Order doesn't matter in includes, 
                                        this is here to make rows easy to compare.*/ 
                        FOR      XML PATH('') ,  TYPE).value('.', 'varchar(max)'), 1, 1, ''))
                        ) D3 ( include_column_names );

        RAISERROR (N'Updating #IndexSanity.include_column_names_no_types (for create tsql)',0,1) WITH NOWAIT;
        UPDATE    #IndexSanity
        SET        include_column_names_no_types = D3.include_column_names_no_types
        FROM    #IndexSanity si
                CROSS APPLY ( SELECT    RTRIM(STUFF( (SELECT    N', ' + QUOTENAME(c.column_name)
                                FROM    #IndexColumns c
                                        WHERE    c.database_id= si.database_id
                                        AND c.object_id = si.object_id
                                        AND c.index_id = si.index_id
                                        AND c.is_included_column = 1 /*Just includes*/
                                ORDER BY c.column_name /*Order doesn't matter in includes, 
                                        this is here to make rows easy to compare.*/ 
                        FOR      XML PATH('') ,  TYPE).value('.', 'varchar(max)'), 1, 1, ''))
                        ) D3 ( include_column_names_no_types );

        RAISERROR (N'Updating #IndexSanity.count_key_columns and count_include_columns',0,1) WITH NOWAIT;
        UPDATE    #IndexSanity
        SET        count_included_columns = D4.count_included_columns,
                count_key_columns = D4.count_key_columns
        FROM    #IndexSanity si
                CROSS APPLY ( SELECT    SUM(CASE WHEN is_included_column = 'true' THEN 1
                                                 ELSE 0
                                            END) AS count_included_columns,
                                        SUM(CASE WHEN is_included_column = 'false' AND c.key_ordinal &gt; 0 THEN 1
                                                 ELSE 0
                                            END) AS count_key_columns
                              FROM        #IndexColumns c
                                    WHERE    c.database_id= si.database_id
                                            AND c.object_id = si.object_id
                                        AND c.index_id = si.index_id 
                                        ) AS D4 ( count_included_columns, count_key_columns );

		 IF (@SkipPartitions = 0)
			BEGIN			
			IF (SELECT LEFT(@SQLServerProductVersion,
			      CHARINDEX('.',@SQLServerProductVersion,0)-1 )) &lt;= 2147483647 --Make change here 			
			BEGIN
            
			RAISERROR (N'Preferring non-2012 syntax with LEFT JOIN to sys.dm_db_index_operational_stats',0,1) WITH NOWAIT;

            --NOTE: If you want to use the newer syntax for 2012+, you'll have to change 2147483647 to 11 on line ~819
			--This change was made because on a table with lots of paritions, the OUTER APPLY was crazy slow.
            SET @dsql = N'SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;
                        SELECT    ' + CAST(@DatabaseID AS NVARCHAR(10)) + ' AS database_id,
                                ps.object_id, 
                                ps.index_id, 
                                ps.partition_number, 
                                ps.row_count,
                                ps.reserved_page_count * 8. / 1024. AS reserved_MB,
                                ps.lob_reserved_page_count * 8. / 1024. AS reserved_LOB_MB,
                                ps.row_overflow_reserved_page_count * 8. / 1024. AS reserved_row_overflow_MB,
                                os.leaf_insert_count, 
                                os.leaf_delete_count, 
                                os.leaf_update_count, 
                                os.range_scan_count, 
                                os.singleton_lookup_count,  
                                os.forwarded_fetch_count,
                                os.lob_fetch_in_pages, 
                                os.lob_fetch_in_bytes, 
                                os.row_overflow_fetch_in_pages,
                                os.row_overflow_fetch_in_bytes, 
                                os.row_lock_count, 
                                os.row_lock_wait_count,
                                os.row_lock_wait_in_ms, 
                                os.page_lock_count, 
                                os.page_lock_wait_count, 
                                os.page_lock_wait_in_ms,
                                os.index_lock_promotion_attempt_count, 
                                os.index_lock_promotion_count, 
                            ' + CASE WHEN @SQLServerProductVersion NOT LIKE '9%' THEN 'par.data_compression_desc ' ELSE 'null as data_compression_desc' END + '
                    FROM    ' + QUOTENAME(@DatabaseName) + '.sys.dm_db_partition_stats AS ps  
                    JOIN ' + QUOTENAME(@DatabaseName) + '.sys.partitions AS par on ps.partition_id=par.partition_id
                    JOIN ' + QUOTENAME(@DatabaseName) + '.sys.objects AS so ON ps.object_id = so.object_id
                               AND so.is_ms_shipped = 0 /*Exclude objects shipped by Microsoft*/
                               AND so.type &lt;&gt; ''TF'' /*Exclude table valued functions*/
                    LEFT JOIN ' + QUOTENAME(@DatabaseName) + '.sys.dm_db_index_operational_stats('
                + CAST(@DatabaseID AS NVARCHAR(10)) + ', NULL, NULL,NULL) AS os ON
                    ps.object_id=os.object_id and ps.index_id=os.index_id and ps.partition_number=os.partition_number 
                    WHERE 1=1 
                    ' + CASE WHEN @ObjectID IS NOT NULL THEN N'AND so.object_id=' + CAST(@ObjectID AS NVARCHAR(30)) + N' ' ELSE N' ' END + '
                    ' + CASE WHEN @Filter = 2 THEN N'AND ps.reserved_page_count * 8./1024. &gt; ' + CAST(@FilterMB AS NVARCHAR(5)) + N' ' ELSE N' ' END + '
            ORDER BY ps.object_id,  ps.index_id, ps.partition_number
            OPTION    ( RECOMPILE );
            ';
        END
        ELSE
        BEGIN
        RAISERROR (N'Using 2012 syntax to query sys.dm_db_index_operational_stats',0,1) WITH NOWAIT;
		--This is the syntax that will be used if you change 2147483647 to 11 on line ~819.
		--If you have a lot of paritions and this suddenly starts running for a long time, change it back.
         SET @dsql = N'SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;
                        SELECT  ' + CAST(@DatabaseID AS NVARCHAR(10)) + ' AS database_id,
                                ps.object_id, 
                                ps.index_id, 
                                ps.partition_number, 
                                ps.row_count,
                                ps.reserved_page_count * 8. / 1024. AS reserved_MB,
                                ps.lob_reserved_page_count * 8. / 1024. AS reserved_LOB_MB,
                                ps.row_overflow_reserved_page_count * 8. / 1024. AS reserved_row_overflow_MB,
                                os.leaf_insert_count, 
                                os.leaf_delete_count, 
                                os.leaf_update_count, 
                                os.range_scan_count, 
                                os.singleton_lookup_count,  
                                os.forwarded_fetch_count,
                                os.lob_fetch_in_pages, 
                                os.lob_fetch_in_bytes, 
                                os.row_overflow_fetch_in_pages,
                                os.row_overflow_fetch_in_bytes, 
                                os.row_lock_count, 
                                os.row_lock_wait_count,
                                os.row_lock_wait_in_ms, 
                                os.page_lock_count, 
                                os.page_lock_wait_count, 
                                os.page_lock_wait_in_ms,
                                os.index_lock_promotion_attempt_count, 
                                os.index_lock_promotion_count, 
                                ' + CASE WHEN @SQLServerProductVersion NOT LIKE '9%' THEN N'par.data_compression_desc ' ELSE N'null as data_compression_desc' END + N'
                        FROM    ' + QUOTENAME(@DatabaseName) + N'.sys.dm_db_partition_stats AS ps  
                        JOIN ' + QUOTENAME(@DatabaseName) + N'.sys.partitions AS par on ps.partition_id=par.partition_id
                        JOIN ' + QUOTENAME(@DatabaseName) + N'.sys.objects AS so ON ps.object_id = so.object_id
                                   AND so.is_ms_shipped = 0 /*Exclude objects shipped by Microsoft*/
                                   AND so.type &lt;&gt; ''TF'' /*Exclude table valued functions*/
                        OUTER APPLY ' + QUOTENAME(@DatabaseName) + N'.sys.dm_db_index_operational_stats('
                    + CAST(@DatabaseID AS NVARCHAR(10)) + N', ps.object_id, ps.index_id,ps.partition_number) AS os
                        WHERE 1=1 
                        ' + CASE WHEN @ObjectID IS NOT NULL THEN N'AND so.object_id=' + CAST(@ObjectID AS NVARCHAR(30)) + N' ' ELSE N' ' END + N'
                        ' + CASE WHEN @Filter = 2 THEN N'AND ps.reserved_page_count * 8./1024. &gt; ' + CAST(@FilterMB AS NVARCHAR(5)) + N' ' ELSE N' ' END + '
                ORDER BY ps.object_id,  ps.index_id, ps.partition_number
                OPTION    ( RECOMPILE );
                ';
        END;       

        IF @dsql IS NULL 
            RAISERROR('@dsql is null',16,1);

        RAISERROR (N'Inserting data into #IndexPartitionSanity',0,1) WITH NOWAIT;
        INSERT    #IndexPartitionSanity ( [database_id],
                                          [object_id], 
                                          index_id, 
                                          partition_number, 
                                          row_count, 
                                          reserved_MB,
                                          reserved_LOB_MB, 
                                          reserved_row_overflow_MB, 
                                          leaf_insert_count,
                                          leaf_delete_count, 
                                          leaf_update_count, 
                                          range_scan_count,
                                          singleton_lookup_count,
                                          forwarded_fetch_count, 
                                          lob_fetch_in_pages, 
                                          lob_fetch_in_bytes, 
                                          row_overflow_fetch_in_pages,
                                          row_overflow_fetch_in_bytes, 
                                          row_lock_count, 
                                          row_lock_wait_count,
                                          row_lock_wait_in_ms, 
                                          page_lock_count, 
                                          page_lock_wait_count,
                                          page_lock_wait_in_ms, 
                                          index_lock_promotion_attempt_count,
                                          index_lock_promotion_count, 
                                          data_compression_desc )
                EXEC sp_executesql @dsql;
        
        RAISERROR (N'Updating index_sanity_id on #IndexPartitionSanity',0,1) WITH NOWAIT;
        UPDATE    #IndexPartitionSanity
        SET        index_sanity_id = i.index_sanity_id
        FROM #IndexPartitionSanity ps
                JOIN #IndexSanity i ON ps.[object_id] = i.[object_id]
                                        AND ps.index_id = i.index_id
                                        AND i.database_id = ps.database_id
		END; --End Check For @SkipPartitions = 0


        RAISERROR (N'Inserting data into #IndexSanitySize',0,1) WITH NOWAIT;
        INSERT    #IndexSanitySize ( [index_sanity_id], [database_id], partition_count, total_rows, total_reserved_MB,
                                     total_reserved_LOB_MB, total_reserved_row_overflow_MB, total_range_scan_count,
                                     total_singleton_lookup_count, total_leaf_delete_count, total_leaf_update_count, 
                                     total_forwarded_fetch_count,total_row_lock_count,
                                     total_row_lock_wait_count, total_row_lock_wait_in_ms, avg_row_lock_wait_in_ms,
                                     total_page_lock_count, total_page_lock_wait_count, total_page_lock_wait_in_ms,
                                     avg_page_lock_wait_in_ms, total_index_lock_promotion_attempt_count, 
                                     total_index_lock_promotion_count, data_compression_desc )
                SELECT    index_sanity_id, ipp.database_id, COUNT(*), SUM(row_count), SUM(reserved_MB), SUM(reserved_LOB_MB),
                        SUM(reserved_row_overflow_MB), 
                        SUM(range_scan_count),
                        SUM(singleton_lookup_count),
                        SUM(leaf_delete_count), 
                        SUM(leaf_update_count),
                        SUM(forwarded_fetch_count),
                        SUM(row_lock_count), 
                        SUM(row_lock_wait_count),
                        SUM(row_lock_wait_in_ms), 
                        CASE WHEN SUM(row_lock_wait_in_ms) &gt; 0 THEN
                            SUM(row_lock_wait_in_ms)/(1.*SUM(row_lock_wait_count))
                        ELSE 0 END AS avg_row_lock_wait_in_ms,           
                        SUM(page_lock_count), 
                        SUM(page_lock_wait_count),
                        SUM(page_lock_wait_in_ms), 
                        CASE WHEN SUM(page_lock_wait_in_ms) &gt; 0 THEN
                            SUM(page_lock_wait_in_ms)/(1.*SUM(page_lock_wait_count))
                        ELSE 0 END AS avg_page_lock_wait_in_ms,           
                        SUM(index_lock_promotion_attempt_count),
                        SUM(index_lock_promotion_count),
                        LEFT(MAX(data_compression_info.data_compression_rollup),8000)
                FROM #IndexPartitionSanity ipp
                /* individual partitions can have distinct compression settings, just roll them into a list here*/
                OUTER APPLY (SELECT STUFF((
                    SELECT    N', ' + data_compression_desc
                    FROM #IndexPartitionSanity ipp2
                    WHERE ipp.[object_id]=ipp2.[object_id]
                        AND ipp.[index_id]=ipp2.[index_id]
                        AND ipp.database_id = @DatabaseID
                    ORDER BY ipp2.partition_number
                    FOR      XML PATH(''),TYPE).value('.', 'varchar(max)'), 1, 1, '')) 
                        data_compression_info(data_compression_rollup)
                WHERE ipp.database_id = @DatabaseID
                GROUP BY index_sanity_id, ipp.database_id
                ORDER BY index_sanity_id 
        OPTION    ( RECOMPILE );

        RAISERROR (N'Adding UQ index on #IndexSanity (database_id, object_id, index_id)',0,1) WITH NOWAIT;
        IF NOT EXISTS(SELECT 1 FROM tempdb.sys.indexes WHERE name='uq_database_id_object_id_index_id') 
            CREATE UNIQUE INDEX uq_database_id_object_id_index_id ON #IndexSanity (database_id, object_id, index_id);

        RAISERROR (N'Inserting data into #MissingIndexes',0,1) WITH NOWAIT;
        SET @dsql=N'SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;
                SELECT    id.object_id, ' + QUOTENAME(@DatabaseName,'''') + N', sc.[name], so.[name], id.statement , gs.avg_total_user_cost, 
                        gs.avg_user_impact, gs.user_seeks, gs.user_scans, gs.unique_compiles,id.equality_columns, 
                        id.inequality_columns,id.included_columns
                FROM    sys.dm_db_missing_index_groups ig
                        JOIN sys.dm_db_missing_index_details id ON ig.index_handle = id.index_handle
                        JOIN sys.dm_db_missing_index_group_stats gs ON ig.index_group_handle = gs.group_handle
                        JOIN ' + QUOTENAME(@DatabaseName) + N'.sys.objects so on 
                            id.object_id=so.object_id
                        JOIN ' + QUOTENAME(@DatabaseName) + N'.sys.schemas sc on 
                            so.schema_id=sc.schema_id
                WHERE    id.database_id = ' + CAST(@DatabaseID AS NVARCHAR(30)) + '
                ' + CASE WHEN @ObjectID IS NULL THEN N'' 
                    ELSE N'and id.object_id=' + CAST(@ObjectID AS NVARCHAR(30)) 
                END +
        N'OPTION (RECOMPILE);'

        IF @dsql IS NULL 
            RAISERROR('@dsql is null',16,1);
        INSERT    #MissingIndexes ( [object_id], [database_name], [schema_name], [table_name], [statement], avg_total_user_cost, 
                                    avg_user_impact, user_seeks, user_scans, unique_compiles, equality_columns, 
                                    inequality_columns,included_columns)
        EXEC sp_executesql @dsql;

        SET @dsql = N'
            SELECT ' + QUOTENAME(@DatabaseName,'''')  + N' AS [database_name],
                fk_object.name AS foreign_key_name,
                parent_object.[object_id] AS parent_object_id,
                parent_object.name AS parent_object_name,
                referenced_object.[object_id] AS referenced_object_id,
                referenced_object.name AS referenced_object_name,
                fk.is_disabled,
                fk.is_not_trusted,
                fk.is_not_for_replication,
                parent.fk_columns,
                referenced.fk_columns,
                [update_referential_action_desc],
                [delete_referential_action_desc]
            FROM ' + QUOTENAME(@DatabaseName) + N'.sys.foreign_keys fk
            JOIN ' + QUOTENAME(@DatabaseName) + N'.sys.objects fk_object ON fk.object_id=fk_object.object_id
            JOIN ' + QUOTENAME(@DatabaseName) + N'.sys.objects parent_object ON fk.parent_object_id=parent_object.object_id
            JOIN ' + QUOTENAME(@DatabaseName) + N'.sys.objects referenced_object ON fk.referenced_object_id=referenced_object.object_id
            CROSS APPLY ( SELECT    STUFF( (SELECT    N'', '' + c_parent.name AS fk_columns
                                            FROM    ' + QUOTENAME(@DatabaseName) + N'.sys.foreign_key_columns fkc 
                                            JOIN ' + QUOTENAME(@DatabaseName) + N'.sys.columns c_parent ON fkc.parent_object_id=c_parent.[object_id]
                                                AND fkc.parent_column_id=c_parent.column_id
                                            WHERE    fk.parent_object_id=fkc.parent_object_id
                                                AND fk.[object_id]=fkc.constraint_object_id
                                            ORDER BY fkc.constraint_column_id 
                                    FOR      XML PATH('''') ,
                                              TYPE).value(''.'', ''varchar(max)''), 1, 1, '''')/*This is how we remove the first comma*/ ) parent ( fk_columns )
            CROSS APPLY ( SELECT    STUFF( (SELECT    N'', '' + c_referenced.name AS fk_columns
                                            FROM    ' + QUOTENAME(@DatabaseName) + N'.sys.    foreign_key_columns fkc 
                                            JOIN ' + QUOTENAME(@DatabaseName) + N'.sys.columns c_referenced ON fkc.referenced_object_id=c_referenced.[object_id]
                                                AND fkc.referenced_column_id=c_referenced.column_id
                                            WHERE    fk.referenced_object_id=fkc.referenced_object_id
                                                and fk.[object_id]=fkc.constraint_object_id
                                            ORDER BY fkc.constraint_column_id  /*order by col name, we don''t have anything better*/
                                    FOR      XML PATH('''') ,
                                              TYPE).value(''.'', ''varchar(max)''), 1, 1, '''') ) referenced ( fk_columns )
            ' + CASE WHEN @ObjectID IS NOT NULL THEN 
                    'WHERE fk.parent_object_id=' + CAST(@ObjectID AS NVARCHAR(30)) + N' OR fk.referenced_object_id=' + CAST(@ObjectID AS NVARCHAR(30)) + N' ' 
                    ELSE N' ' END + '
            ORDER BY parent_object_name, foreign_key_name
			OPTION (RECOMPILE);';
        IF @dsql IS NULL 
            RAISERROR('@dsql is null',16,1);

        RAISERROR (N'Inserting data into #ForeignKeys',0,1) WITH NOWAIT;
        INSERT  #ForeignKeys ( [database_name], foreign_key_name, parent_object_id,parent_object_name, referenced_object_id, referenced_object_name,
                                is_disabled, is_not_trusted, is_not_for_replication, parent_fk_columns, referenced_fk_columns,
                                [update_referential_action_desc], [delete_referential_action_desc] )
                EXEC sp_executesql @dsql;

        RAISERROR (N'Updating #IndexSanity.referenced_by_foreign_key',0,1) WITH NOWAIT;
        UPDATE #IndexSanity
            SET is_referenced_by_foreign_key=1
        FROM #IndexSanity s
        JOIN #ForeignKeys fk ON 
            s.object_id=fk.referenced_object_id
            AND LEFT(s.key_column_names,LEN(fk.referenced_fk_columns)) = fk.referenced_fk_columns

        RAISERROR (N'Update index_secret on #IndexSanity for NC indexes.',0,1) WITH NOWAIT;
        UPDATE nc 
        SET secret_columns=
            N'[' + 
            CASE tb.count_key_columns WHEN 0 THEN '1' ELSE CAST(tb.count_key_columns AS VARCHAR(10)) END +
            CASE nc.is_unique WHEN 1 THEN N' INCLUDE' ELSE N' KEY' END +
            CASE WHEN tb.count_key_columns &gt; 1 THEN  N'S] ' ELSE N'] ' END +
            CASE tb.index_id WHEN 0 THEN '[RID]' ELSE LTRIM(tb.key_column_names) +
                /* Uniquifiers only needed on non-unique clustereds-- not heaps */
                CASE tb.is_unique WHEN 0 THEN ' [UNIQUIFIER]' ELSE N'' END
            END
            , count_secret_columns=
            CASE tb.index_id WHEN 0 THEN 1 ELSE 
                tb.count_key_columns +
                    CASE tb.is_unique WHEN 0 THEN 1 ELSE 0 END
            END
        FROM #IndexSanity AS nc
        JOIN #IndexSanity AS tb ON nc.object_id=tb.object_id
            AND tb.index_id IN (0,1) 
        WHERE nc.index_id &gt; 1;

        RAISERROR (N'Update index_secret on #IndexSanity for heaps and non-unique clustered.',0,1) WITH NOWAIT;
        UPDATE tb
        SET secret_columns=    CASE tb.index_id WHEN 0 THEN '[RID]' ELSE '[UNIQUIFIER]' END
            , count_secret_columns = 1
        FROM #IndexSanity AS tb
        WHERE tb.index_id = 0 /*Heaps-- these have the RID */
            OR (tb.index_id=1 AND tb.is_unique=0); /* Non-unique CX: has uniquifer (when needed) */


        RAISERROR (N'Populate #IndexCreateTsql.',0,1) WITH NOWAIT;
        INSERT #IndexCreateTsql (index_sanity_id, create_tsql)
        SELECT
            index_sanity_id,
            ISNULL (
            /* Script drops for disabled non-clustered indexes*/
            CASE WHEN is_disabled = 1 AND index_id &lt;&gt; 1
                THEN N'--DROP INDEX ' + QUOTENAME([index_name]) + N' ON '
                 + QUOTENAME([schema_name]) + N'.' + QUOTENAME([object_name]) 
            ELSE
                CASE index_id WHEN 0 THEN N'--I''m a Heap!' 
                ELSE 
                    CASE WHEN is_XML = 1 OR is_spatial=1 THEN N'' /* Not even trying for these just yet...*/
                    ELSE 
                        CASE WHEN is_primary_key=1 THEN
                            N'ALTER TABLE ' + QUOTENAME([schema_name]) +
                                N'.' + QUOTENAME([object_name]) + 
                                N' ADD CONSTRAINT [' +
                                index_name + 
                                N'] PRIMARY KEY ' + 
                                CASE WHEN index_id=1 THEN N'CLUSTERED (' ELSE N'(' END +
                                key_column_names_with_sort_order_no_types + N' )' 
                            WHEN is_CX_columnstore= 1 THEN
                                 N'CREATE CLUSTERED COLUMNSTORE INDEX ' + QUOTENAME(index_name) + N' on ' + QUOTENAME([schema_name]) + '.' + QUOTENAME([object_name])
                        ELSE /*Else not a PK or cx columnstore */ 
                            N'CREATE ' + 
                            CASE WHEN is_unique=1 THEN N'UNIQUE ' ELSE N'' END +
                            CASE WHEN index_id=1 THEN N'CLUSTERED ' ELSE N'' END +
                            CASE WHEN is_NC_columnstore=1 THEN N'NONCLUSTERED COLUMNSTORE ' 
                            ELSE N'' END +
                            N'INDEX ['
                                 + index_name + N'] ON ' + 
                                QUOTENAME([schema_name]) + '.' + QUOTENAME([object_name]) + 
                                    CASE WHEN is_NC_columnstore=1 THEN 
                                        N' (' + ISNULL(include_column_names_no_types,'') +  N' )' 
                                    ELSE /*Else not colunnstore */ 
                                        N' (' + ISNULL(key_column_names_with_sort_order_no_types,'') +  N' )' 
                                        + CASE WHEN include_column_names_no_types IS NOT NULL THEN 
                                            N' INCLUDE (' + include_column_names_no_types + N')' 
                                            ELSE N'' 
                                        END
                                    END /*End non-colunnstore case */ 
                                + CASE WHEN filter_definition &lt;&gt; N'' THEN N' WHERE ' + filter_definition ELSE N'' END
                            END /*End Non-PK index CASE */ 
                        + CASE WHEN is_NC_columnstore=0 AND is_CX_columnstore=0 THEN
                            N' WITH (' 
                                + N'FILLFACTOR=' + CASE fill_factor WHEN 0 THEN N'100' ELSE CAST(fill_factor AS NVARCHAR(5)) END + ', '
                                + N'ONLINE=?, SORT_IN_TEMPDB=?'
                            + N')'
                        ELSE N'' END
                        + N';'
                      END /*End non-spatial and non-xml CASE */ 
                END
            END, '[Unknown Error]')
                AS create_tsql
        FROM #IndexSanity
        WHERE database_id = @DatabaseID;
	  
	  RAISERROR (N'Populate #PartitionCompressionInfo.',0,1) WITH NOWAIT;
	 ;WITH    [maps]
			  AS ( SELECT   
							index_sanity_id,
							partition_number,
							data_compression_desc,
							partition_number - ROW_NUMBER() OVER (PARTITION BY ips.index_sanity_id, data_compression_desc ORDER BY partition_number ) AS [rN]
				   FROM     #IndexPartitionSanity ips
					),
			[grps]
			  AS ( SELECT   MIN([maps].[partition_number]) AS [MinKey] ,
							MAX([maps].[partition_number]) AS [MaxKey] ,
							index_sanity_id,
							maps.data_compression_desc
				   FROM     [maps]
				   GROUP BY [maps].[rN], index_sanity_id, maps.data_compression_desc)
		INSERT #PartitionCompressionInfo
				(index_sanity_id, partition_compression_detail)
		SELECT DISTINCT grps.index_sanity_id , SUBSTRING((  STUFF((SELECT ', ' + ' Partition'
													+ CASE WHEN [grps2].[MinKey] &lt; [grps2].[MaxKey]
														   THEN +'s '
																+ CAST([grps2].[MinKey] AS VARCHAR)
																+ ' - '
																+ CAST([grps2].[MaxKey] AS VARCHAR)
																+ ' use ' + grps2.data_compression_desc
														   ELSE ' '
																+ CAST([grps2].[MinKey] AS VARCHAR)
																+ ' uses '  + grps2.data_compression_desc
													  END AS [Partitions]
											 FROM   [grps] AS grps2
											 WHERE grps2.index_sanity_id = grps.index_sanity_id
											 ORDER BY grps2.MinKey, grps2.MaxKey
									FOR     XML PATH('') ,
												TYPE 
							).[value]('.', 'VARCHAR(MAX)'), 1, 1, '') ), 0, 8000) AS [partition_compression_detail]
		FROM grps;
		
		RAISERROR (N'Update #PartitionCompressionInfo.',0,1) WITH NOWAIT;
		UPDATE sz
		SET sz.data_compression_desc = pci.partition_compression_detail
		FROM #IndexSanitySize sz
		JOIN #PartitionCompressionInfo AS pci
		ON pci.index_sanity_id = sz.index_sanity_id;
                  



		IF @SkipStatistics = 0 
			BEGIN
		IF  ((PARSENAME(@SQLServerProductVersion, 4) &gt;= 12)
		OR   (PARSENAME(@SQLServerProductVersion, 4) = 11 AND PARSENAME(@SQLServerProductVersion, 2) &gt;= 3000)
		OR   (PARSENAME(@SQLServerProductVersion, 4) = 10 AND PARSENAME(@SQLServerProductVersion, 3) = 50 AND PARSENAME(@SQLServerProductVersion, 2) &gt;= 2500))
		BEGIN
		RAISERROR (N'Gathering Statistics Info With Newer Syntax.',0,1) WITH NOWAIT;
		SET @dsql=N'SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;
					SELECT  ' + QUOTENAME(@DatabaseName,'''') + N' AS database_name,
					obj.name AS table_name,
					sch.name AS schema_name,
			        ISNULL(i.name, ''System Or User Statistic'') AS index_name,
			        c.name AS column_name,
			        s.name AS statistics_name,
			        CONVERT(DATETIME, ddsp.last_updated) AS last_statistics_update,
			        DATEDIFF(DAY, ddsp.last_updated, GETDATE()) AS days_since_last_stats_update,
			        ddsp.rows,
			        ddsp.rows_sampled,
			        CAST(ddsp.rows_sampled / ( 1. * NULLIF(ddsp.rows, 0) ) * 100 AS DECIMAL(18, 1)) AS percent_sampled,
			        ddsp.steps AS histogram_steps,
			        ddsp.modification_counter,
			        CASE WHEN ddsp.modification_counter &gt; 0
			             THEN CAST(ddsp.modification_counter / ( 1. * NULLIF(ddsp.rows, 0) ) * 100 AS DECIMAL(18, 1))
			             ELSE ddsp.modification_counter
			        END AS percent_modifications,
			        CASE WHEN ddsp.rows &lt; 500 THEN 500
			             ELSE CAST(( ddsp.rows * .20 ) + 500 AS INT)
			        END AS modifications_before_auto_update,
			        ISNULL(i.type_desc, ''System Or User Statistic - N/A'') AS index_type_desc,
			        CONVERT(DATETIME, obj.create_date) AS table_create_date,
			        CONVERT(DATETIME, obj.modify_date) AS table_modify_date,
					s.no_recompute,
					s.has_filter,
					s.filter_definition
			FROM    ' + QUOTENAME(@DatabaseName) + N'.sys.stats AS s
			JOIN    ' + QUOTENAME(@DatabaseName) + N'.sys.stats_columns sc
			ON      sc.object_id = s.object_id
			        AND sc.stats_id = s.stats_id
			JOIN    ' + QUOTENAME(@DatabaseName) + N'.sys.columns c
			ON      c.object_id = sc.object_id
			        AND c.column_id = sc.column_id
			JOIN    ' + QUOTENAME(@DatabaseName) + N'.sys.objects obj
			ON      s.object_id = obj.object_id
			JOIN    ' + QUOTENAME(@DatabaseName) + N'.sys.schemas sch
			ON		sch.schema_id = obj.schema_id
			LEFT JOIN    ' + QUOTENAME(@DatabaseName) + N'.sys.indexes AS i
			ON      i.object_id = s.object_id
			        AND i.index_id = s.stats_id
			OUTER APPLY ' + QUOTENAME(@DatabaseName) + N'.sys.dm_db_stats_properties(s.object_id, s.stats_id) AS ddsp
			WHERE obj.is_ms_shipped = 0
			OPTION (RECOMPILE);'
			
			IF @dsql IS NULL 
            RAISERROR('@dsql is null',16,1);

			RAISERROR (N'Inserting data into #Statistics',0,1) WITH NOWAIT;
			INSERT #Statistics ( database_name, table_name, schema_name, index_name, column_name, statistics_name, last_statistics_update, 
								days_since_last_stats_update, rows, rows_sampled, percent_sampled, histogram_steps, modification_counter, 
								percent_modifications, modifications_before_auto_update, index_type_desc, table_create_date, table_modify_date,
								no_recompute, has_filter, filter_definition)
			
			EXEC sp_executesql @dsql;
			END
			ELSE 
			BEGIN
			RAISERROR (N'Gathering Statistics Info With Older Syntax.',0,1) WITH NOWAIT;
			SET @dsql=N'SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;
						SELECT  ' + QUOTENAME(@DatabaseName,'''') + N' AS database_name,
								obj.name AS table_name,
								sch.name AS schema_name,
						        ISNULL(i.name, ''System Or User Statistic'') AS index_name,
						        c.name AS column_name,
						        s.name AS statistics_name,
						        CONVERT(DATETIME, STATS_DATE(s.object_id, s.stats_id)) AS last_statistics_update,
						        DATEDIFF(DAY, STATS_DATE(s.object_id, s.stats_id), GETDATE()) AS days_since_last_stats_update,
						        si.rowcnt,
						        si.rowmodctr,
						        CASE WHEN si.rowmodctr &gt; 0 THEN CAST(si.rowmodctr / ( 1. * NULLIF(si.rowcnt, 0) ) * 100 AS DECIMAL(18, 1))
						             ELSE si.rowmodctr
						        END AS percent_modifications,
						        CASE WHEN si.rowcnt &lt; 500 THEN 500
						             ELSE CAST(( si.rowcnt * .20 ) + 500 AS INT)
						        END AS modifications_before_auto_update,
						        ISNULL(i.type_desc, ''System Or User Statistic - N/A'') AS index_type_desc,
						        CONVERT(DATETIME, obj.create_date) AS table_create_date,
						        CONVERT(DATETIME, obj.modify_date) AS table_modify_date,
								s.no_recompute,
								'
								+ CASE WHEN @SQLServerProductVersion NOT LIKE '9%' 
								THEN N's.has_filter,
									   s.filter_definition' 
								ELSE N'NULL AS has_filter,
								       NULL AS filter_definition' END 
						+ N'								
						FROM    ' + QUOTENAME(@DatabaseName) + N'.sys.stats AS s
						INNER HASH JOIN    ' + QUOTENAME(@DatabaseName) + N'.sys.sysindexes si
						ON      si.name = s.name
						INNER HASH JOIN    ' + QUOTENAME(@DatabaseName) + N'.sys.stats_columns sc
						ON      sc.object_id = s.object_id
						        AND sc.stats_id = s.stats_id
						INNER HASH JOIN    ' + QUOTENAME(@DatabaseName) + N'.sys.columns c
						ON      c.object_id = sc.object_id
						        AND c.column_id = sc.column_id
						INNER HASH JOIN    ' + QUOTENAME(@DatabaseName) + N'.sys.objects obj
						ON      s.object_id = obj.object_id
						INNER HASH JOIN    ' + QUOTENAME(@DatabaseName) + N'.sys.schemas sch
						ON		sch.schema_id = obj.schema_id
						LEFT HASH JOIN ' + QUOTENAME(@DatabaseName) + N'.sys.indexes AS i
						ON      i.object_id = s.object_id
						        AND i.index_id = s.stats_id
						WHERE obj.is_ms_shipped = 0
						AND si.rowcnt &gt; 0
						OPTION (RECOMPILE);'

			IF @dsql IS NULL 
            RAISERROR('@dsql is null',16,1);

			RAISERROR (N'Inserting data into #Statistics',0,1) WITH NOWAIT;
			INSERT #Statistics(database_name, table_name, schema_name, index_name, column_name, statistics_name, 
								last_statistics_update, days_since_last_stats_update, rows, modification_counter, 
								percent_modifications, modifications_before_auto_update, index_type_desc, table_create_date, table_modify_date,
								no_recompute, has_filter, filter_definition)
			
			EXEC sp_executesql @dsql;
			END

			END

			IF  (PARSENAME(@SQLServerProductVersion, 4) &gt;= 10)
			BEGIN
			RAISERROR (N'Gathering Computed Column Info.',0,1) WITH NOWAIT;
			SET @dsql=N'SELECT ' + QUOTENAME(@DatabaseName,'''') + N' AS DatabaseName,
   					   		   t.name AS table_name,
   					           s.name AS schema_name,
   					           c.name AS column_name,
   					           cc.is_nullable,
   					           cc.definition,
   					           cc.uses_database_collation,
   					           cc.is_persisted,
   					           cc.is_computed,
   					   		   CASE WHEN cc.definition LIKE ''%.%'' THEN 1 ELSE 0 END AS is_function,
   					   		   ''ALTER TABLE '' + QUOTENAME(s.name) + ''.'' + QUOTENAME(t.name) + 
   					   		   '' ADD '' + QUOTENAME(c.name) + '' AS '' + cc.definition  + 
							   CASE WHEN is_persisted = 1 THEN '' PERSISTED'' ELSE '''' END + '';'' COLLATE DATABASE_DEFAULT AS [column_definition]
   					   FROM    ' + QUOTENAME(@DatabaseName) + N'.sys.computed_columns AS cc
   					   JOIN    ' + QUOTENAME(@DatabaseName) + N'.sys.columns AS c
   					   ON      cc.object_id = c.object_id
   					   		   AND cc.column_id = c.column_id
   					   JOIN    ' + QUOTENAME(@DatabaseName) + N'.sys.tables AS t
   					   ON      t.object_id = cc.object_id
   					   JOIN    ' + QUOTENAME(@DatabaseName) + N'.sys.schemas AS s
   					   ON      s.schema_id = t.schema_id
					   OPTION (RECOMPILE);'

			IF @dsql IS NULL 
            RAISERROR('@dsql is null',16,1);

			INSERT #ComputedColumns
			        ( database_name, table_name, schema_name, column_name, is_nullable, definition, 
					  uses_database_collation, is_persisted, is_computed, is_function, column_definition )			
			EXEC sp_executesql @dsql;

			END 
			
			RAISERROR (N'Gathering Trace Flag Information',0,1) WITH NOWAIT;
			INSERT #TraceStatus
			EXEC ('DBCC TRACESTATUS(-1) WITH NO_INFOMSGS')			
			
END                    
END TRY
BEGIN CATCH
        RAISERROR (N'Failure populating temp tables.', 0,1) WITH NOWAIT;

        IF @dsql IS NOT NULL
        BEGIN
            SET @msg= 'Last @dsql: ' + @dsql;
            RAISERROR(@msg, 0, 1) WITH NOWAIT;
        END

        SELECT    @msg = @DatabaseName + N' database failed to process. ' + ERROR_MESSAGE(), @ErrorSeverity = ERROR_SEVERITY(), @ErrorState = ERROR_STATE();
        RAISERROR (@msg,@ErrorSeverity, @ErrorState )WITH NOWAIT;
        
        
        WHILE @@trancount &gt; 0 
            ROLLBACK;

        RETURN;
END CATCH;
 FETCH NEXT FROM c1 INTO @DatabaseName
END
DEALLOCATE c1;

----------------------------------------
--STEP 2: DIAGNOSE THE PATIENT
--EVERY QUERY AFTER THIS GOES AGAINST TEMP TABLES ONLY.
----------------------------------------
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                BEGIN TRY
----------------------------------------
--If @TableName is specified, just return information for that table.
--The @Mode parameter doesn't matter if you're looking at a specific table.
----------------------------------------
IF @TableName IS NOT NULL
BEGIN
    RAISERROR(N'@TableName specified, giving detail only on that table.', 0,1) WITH NOWAIT;

    --We do a left join here in case this is a disabled NC.
    --In that case, it won't have any size info/pages allocated.
 
   	
	   WITH table_mode_cte AS (
        SELECT 
            s.db_schema_object_indexid, 
            s.key_column_names,
            s.index_definition, 
            ISNULL(s.secret_columns,N'') AS secret_columns,
            s.fill_factor,
            s.index_usage_summary, 
            sz.index_op_stats,
            ISNULL(sz.index_size_summary,'') /*disabled NCs will be null*/ AS index_size_summary,
			partition_compression_detail ,
            ISNULL(sz.index_lock_wait_summary,'') AS index_lock_wait_summary,
            s.is_referenced_by_foreign_key,
            (SELECT COUNT(*)
                FROM #ForeignKeys fk WHERE fk.parent_object_id=s.object_id
                AND PATINDEX (fk.parent_fk_columns, s.key_column_names)=1) AS FKs_covered_by_index,
            s.last_user_seek,
            s.last_user_scan,
            s.last_user_lookup,
            s.last_user_update,
            s.create_date,
            s.modify_date,
            ct.create_tsql,
            1 AS display_order
        FROM #IndexSanity s
        LEFT JOIN #IndexSanitySize sz ON 
            s.index_sanity_id=sz.index_sanity_id
        LEFT JOIN #IndexCreateTsql ct ON 
            s.index_sanity_id=ct.index_sanity_id
		LEFT JOIN #PartitionCompressionInfo pci ON 
			pci.index_sanity_id = s.index_sanity_id
        WHERE s.[object_id]=@ObjectID
        UNION ALL
        SELECT     N'Database ' + QUOTENAME(@DatabaseName) + N' as of ' + CONVERT(NVARCHAR(16),GETDATE(),121) +             
                N' (' + @ScriptVersionName + ')' ,   
                N'SQL Server First Responder Kit' ,   
                N'http://FirstResponderKit.org' ,
                N'From Your Community Volunteers',
                NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,
                0 AS display_order
    )
    SELECT 
            db_schema_object_indexid AS [Details: db_schema.table.index(indexid)], 
            index_definition AS [Definition: [Property]] ColumnName {datatype maxbytes}], 
            secret_columns AS [Secret Columns],
            fill_factor AS [Fillfactor],
            index_usage_summary AS [Usage Stats], 
            index_op_stats AS [Op Stats],
            index_size_summary AS [Size],
			partition_compression_detail AS [Compression Type],
            index_lock_wait_summary AS [Lock Waits],
            is_referenced_by_foreign_key AS [Referenced by FK?],
            FKs_covered_by_index AS [FK Covered by Index?],
            last_user_seek AS [Last User Seek],
            last_user_scan AS [Last User Scan],
            last_user_lookup AS [Last User Lookup],
            last_user_update AS [Last User Write],
            create_date AS [Created],
            modify_date AS [Last Modified],
            create_tsql AS [Create TSQL]
    FROM table_mode_cte
    ORDER BY display_order ASC, key_column_names ASC
    OPTION    ( RECOMPILE );                        

    IF (SELECT TOP 1 [object_id] FROM    #MissingIndexes mi) IS NOT NULL
    BEGIN  
        SELECT  N'Missing index.' AS Finding ,
                N'http://BrentOzar.com/go/Indexaphobia' AS URL ,
                mi.[statement] + 
                ' Est. Benefit: '
                    + CASE WHEN magic_benefit_number &gt;= 922337203685477 THEN '&gt;= 922,337,203,685,477'
                    ELSE REPLACE(CONVERT(NVARCHAR(256),CAST(CAST(
                                        (magic_benefit_number/@DaysUptime)
                                        AS BIGINT) AS MONEY), 1), '.00', '')
                    END AS [Estimated Benefit],
                missing_index_details AS [Missing Index Request] ,
                index_estimated_impact AS [Estimated Impact],
                create_tsql AS [Create TSQL]
        FROM    #MissingIndexes mi
        WHERE   [object_id] = @ObjectID
                /* Minimum benefit threshold = 100k/day of uptime */
        AND (magic_benefit_number/@DaysUptime) &gt;= 100000
        ORDER BY magic_benefit_number DESC
        OPTION    ( RECOMPILE );
    END       
    ELSE     
    SELECT 'No missing indexes.' AS finding;

    SELECT     
        column_name AS [Column Name],
        (SELECT COUNT(*)  
            FROM #IndexColumns c2 
            WHERE c2.column_name=c.column_name
            AND c2.key_ordinal IS NOT NULL)
        + CASE WHEN c.index_id = 1 AND c.key_ordinal IS NOT NULL THEN
            -1+ (SELECT COUNT(DISTINCT index_id)
            FROM #IndexColumns c3
            WHERE c3.index_id NOT IN (0,1))
            ELSE 0 END
                AS [Found In],
        system_type_name + 
            CASE max_length WHEN -1 THEN N' (max)' ELSE
                CASE  
                    WHEN system_type_name IN (N'char',N'nchar',N'binary',N'varbinary') THEN N' (' + CAST(max_length AS NVARCHAR(20)) + N')' 
                    WHEN system_type_name IN (N'varchar',N'nvarchar') THEN N' (' + CAST(max_length/2 AS NVARCHAR(20)) + N')' 
                    ELSE '' 
                END
            END
            AS [Type],
        CASE is_computed WHEN 1 THEN 'yes' ELSE '' END AS [Computed?],
        max_length AS [Length (max bytes)],
        [precision] AS [Prec],
        [scale] AS [Scale],
        CASE is_nullable WHEN 1 THEN 'yes' ELSE '' END AS [Nullable?],
        CASE is_identity WHEN 1 THEN 'yes' ELSE '' END AS [Identity?],
        CASE is_replicated WHEN 1 THEN 'yes' ELSE '' END AS [Replicated?],
        CASE is_sparse WHEN 1 THEN 'yes' ELSE '' END AS [Sparse?],
        CASE is_filestream WHEN 1 THEN 'yes' ELSE '' END AS [Filestream?],
        collation_name AS [Collation]
    FROM #IndexColumns AS c
    WHERE index_id IN (0,1);

    IF (SELECT TOP 1 parent_object_id FROM #ForeignKeys) IS NOT NULL
    BEGIN
        SELECT [database_name] + N':' + parent_object_name + N': ' + foreign_key_name AS [Foreign Key],
            parent_fk_columns AS [Foreign Key Columns],
            referenced_object_name AS [Referenced Table],
            referenced_fk_columns AS [Referenced Table Columns],
            is_disabled AS [Is Disabled?],
            is_not_trusted AS [Not Trusted?],
            is_not_for_replication [Not for Replication?],
            [update_referential_action_desc] AS [Cascading Updates?],
            [delete_referential_action_desc] AS [Cascading Deletes?]
        FROM #ForeignKeys
        ORDER BY [Foreign Key]
        OPTION    ( RECOMPILE );
    END
    ELSE
    SELECT 'No foreign keys.' AS finding;
END 

--If @TableName is NOT specified...
--Act based on the @Mode and @Filter. (@Filter applies only when @Mode=0 "diagnose")
ELSE
BEGIN;
    IF @Mode IN (0, 4) /* DIAGNOSE*/
    BEGIN;
        RAISERROR(N'@Mode=0 or 4, we are diagnosing.', 0,1) WITH NOWAIT;

        ----------------------------------------
        --Multiple Index Personalities: Check_id 0-10
        ----------------------------------------
        BEGIN;

        --SELECT    [object_id], key_column_names, database_id
        --                   FROM        #IndexSanity
        --                   WHERE  index_type IN (1,2) /* Clustered, NC only*/
        --                        AND is_hypothetical = 0
        --                        AND is_disabled = 0
        --                   GROUP BY    [object_id], key_column_names, database_id
        --                   HAVING    COUNT(*) &gt; 1


        RAISERROR('check_id 1: Duplicate keys', 0,1) WITH NOWAIT;
            WITH    duplicate_indexes
                      AS ( SELECT    [object_id], key_column_names, database_id
                           FROM        #IndexSanity
                           WHERE  index_type IN (1,2) /* Clustered, NC only*/
                                AND is_hypothetical = 0
                                AND is_disabled = 0
								AND is_primary_key = 0
                           GROUP BY    [object_id], key_column_names, database_id
                           HAVING    COUNT(*) &gt; 1)
                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
                        SELECT    1 AS check_id, 
                                ip.index_sanity_id,
                                50 AS Priority,
                                'Multiple Index Personalities' AS findings_group,
                                'Duplicate keys' AS finding,
                                [database_name] AS [Database Name],
                                N'http://BrentOzar.com/go/duplicateindex' AS URL,
                                N'Index Name: ' + ip.index_name + N' Table Name: ' + ip.db_schema_object_name AS details,
                                ip.index_definition, 
                                ip.secret_columns, 
                                ip.index_usage_summary,
                                ips.index_size_summary
                        FROM    duplicate_indexes di
                                JOIN #IndexSanity ip ON di.[object_id] = ip.[object_id]
                                                         AND ip.database_id = di.database_id
                                                         AND di.key_column_names = ip.key_column_names
                                JOIN #IndexSanitySize ips ON ip.index_sanity_id = ips.index_sanity_id AND ip.database_id = ips.database_id
                        /* WHERE clause limits to only @ThresholdMB or larger duplicate indexes when getting all databases or using PainRelief mode */
                        WHERE ips.total_reserved_MB &gt;= CASE WHEN (@GetAllDatabases = 1 OR @Mode = 0) THEN @ThresholdMB ELSE ips.total_reserved_MB END
						AND ip.is_primary_key = 0
                        ORDER BY ip.object_id, ip.key_column_names_with_sort_order    
                OPTION    ( RECOMPILE );

        RAISERROR('check_id 2: Keys w/ identical leading columns.', 0,1) WITH NOWAIT;
            WITH    borderline_duplicate_indexes
                      AS ( SELECT DISTINCT [object_id], first_key_column_name, key_column_names,
                                    COUNT([object_id]) OVER ( PARTITION BY [object_id], first_key_column_name ) AS number_dupes
                           FROM        #IndexSanity
                           WHERE index_type IN (1,2) /* Clustered, NC only*/
                            AND is_hypothetical=0
                            AND is_disabled=0
							AND is_primary_key = 0)
                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
                        SELECT    2 AS check_id, 
                                ip.index_sanity_id,
                                60 AS Priority,
                                'Multiple Index Personalities' AS findings_group,
                                'Borderline duplicate keys' AS finding,
                                [database_name] AS [Database Name],
                                N'http://BrentOzar.com/go/duplicateindex' AS URL,
                                ip.db_schema_object_indexid AS details, 
                                ip.index_definition, 
                                ip.secret_columns,
                                ip.index_usage_summary,
                                ips.index_size_summary
                        FROM    #IndexSanity AS ip 
                        JOIN #IndexSanitySize ips ON ip.index_sanity_id = ips.index_sanity_id
                        WHERE EXISTS (
                            SELECT di.[object_id]
                            FROM borderline_duplicate_indexes AS di
                            WHERE di.[object_id] = ip.[object_id] AND
                                di.first_key_column_name = ip.first_key_column_name AND
                                di.key_column_names &lt;&gt; ip.key_column_names AND
                                di.number_dupes &gt; 1    
                        )
						AND ip.is_primary_key = 0
                        /* WHERE clause skips near-duplicate indexes when getting all databases or using PainRelief mode */
                        AND NOT (@GetAllDatabases = 1 OR @Mode = 0)
                                                
                        ORDER BY ip.[schema_name], ip.[object_name], ip.key_column_names, ip.include_column_names
            OPTION    ( RECOMPILE );

        END
        ----------------------------------------
        --Aggressive Indexes: Check_id 10-19
        ----------------------------------------
        BEGIN;

        RAISERROR(N'check_id 11: Total lock wait time &gt; 5 minutes (row + page) with long average waits', 0,1) WITH NOWAIT;
                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
                SELECT    11 AS check_id, 
                        i.index_sanity_id,
                        10 AS Priority,
                        N'Aggressive Indexes' AS findings_group,
                        N'Total lock wait time &gt; 5 minutes (row + page) with long average waits' AS finding, 
                        [database_name] AS [Database Name],
                        N'http://BrentOzar.com/go/AggressiveIndexes' AS URL,
                        i.db_schema_object_indexid + N': ' +
                            sz.index_lock_wait_summary AS details, 
                        i.index_definition,
                        i.secret_columns,
                        i.index_usage_summary,
                        sz.index_size_summary
                FROM    #IndexSanity AS i
                JOIN #IndexSanitySize AS sz ON i.index_sanity_id = sz.index_sanity_id
                WHERE    (total_row_lock_wait_in_ms + total_page_lock_wait_in_ms) &gt; 300000
				AND (sz.avg_page_lock_wait_in_ms + sz.avg_row_lock_wait_in_ms) &gt; 5000
                OPTION    ( RECOMPILE );

        RAISERROR(N'check_id 12: Total lock wait time &gt; 5 minutes (row + page) with short average waits', 0,1) WITH NOWAIT;
                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
                SELECT    12 AS check_id, 
                        i.index_sanity_id,
                        10 AS Priority,
                        N'Aggressive Indexes' AS findings_group,
                        N'Total lock wait time &gt; 5 minutes (row + page) with short average waits' AS finding, 
                        [database_name] AS [Database Name],
                        N'http://BrentOzar.com/go/AggressiveIndexes' AS URL,
                        i.db_schema_object_indexid + N': ' +
                            sz.index_lock_wait_summary AS details, 
                        i.index_definition,
                        i.secret_columns,
                        i.index_usage_summary,
                        sz.index_size_summary
                FROM    #IndexSanity AS i
                JOIN #IndexSanitySize AS sz ON i.index_sanity_id = sz.index_sanity_id
                WHERE    (total_row_lock_wait_in_ms + total_page_lock_wait_in_ms) &gt; 300000
				AND (sz.avg_page_lock_wait_in_ms + sz.avg_row_lock_wait_in_ms) &lt; 5000
                OPTION    ( RECOMPILE );

        END

        ---------------------------------------- 
        --Index Hoarder: Check_id 20-29
        ----------------------------------------
        BEGIN
            RAISERROR(N'check_id 20: &gt;=7 NC indexes on any given table. Yes, 7 is an arbitrary number.', 0,1) WITH NOWAIT;
                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
                        SELECT    20 AS check_id, 
                                MAX(i.index_sanity_id) AS index_sanity_id, 
                                100 AS Priority,
                                'Index Hoarder' AS findings_group,
                                'Many NC indexes on a single table' AS finding,
                                [database_name] AS [Database Name],
                                N'http://BrentOzar.com/go/IndexHoarder' AS URL,
                                CAST (COUNT(*) AS NVARCHAR(30)) + ' NC indexes on ' + i.db_schema_object_name AS details,
                                i.db_schema_object_name + ' (' + CAST (COUNT(*) AS NVARCHAR(30)) + ' indexes)' AS index_definition,
                                '' AS secret_columns,
                                REPLACE(CONVERT(NVARCHAR(30),CAST(SUM(total_reads) AS MONEY), 1), N'.00', N'') + N' reads (ALL); '
                                    + REPLACE(CONVERT(NVARCHAR(30),CAST(SUM(user_updates) AS MONEY), 1), N'.00', N'') + N' writes (ALL); ',
                                REPLACE(CONVERT(NVARCHAR(30),CAST(MAX(total_rows) AS MONEY), 1), N'.00', N'') + N' rows (MAX)'
                                    + CASE WHEN SUM(total_reserved_MB) &gt; 1024 THEN 
                                        N'; ' + CAST(CAST(SUM(total_reserved_MB)/1024. AS NUMERIC(29,1)) AS NVARCHAR(30)) + 'GB (ALL)'
                                    WHEN SUM(total_reserved_MB) &gt; 0 THEN
                                        N'; ' + CAST(CAST(SUM(total_reserved_MB) AS NUMERIC(29,1)) AS NVARCHAR(30)) + 'MB (ALL)'
                                    ELSE ''
                                    END AS index_size_summary
                        FROM    #IndexSanity i
                        JOIN #IndexSanitySize ip ON i.index_sanity_id = ip.index_sanity_id
                        WHERE    index_id NOT IN ( 0, 1 )
                        AND NOT (@GetAllDatabases = 1 OR @Mode = 0)
                        GROUP BY db_schema_object_name, [i].[database_name]
                        HAVING    COUNT(*) &gt;= 7
                        ORDER BY i.db_schema_object_name DESC  OPTION    ( RECOMPILE );

            IF @Filter = 1 /*@Filter=1 is "ignore unusued" */
            BEGIN
                RAISERROR(N'Skipping checks on unused indexes (21 and 22) because @Filter=1', 0,1) WITH NOWAIT;
            END
            ELSE /*Otherwise, go ahead and do the checks*/
            BEGIN
                RAISERROR(N'check_id 21: &gt;=5 percent of indexes are unused. Yes, 5 is an arbitrary number.', 0,1) WITH NOWAIT;
                    DECLARE @percent_NC_indexes_unused NUMERIC(29,1);
                    DECLARE @NC_indexes_unused_reserved_MB NUMERIC(29,1);

                    SELECT    @percent_NC_indexes_unused =( 100.00 * SUM(CASE    WHEN total_reads = 0 THEN 1
                                                ELSE 0
                                           END) ) / COUNT(*) ,
                            @NC_indexes_unused_reserved_MB = SUM(CASE WHEN total_reads = 0 THEN sz.total_reserved_MB
                                     ELSE 0
                                END) 
                    FROM    #IndexSanity i
                    JOIN    #IndexSanitySize sz ON i.index_sanity_id = sz.index_sanity_id
                    WHERE    index_id NOT IN ( 0, 1 ) 
                            AND i.is_unique = 0
							/*Skipping tables created in the last week, or modified in past 2 days*/
							AND	i.create_date &gt;= DATEADD(dd,-7,GETDATE()) 
							AND i.modify_date &gt; DATEADD(dd,-2,GETDATE()) 
                    OPTION    ( RECOMPILE );

                IF @percent_NC_indexes_unused &gt;= 5 
                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
                            SELECT    21 AS check_id, 
                                    MAX(i.index_sanity_id) AS index_sanity_id, 
                                    150 AS Priority,
                                    N'Index Hoarder' AS findings_group,
                                    N'More than 5 percent NC indexes are unused' AS finding,
                                    [database_name] AS [Database Name],
                                    N'http://BrentOzar.com/go/IndexHoarder' AS URL,
                                    CAST (@percent_NC_indexes_unused AS NVARCHAR(30)) + N' percent NC indexes (' + CAST(COUNT(*) AS NVARCHAR(10)) + N') unused. ' +
                                    N'These take up ' + CAST (@NC_indexes_unused_reserved_MB AS NVARCHAR(30)) + N'MB of space.' AS details,
                                    i.database_name + ' (' + CAST (COUNT(*) AS NVARCHAR(30)) + N' indexes)' AS index_definition,
                                    '' AS secret_columns, 
                                    CAST(SUM(total_reads) AS NVARCHAR(256)) + N' reads (ALL); '
                                        + CAST(SUM([user_updates]) AS NVARCHAR(256)) + N' writes (ALL)' AS index_usage_summary,
                                
                                    REPLACE(CONVERT(NVARCHAR(30),CAST(MAX([total_rows]) AS MONEY), 1), '.00', '') + N' rows (MAX)'
                                        + CASE WHEN SUM(total_reserved_MB) &gt; 1024 THEN 
                                            N'; ' + CAST(CAST(SUM(total_reserved_MB)/1024. AS NUMERIC(29,1)) AS NVARCHAR(30)) + 'GB (ALL)'
                                        WHEN SUM(total_reserved_MB) &gt; 0 THEN
                                            N'; ' + CAST(CAST(SUM(total_reserved_MB) AS NUMERIC(29,1)) AS NVARCHAR(30)) + 'MB (ALL)'
                                        ELSE ''
                                        END AS index_size_summary
                            FROM    #IndexSanity i
                            JOIN    #IndexSanitySize sz ON i.index_sanity_id = sz.index_sanity_id
                            WHERE    index_id NOT IN ( 0, 1 )
                                    AND i.is_unique = 0
                                    AND total_reads = 0
                                    AND NOT (@GetAllDatabases = 1 OR @Mode = 0)
									/*Skipping tables created in the last week, or modified in past 2 days*/
									AND	i.create_date &gt;= DATEADD(dd,-7,GETDATE()) 
									AND i.modify_date &gt; DATEADD(dd,-2,GETDATE())
                            GROUP BY i.database_name 
                    OPTION    ( RECOMPILE );

                RAISERROR(N'check_id 22: NC indexes with 0 reads. (Borderline)', 0,1) WITH NOWAIT;
                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
                        SELECT    22 AS check_id, 
                                i.index_sanity_id,
                                150 AS Priority,
                                N'Index Hoarder' AS findings_group,
                                N'Unused NC index' AS finding, 
                                [database_name] AS [Database Name],
                                N'http://BrentOzar.com/go/IndexHoarder' AS URL,
                                N'0 reads: ' + i.db_schema_object_indexid AS details, 
                                i.index_definition, 
                                i.secret_columns, 
                                i.index_usage_summary,
                                sz.index_size_summary
                        FROM    #IndexSanity AS i
                        JOIN    #IndexSanitySize AS sz ON i.index_sanity_id = sz.index_sanity_id
                        WHERE    i.total_reads=0
                                AND i.index_id NOT IN (0,1) /*NCs only*/
                                AND i.is_unique = 0
                                AND sz.total_reserved_MB &gt;= CASE WHEN (@GetAllDatabases = 1 OR @Mode = 0) THEN @ThresholdMB ELSE sz.total_reserved_MB END
                        ORDER BY i.db_schema_object_indexid
                        OPTION    ( RECOMPILE );
            END /*end checks only run when @Filter &lt;&gt; 1*/

            RAISERROR(N'check_id 23: Indexes with 7 or more columns. (Borderline)', 0,1) WITH NOWAIT;
                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
                    SELECT    23 AS check_id, 
                            i.index_sanity_id,
                            150 AS Priority, 
                            N'Index Hoarder' AS findings_group,
                            N'Borderline: Wide indexes (7 or more columns)' AS finding, 
                            [database_name] AS [Database Name],
                            N'http://BrentOzar.com/go/IndexHoarder' AS URL,
                            CAST(count_key_columns + count_included_columns AS NVARCHAR(10)) + ' columns on '
                            + i.db_schema_object_indexid AS details, i.index_definition, 
                            i.secret_columns, 
                            i.index_usage_summary,
                            sz.index_size_summary
                    FROM    #IndexSanity AS i
                    JOIN    #IndexSanitySize AS sz ON i.index_sanity_id = sz.index_sanity_id
                    WHERE    ( count_key_columns + count_included_columns ) &gt;= 7
                            AND NOT (@GetAllDatabases = 1 OR @Mode = 0)
                    OPTION    ( RECOMPILE );

            RAISERROR(N'check_id 24: Wide clustered indexes (&gt; 3 columns or &gt; 16 bytes).', 0,1) WITH NOWAIT;
                WITH count_columns AS (
                            SELECT [object_id],
                                SUM(CASE max_length WHEN -1 THEN 0 ELSE max_length END) AS sum_max_length
                            FROM #IndexColumns ic
                            WHERE index_id IN (1,0) /*Heap or clustered only*/
                            AND key_ordinal &gt; 0
                            GROUP BY object_id
                            )
                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
                        SELECT    24 AS check_id, 
                                i.index_sanity_id, 
                                150 AS Priority,
                                N'Index Hoarder' AS findings_group,
                                N'Wide clustered index (&gt; 3 columns OR &gt; 16 bytes)' AS finding,
                                [database_name] AS [Database Name],
                                N'http://BrentOzar.com/go/IndexHoarder' AS URL,
                                CAST (i.count_key_columns AS NVARCHAR(10)) + N' columns with potential size of '
                                    + CAST(cc.sum_max_length AS NVARCHAR(10))
                                    + N' bytes in clustered index:' + i.db_schema_object_name 
                                    + N'. ' + 
                                        (SELECT CAST(COUNT(*) AS NVARCHAR(23)) FROM #IndexSanity i2 
                                        WHERE i2.[object_id]=i.[object_id] AND i2.index_id &lt;&gt; 1
                                        AND i2.is_disabled=0 AND i2.is_hypothetical=0)
                                        + N' NC indexes on the table.'
                                    AS details,
                                i.index_definition,
                                secret_columns, 
                                i.index_usage_summary,
                                ip.index_size_summary
                        FROM    #IndexSanity i
                        JOIN    #IndexSanitySize ip ON i.index_sanity_id = ip.index_sanity_id
                        JOIN    count_columns AS cc ON i.[object_id]=cc.[object_id]    
                        WHERE    index_id = 1 /* clustered only */
                                AND NOT (@GetAllDatabases = 1 OR @Mode = 0)
                                AND 
                                    (count_key_columns &gt; 3 /*More than three key columns.*/
                                    OR cc.sum_max_length &gt; 16 /*More than 16 bytes in key */)
									AND i.is_CX_columnstore = 0
                        ORDER BY i.db_schema_object_name DESC OPTION    ( RECOMPILE );

            RAISERROR(N'check_id 25: Addicted to nullable columns.', 0,1) WITH NOWAIT;
                WITH count_columns AS (
                            SELECT [object_id],
                                SUM(CASE is_nullable WHEN 1 THEN 0 ELSE 1 END) AS non_nullable_columns,
                                COUNT(*) AS total_columns
                            FROM #IndexColumns ic
                            WHERE index_id IN (1,0) /*Heap or clustered only*/
                            GROUP BY object_id
                            )
                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
                        SELECT    25 AS check_id, 
                                i.index_sanity_id, 
                                200 AS Priority,
                                N'Index Hoarder' AS findings_group,
                                N'Addicted to nulls' AS finding,
                                [database_name] AS [Database Name],
                                N'http://BrentOzar.com/go/IndexHoarder' AS URL,
                                i.db_schema_object_name 
                                    + N' allows null in ' + CAST((total_columns-non_nullable_columns) AS NVARCHAR(10))
                                    + N' of ' + CAST(total_columns AS NVARCHAR(10))
                                    + N' columns.' AS details,
                                i.index_definition,
                                secret_columns, 
                                ISNULL(i.index_usage_summary,''),
                                ISNULL(ip.index_size_summary,'')
                        FROM    #IndexSanity i
                        JOIN    #IndexSanitySize ip ON i.index_sanity_id = ip.index_sanity_id
                        JOIN    count_columns AS cc ON i.[object_id]=cc.[object_id]
                        WHERE    i.index_id IN (1,0)
                        AND NOT (@GetAllDatabases = 1 OR @Mode = 0)
                            AND cc.non_nullable_columns &lt; 2
                            AND cc.total_columns &gt; 3
                        ORDER BY i.db_schema_object_name DESC OPTION    ( RECOMPILE );

            RAISERROR(N'check_id 26: Wide tables (35+ cols or &gt; 2000 non-LOB bytes).', 0,1) WITH NOWAIT;
                WITH count_columns AS (
                            SELECT [object_id],
                                SUM(CASE max_length WHEN -1 THEN 1 ELSE 0 END) AS count_lob_columns,
                                SUM(CASE max_length WHEN -1 THEN 0 ELSE max_length END) AS sum_max_length,
                                COUNT(*) AS total_columns
                            FROM #IndexColumns ic
                            WHERE index_id IN (1,0) /*Heap or clustered only*/
                            GROUP BY object_id
                            )
                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
                        SELECT    26 AS check_id, 
                                i.index_sanity_id, 
                                150 AS Priority,
                                N'Index Hoarder' AS findings_group,
                                N'Wide tables: 35+ cols or &gt; 2000 non-LOB bytes' AS finding,
                                [database_name] AS [Database Name],
                                N'http://BrentOzar.com/go/IndexHoarder' AS URL,
                                i.db_schema_object_name 
                                    + N' has ' + CAST((total_columns) AS NVARCHAR(10))
                                    + N' total columns with a max possible width of ' + CAST(sum_max_length AS NVARCHAR(10))
                                    + N' bytes.' +
                                    CASE WHEN count_lob_columns &gt; 0 THEN CAST((count_lob_columns) AS NVARCHAR(10))
                                        + ' columns are LOB types.' ELSE ''
                                    END
                                        AS details,
                                i.index_definition,
                                secret_columns, 
                                ISNULL(i.index_usage_summary,''),
                                ISNULL(ip.index_size_summary,'')
                        FROM    #IndexSanity i
                        JOIN    #IndexSanitySize ip ON i.index_sanity_id = ip.index_sanity_id
                        JOIN    count_columns AS cc ON i.[object_id]=cc.[object_id]
                        WHERE    i.index_id IN (1,0)
                        AND NOT (@GetAllDatabases = 1 OR @Mode = 0)
                            AND 
                            (cc.total_columns &gt;= 35 OR
                            cc.sum_max_length &gt;= 2000)
                        ORDER BY i.db_schema_object_name DESC OPTION    ( RECOMPILE );
                    
            RAISERROR(N'check_id 27: Addicted to strings.', 0,1) WITH NOWAIT;
                WITH count_columns AS (
                            SELECT [object_id],
                                SUM(CASE WHEN system_type_name IN ('varchar','nvarchar','char') OR max_length=-1 THEN 1 ELSE 0 END) AS string_or_LOB_columns,
                                COUNT(*) AS total_columns
                            FROM #IndexColumns ic
                            WHERE index_id IN (1,0) /*Heap or clustered only*/
                            GROUP BY object_id
                            )
                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
                        SELECT    27 AS check_id, 
                                i.index_sanity_id, 
                                200 AS Priority,
                                N'Index Hoarder' AS findings_group,
                                N'Addicted to strings' AS finding,
                                [database_name] AS [Database Name],
                                N'http://BrentOzar.com/go/IndexHoarder' AS URL,
                                i.db_schema_object_name 
                                    + N' uses string or LOB types for ' + CAST((string_or_LOB_columns) AS NVARCHAR(10))
                                    + N' of ' + CAST(total_columns AS NVARCHAR(10))
                                    + N' columns. Check if data types are valid.' AS details,
                                i.index_definition,
                                secret_columns, 
                                ISNULL(i.index_usage_summary,''),
                                ISNULL(ip.index_size_summary,'')
                        FROM    #IndexSanity i
                        JOIN    #IndexSanitySize ip ON i.index_sanity_id = ip.index_sanity_id
                        JOIN    count_columns AS cc ON i.[object_id]=cc.[object_id]
                        CROSS APPLY (SELECT cc.total_columns - string_or_LOB_columns AS non_string_or_lob_columns) AS calc1
                        WHERE    i.index_id IN (1,0)
                        AND NOT (@GetAllDatabases = 1 OR @Mode = 0)
                            AND calc1.non_string_or_lob_columns &lt;= 1
                            AND cc.total_columns &gt; 3
                        ORDER BY i.db_schema_object_name DESC OPTION    ( RECOMPILE );

            RAISERROR(N'check_id 28: Non-unique clustered index.', 0,1) WITH NOWAIT;
                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
                        SELECT    28 AS check_id, 
                                i.index_sanity_id, 
                                100 AS Priority,
                                N'Index Hoarder' AS findings_group,
                                N'Non-Unique clustered index' AS finding,
                                [database_name] AS [Database Name],
                                N'http://BrentOzar.com/go/IndexHoarder' AS URL,
                                N'Uniquifiers will be required! Clustered index: ' + i.db_schema_object_name 
                                    + N' and all NC indexes. ' + 
                                        (SELECT CAST(COUNT(*) AS NVARCHAR(23)) FROM #IndexSanity i2 
                                        WHERE i2.[object_id]=i.[object_id] AND i2.index_id &lt;&gt; 1
                                        AND i2.is_disabled=0 AND i2.is_hypothetical=0)
                                        + N' NC indexes on the table.'
                                    AS details,
                                i.index_definition,
                                secret_columns, 
                                i.index_usage_summary,
                                ip.index_size_summary
                        FROM    #IndexSanity i
                        JOIN    #IndexSanitySize ip ON i.index_sanity_id = ip.index_sanity_id
                        WHERE    index_id = 1 /* clustered only */
                        AND NOT (@GetAllDatabases = 1 OR @Mode = 0)
                                AND is_unique=0 /* not unique */
                                AND is_CX_columnstore=0 /* not a clustered columnstore-- no unique option on those */
                        ORDER BY i.db_schema_object_name DESC OPTION    ( RECOMPILE )



        END
         ----------------------------------------
        --Feature-Phobic Indexes: Check_id 30-39
        ---------------------------------------- 
        BEGIN
            RAISERROR(N'check_id 30: No indexes with includes', 0,1) WITH NOWAIT;

            DECLARE    @number_indexes_with_includes INT;
            DECLARE    @percent_indexes_with_includes NUMERIC(10, 1);

            SELECT    @number_indexes_with_includes = SUM(CASE WHEN count_included_columns &gt; 0 THEN 1 ELSE 0    END),
                    @percent_indexes_with_includes = 100.* 
                        SUM(CASE WHEN count_included_columns &gt; 0 THEN 1 ELSE 0 END) / ( 1.0 * COUNT(*) )
            FROM    #IndexSanity;

            IF @number_indexes_with_includes = 0 AND NOT (@GetAllDatabases = 1 OR @Mode = 0)
                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
                        SELECT    30 AS check_id, 
                                NULL AS index_sanity_id, 
                                250 AS Priority,
                                N'Feature-Phobic Indexes' AS findings_group,
                                N'No indexes use includes' AS finding, 'http://BrentOzar.com/go/IndexFeatures' AS URL,
                                N'No indexes use includes' AS details,
                                @DatabaseName + N' (Entire database)' AS index_definition, 
                                N'' AS secret_columns, 
                                N'N/A' AS index_usage_summary, 
                                N'N/A' AS index_size_summary OPTION    ( RECOMPILE );

            RAISERROR(N'check_id 31: &lt; 3 percent of indexes have includes', 0,1) WITH NOWAIT;
            IF @percent_indexes_with_includes &lt;= 3 AND @number_indexes_with_includes &gt; 0 AND NOT (@GetAllDatabases = 1 OR @Mode = 0)
                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
                        SELECT    31 AS check_id,
                                NULL AS index_sanity_id, 
                                150 AS Priority,
                                N'Feature-Phobic Indexes' AS findings_group,
                                N'Borderline: Includes are used in &lt; 3% of indexes' AS findings,
                                @DatabaseName AS [Database Name],
                                N'http://BrentOzar.com/go/IndexFeatures' AS URL,
                                N'Only ' + CAST(@percent_indexes_with_includes AS NVARCHAR(10)) + '% of indexes have includes' AS details, 
                                N'Entire database' AS index_definition, 
                                N'' AS secret_columns,
                                N'N/A' AS index_usage_summary, 
                                N'N/A' AS index_size_summary OPTION    ( RECOMPILE );

            RAISERROR(N'check_id 32: filtered indexes and indexed views', 0,1) WITH NOWAIT;
            DECLARE @count_filtered_indexes INT;
            DECLARE @count_indexed_views INT;

                SELECT    @count_filtered_indexes=COUNT(*)
                FROM    #IndexSanity
                WHERE    filter_definition &lt;&gt; '' OPTION    ( RECOMPILE );

                SELECT    @count_indexed_views=COUNT(*)
                FROM    #IndexSanity AS i
                        JOIN #IndexSanitySize AS sz ON i.index_sanity_id = sz.index_sanity_id
                WHERE    is_indexed_view = 1 OPTION    ( RECOMPILE );

            IF @count_filtered_indexes = 0 AND @count_indexed_views=0 AND NOT (@GetAllDatabases = 1 OR @Mode = 0)
                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
                        SELECT    32 AS check_id, 
                                NULL AS index_sanity_id,
                                250 AS Priority,
                                N'Feature-Phobic Indexes' AS findings_group,
                                N'Borderline: No filtered indexes or indexed views exist' AS finding, 
                                @DatabaseName AS [Database Name],
                                N'http://BrentOzar.com/go/IndexFeatures' AS URL,
                                N'These are NOT always needed-- but do you know when you would use them?' AS details,
                                @DatabaseName + N' (Entire database)' AS index_definition, 
                                N'' AS secret_columns,
                                N'N/A' AS index_usage_summary, 
                                N'N/A' AS index_size_summary OPTION    ( RECOMPILE );
        END;

        RAISERROR(N'check_id 33: Potential filtered indexes based on column names.', 0,1) WITH NOWAIT;

                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
        SELECT    33 AS check_id, 
                i.index_sanity_id AS index_sanity_id,
                250 AS Priority,
                N'Feature-Phobic Indexes' AS findings_group,
                N'Potential filtered index (based on column name)' AS finding, 
                [database_name] AS [Database Name],
                N'http://BrentOzar.com/go/IndexFeatures' AS URL,
                N'A column name in this index suggests it might be a candidate for filtering (is%, %archive%, %active%, %flag%)' AS details,
                i.index_definition, 
                i.secret_columns,
                i.index_usage_summary, 
                sz.index_size_summary
        FROM #IndexColumns ic 
        JOIN #IndexSanity i ON 
            ic.[object_id]=i.[object_id] AND
            ic.[index_id]=i.[index_id] AND
            i.[index_id] &gt; 1 /* non-clustered index */
        JOIN    #IndexSanitySize AS sz ON i.index_sanity_id = sz.index_sanity_id
        WHERE (column_name LIKE 'is%'
            OR column_name LIKE '%archive%'
            OR column_name LIKE '%active%'
            OR column_name LIKE '%flag%')
            AND NOT (@GetAllDatabases = 1 OR @Mode = 0)
        OPTION    ( RECOMPILE );
        
         ----------------------------------------
        --Self Loathing Indexes : Check_id 40-49
        ----------------------------------------
        BEGIN
        
            RAISERROR(N'check_id 40: Fillfactor in nonclustered 80 percent or less', 0,1) WITH NOWAIT;
                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
                    SELECT    40 AS check_id, 
                            i.index_sanity_id,
                            100 AS Priority,
                            N'Self Loathing Indexes' AS findings_group,
                            N'Low Fill Factor: nonclustered index' AS finding, 
                            [database_name] AS [Database Name],
                            N'http://BrentOzar.com/go/SelfLoathing' AS URL,
                            CAST(fill_factor AS NVARCHAR(10)) + N'% fill factor on ' + db_schema_object_indexid + N'. '+
                                CASE WHEN (last_user_update IS NULL OR user_updates &lt; 1)
                                THEN N'No writes have been made.'
                                ELSE
                                    N'Last write was ' +  CONVERT(NVARCHAR(16),last_user_update,121) + N' and ' + 
                                    CAST(user_updates AS NVARCHAR(25)) + N' updates have been made.'
                                END
                                AS details, 
                            i.index_definition,
                            i.secret_columns,
                            i.index_usage_summary,
                            sz.index_size_summary
                    FROM    #IndexSanity AS i
                    JOIN    #IndexSanitySize AS sz ON i.index_sanity_id = sz.index_sanity_id
                    WHERE    index_id &gt; 1
                    AND NOT (@GetAllDatabases = 1 OR @Mode = 0)
                    AND    fill_factor BETWEEN 1 AND 80 OPTION    ( RECOMPILE );

            RAISERROR(N'check_id 40: Fillfactor in clustered 80 percent or less', 0,1) WITH NOWAIT;
                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
                    SELECT    40 AS check_id, 
                            i.index_sanity_id,
                            100 AS Priority,
                            N'Self Loathing Indexes' AS findings_group,
                            N'Low Fill Factor: clustered index' AS finding, 
                            [database_name] AS [Database Name],
                            N'http://BrentOzar.com/go/SelfLoathing' AS URL,
                            N'Fill factor on ' + db_schema_object_indexid + N' is ' + CAST(fill_factor AS NVARCHAR(10)) + N'%. '+
                                CASE WHEN (last_user_update IS NULL OR user_updates &lt; 1)
                                THEN N'No writes have been made.'
                                ELSE
                                    N'Last write was ' +  CONVERT(NVARCHAR(16),last_user_update,121) + N' and ' + 
                                    CAST(user_updates AS NVARCHAR(25)) + N' updates have been made.'
                                END
                                AS details, 
                            i.index_definition,
                            i.secret_columns,
                            i.index_usage_summary,
                            sz.index_size_summary
                    FROM    #IndexSanity AS i
                    JOIN #IndexSanitySize AS sz ON i.index_sanity_id = sz.index_sanity_id
                    WHERE    index_id = 1
                    AND NOT (@GetAllDatabases = 1 OR @Mode = 0)
                    AND fill_factor BETWEEN 1 AND 80 OPTION    ( RECOMPILE );


            RAISERROR(N'check_id 41: Hypothetical indexes ', 0,1) WITH NOWAIT;
                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
                    SELECT    41 AS check_id, 
                            i.index_sanity_id,
                            150 AS Priority,
                            N'Self Loathing Indexes' AS findings_group,
                            N'Hypothetical Index' AS finding,
                            [database_name] AS [Database Name],
                            N'http://BrentOzar.com/go/SelfLoathing' AS URL,
                            N'Hypothetical Index: ' + db_schema_object_indexid AS details, 
                            i.index_definition,
                            i.secret_columns,
                            N'' AS index_usage_summary, 
                            N'' AS index_size_summary
                    FROM    #IndexSanity AS i
                    WHERE    is_hypothetical = 1 
                    AND NOT (@GetAllDatabases = 1 OR @Mode = 0)
                    OPTION    ( RECOMPILE );


            RAISERROR(N'check_id 42: Disabled indexes', 0,1) WITH NOWAIT;
            --Note: disabled NC indexes will have O rows in #IndexSanitySize!
                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
                    SELECT    42 AS check_id, 
                            index_sanity_id,
                            150 AS Priority,
                            N'Self Loathing Indexes' AS findings_group,
                            N'Disabled Index' AS finding, 
                            [database_name] AS [Database Name],
                            N'http://BrentOzar.com/go/SelfLoathing' AS URL,
                            N'Disabled Index:' + db_schema_object_indexid AS details, 
                            i.index_definition,
                            i.secret_columns,
                            i.index_usage_summary,
                            'DISABLED' AS index_size_summary
                    FROM    #IndexSanity AS i
                    WHERE    is_disabled = 1
                    AND NOT (@GetAllDatabases = 1 OR @Mode = 0)
                    OPTION    ( RECOMPILE );

            RAISERROR(N'check_id 43: Heaps with forwarded records or deletes', 0,1) WITH NOWAIT;
            WITH    heaps_cte
                      AS ( SELECT    [object_id], 
                                    SUM(forwarded_fetch_count) AS forwarded_fetch_count,
                                    SUM(leaf_delete_count) AS leaf_delete_count
                           FROM        #IndexPartitionSanity
                           GROUP BY    [object_id]
                           HAVING    SUM(forwarded_fetch_count) &gt; 0
                                    OR SUM(leaf_delete_count) &gt; 0)
                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
                        SELECT    43 AS check_id, 
                                i.index_sanity_id,
                                100 AS Priority,
                                N'Self Loathing Indexes' AS findings_group,
                                N'Heaps with forwarded records or deletes' AS finding, 
                                [database_name] AS [Database Name],
                                N'http://BrentOzar.com/go/SelfLoathing' AS URL,
                                CAST(h.forwarded_fetch_count AS NVARCHAR(256)) + ' forwarded fetches, '
                                + CAST(h.leaf_delete_count AS NVARCHAR(256)) + ' deletes against heap:'
                                + db_schema_object_indexid AS details, 
                                i.index_definition, 
                                i.secret_columns,
                                i.index_usage_summary,
                                sz.index_size_summary
                        FROM    #IndexSanity i
                        JOIN heaps_cte h ON i.[object_id] = h.[object_id]
                        JOIN #IndexSanitySize sz ON i.index_sanity_id = sz.index_sanity_id
                        WHERE    i.index_id = 0 
                        AND sz.total_reserved_MB &gt;= CASE WHEN NOT (@GetAllDatabases = 1 OR @Mode = 4) THEN @ThresholdMB ELSE sz.total_reserved_MB END
                OPTION    ( RECOMPILE );

            RAISERROR(N'check_id 44: Large Heaps with reads or writes.', 0,1) WITH NOWAIT;
            WITH    heaps_cte
                      AS ( SELECT    [object_id], SUM(forwarded_fetch_count) AS forwarded_fetch_count,
                                    SUM(leaf_delete_count) AS leaf_delete_count
                           FROM        #IndexPartitionSanity
                           GROUP BY    [object_id]
                           HAVING    SUM(forwarded_fetch_count) &gt; 0
                                    OR SUM(leaf_delete_count) &gt; 0)
                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
                        SELECT    44 AS check_id, 
                                i.index_sanity_id,
                                100 AS Priority,
                                N'Self Loathing Indexes' AS findings_group,
                                N'Large Active heap' AS finding, 
                                [database_name] AS [Database Name],
                                N'http://BrentOzar.com/go/SelfLoathing' AS URL,
                                N'Should this table be a heap? ' + db_schema_object_indexid AS details, 
                                i.index_definition, 
                                'N/A' AS secret_columns,
                                i.index_usage_summary,
                                sz.index_size_summary
                        FROM    #IndexSanity i
                        LEFT JOIN heaps_cte h ON i.[object_id] = h.[object_id]
                        JOIN #IndexSanitySize sz ON i.index_sanity_id = sz.index_sanity_id
                        WHERE    i.index_id = 0 
                                AND 
                                    (i.total_reads &gt; 0 OR i.user_updates &gt; 0)
								AND sz.total_rows &gt;= 100000
                                AND h.[object_id] IS NULL /*don't duplicate the prior check.*/
                                AND NOT (@GetAllDatabases = 1 OR @Mode = 0)
                OPTION    ( RECOMPILE );

            RAISERROR(N'check_id 45: Medium Heaps with reads or writes.', 0,1) WITH NOWAIT;
            WITH    heaps_cte
                      AS ( SELECT    [object_id], SUM(forwarded_fetch_count) AS forwarded_fetch_count,
                                    SUM(leaf_delete_count) AS leaf_delete_count
                           FROM        #IndexPartitionSanity
                           GROUP BY    [object_id]
                           HAVING    SUM(forwarded_fetch_count) &gt; 0
                                    OR SUM(leaf_delete_count) &gt; 0)
                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
                        SELECT    45 AS check_id, 
                                i.index_sanity_id,
                                100 AS Priority,
                                N'Self Loathing Indexes' AS findings_group,
                                N'Medium Active heap' AS finding, 
                                [database_name] AS [Database Name],
                                N'http://BrentOzar.com/go/SelfLoathing' AS URL,
                                N'Should this table be a heap? ' + db_schema_object_indexid AS details, 
                                i.index_definition, 
                                'N/A' AS secret_columns,
                                i.index_usage_summary,
                                sz.index_size_summary
                        FROM    #IndexSanity i
                        LEFT JOIN heaps_cte h ON i.[object_id] = h.[object_id]
                        JOIN #IndexSanitySize sz ON i.index_sanity_id = sz.index_sanity_id
                        WHERE    i.index_id = 0 
                                AND 
                                    (i.total_reads &gt; 0 OR i.user_updates &gt; 0)
								AND sz.total_rows &gt;= 10000 AND sz.total_rows &lt; 100000
                                AND h.[object_id] IS NULL /*don't duplicate the prior check.*/
                                AND NOT (@GetAllDatabases = 1 OR @Mode = 0)
                OPTION    ( RECOMPILE );

            RAISERROR(N'check_id 46: Small Heaps with reads or writes.', 0,1) WITH NOWAIT;
            WITH    heaps_cte
                      AS ( SELECT    [object_id], SUM(forwarded_fetch_count) AS forwarded_fetch_count,
                                    SUM(leaf_delete_count) AS leaf_delete_count
                           FROM        #IndexPartitionSanity
                           GROUP BY    [object_id]
                           HAVING    SUM(forwarded_fetch_count) &gt; 0
                                    OR SUM(leaf_delete_count) &gt; 0)
                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
                        SELECT    46 AS check_id, 
                                i.index_sanity_id,
                                100 AS Priority,
                                N'Self Loathing Indexes' AS findings_group,
                                N'Small Active heap' AS finding, 
                                [database_name] AS [Database Name],
                                N'http://BrentOzar.com/go/SelfLoathing' AS URL,
                                N'Should this table be a heap? ' + db_schema_object_indexid AS details, 
                                i.index_definition, 
                                'N/A' AS secret_columns,
                                i.index_usage_summary,
                                sz.index_size_summary
                        FROM    #IndexSanity i
                        LEFT JOIN heaps_cte h ON i.[object_id] = h.[object_id]
                        JOIN #IndexSanitySize sz ON i.index_sanity_id = sz.index_sanity_id
                        WHERE    i.index_id = 0 
                                AND 
                                    (i.total_reads &gt; 0 OR i.user_updates &gt; 0)
								AND sz.total_rows &lt; 10000
                                AND h.[object_id] IS NULL /*don't duplicate the prior check.*/
                                AND NOT (@GetAllDatabases = 1 OR @Mode = 0)
                OPTION    ( RECOMPILE );

				            RAISERROR(N'check_id 47: Heap with a Nonclustered Primary Key', 0,1) WITH NOWAIT;
                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
                        SELECT  47 AS check_id, 
                                i.index_sanity_id,
                                100 AS Priority,
                                N'Self Loathing Indexes' AS findings_group,
                                N'Heap with a Nonclustered Primary Key' AS finding, 
                                [database_name] AS [Database Name],
                                N'http://BrentOzar.com/go/SelfLoathing' AS URL,
								db_schema_object_indexid + N' is a HEAP with a Nonclustered Primary Key' AS details, 
                                i.index_definition, 
                                i.secret_columns,
                                i.index_usage_summary,
                                sz.index_size_summary
                        FROM    #IndexSanity i
                        JOIN #IndexSanitySize sz ON i.index_sanity_id = sz.index_sanity_id
                        WHERE    i.index_type = 2 AND i.is_primary_key = 1 AND i.secret_columns LIKE '%RID%'
                OPTION    ( RECOMPILE );

            END;
        ----------------------------------------
        --Indexaphobia
        --Missing indexes with value &gt;= 5 million: : Check_id 50-59
        ----------------------------------------
        BEGIN
            RAISERROR(N'check_id 50: Indexaphobia.', 0,1) WITH NOWAIT;
            WITH    index_size_cte
                      AS ( SELECT   i.database_id,
									i.[object_id], 
                                    MAX(i.index_sanity_id) AS index_sanity_id,
                                ISNULL (
                                    CAST(SUM(CASE WHEN index_id NOT IN (0,1) THEN 1 ELSE 0 END)
                                         AS NVARCHAR(30))+ N' NC indexes exist (' + 
                                    CASE WHEN SUM(CASE WHEN index_id NOT IN (0,1) THEN sz.total_reserved_MB ELSE 0 END) &gt; 1024
                                        THEN CAST(CAST(SUM(CASE WHEN index_id NOT IN (0,1) THEN sz.total_reserved_MB ELSE 0 END )/1024. 

                                            AS NUMERIC(29,1)) AS NVARCHAR(30)) + N'GB); ' 
                                        ELSE CAST(SUM(CASE WHEN index_id NOT IN (0,1) THEN sz.total_reserved_MB ELSE 0 END) 
                                            AS NVARCHAR(30)) + N'MB); '
                                    END + 
                                        CASE WHEN MAX(sz.[total_rows]) &gt;= 922337203685477 THEN '&gt;= 922,337,203,685,477'
                                        ELSE REPLACE(CONVERT(NVARCHAR(30),CAST(MAX(sz.[total_rows]) AS MONEY), 1), '.00', '') 
                                        END +
                                    + N' Estimated Rows;' 
                                ,N'') AS index_size_summary
                            FROM    #IndexSanity AS i
                            LEFT    JOIN #IndexSanitySize AS sz ON i.index_sanity_id = sz.index_sanity_id  AND i.database_id = sz.database_id
							WHERE i.is_hypothetical = 0
                                  AND i.is_disabled = 0
                           GROUP BY    i.database_id, i.[object_id])
                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               index_usage_summary, index_size_summary, create_tsql, more_info )
                        
                        SELECT check_id, t.index_sanity_id, t.check_id, t.findings_group, t.finding, t.[Database Name], t.URL, t.details, t.[definition],
                                index_estimated_impact, t.index_size_summary, create_tsql, more_info
                        FROM
                        (
                            SELECT  ROW_NUMBER() OVER (ORDER BY magic_benefit_number DESC) AS rownum,
                                50 AS check_id, 
                                sz.index_sanity_id,
                                10 AS Priority,
                                N'Indexaphobia' AS findings_group,
                                N'High value missing index' AS finding, 
                                [database_name] AS [Database Name],
                                N'http://BrentOzar.com/go/Indexaphobia' AS URL,
                                mi.[statement] + 
                                N' Est. benefit per day: ' + 
                                    CASE WHEN magic_benefit_number &gt;= 922337203685477 THEN '&gt;= 922,337,203,685,477'
                                    ELSE REPLACE(CONVERT(NVARCHAR(256),CAST(CAST(
                                    (magic_benefit_number/@DaysUptime)
                                     AS BIGINT) AS MONEY), 1), '.00', '') 
                                    END AS details,
                                missing_index_details AS [definition],
                                index_estimated_impact,
                                sz.index_size_summary,
                                mi.create_tsql,
                                mi.more_info,
                                magic_benefit_number
                        FROM    #MissingIndexes mi
                                LEFT JOIN index_size_cte sz ON mi.[object_id] = sz.object_id AND DB_ID(mi.database_name) = sz.database_id
                                        /* Minimum benefit threshold = 100k/day of uptime */
                        WHERE ( @Mode = 4 AND (magic_benefit_number/@DaysUptime) &gt;= 100000 ) OR (magic_benefit_number/@DaysUptime) &gt;= 100000
                        ) AS t
                        WHERE t.rownum &lt;= CASE WHEN (@Mode &lt;&gt; 4) THEN 20 ELSE t.rownum END
                        ORDER BY magic_benefit_number DESC


    END
         ----------------------------------------
        --Abnormal Psychology : Check_id 60-79
        ----------------------------------------
    BEGIN
            RAISERROR(N'check_id 60: XML indexes', 0,1) WITH NOWAIT;
                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
                    SELECT    60 AS check_id, 
                            i.index_sanity_id,
                            150 AS Priority,
                            N'Abnormal Psychology' AS findings_group,
                            N'XML Indexes' AS finding, 
                            [database_name] AS [Database Name],
                            N'http://BrentOzar.com/go/AbnormalPsychology' AS URL,
                            i.db_schema_object_indexid AS details, 
                            i.index_definition,
                            i.secret_columns,
                            N'' AS index_usage_summary,
                            ISNULL(sz.index_size_summary,'') AS index_size_summary
                    FROM    #IndexSanity AS i
                    JOIN #IndexSanitySize sz ON i.index_sanity_id = sz.index_sanity_id
                    WHERE i.is_XML = 1 OPTION    ( RECOMPILE );

            RAISERROR(N'check_id 61: Columnstore indexes', 0,1) WITH NOWAIT;
                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
                    SELECT    61 AS check_id, 
                            i.index_sanity_id,
                            150 AS Priority,
                            N'Abnormal Psychology' AS findings_group,
                            CASE WHEN i.is_NC_columnstore=1
                                THEN N'NC Columnstore Index' 
                                ELSE N'Clustered Columnstore Index' 
                                END AS finding, 
                            [database_name] AS [Database Name],
                            N'http://BrentOzar.com/go/AbnormalPsychology' AS URL,
                            i.db_schema_object_indexid AS details, 
                            i.index_definition,
                            i.secret_columns,
                            i.index_usage_summary,
                            ISNULL(sz.index_size_summary,'') AS index_size_summary
                    FROM    #IndexSanity AS i
                    JOIN #IndexSanitySize sz ON i.index_sanity_id = sz.index_sanity_id
                    WHERE i.is_NC_columnstore = 1 OR i.is_CX_columnstore=1
                    OPTION    ( RECOMPILE );


            RAISERROR(N'check_id 62: Spatial indexes', 0,1) WITH NOWAIT;
                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
                    SELECT    62 AS check_id, 
                            i.index_sanity_id,
                            150 AS Priority,
                            N'Abnormal Psychology' AS findings_group,
                            N'Spatial indexes' AS finding,
                            [database_name] AS [Database Name], 
                            N'http://BrentOzar.com/go/AbnormalPsychology' AS URL,
                            i.db_schema_object_indexid AS details, 
                            i.index_definition,
                            i.secret_columns,
                            i.index_usage_summary,
                            ISNULL(sz.index_size_summary,'') AS index_size_summary
                    FROM    #IndexSanity AS i
                    JOIN #IndexSanitySize sz ON i.index_sanity_id = sz.index_sanity_id
                    WHERE i.is_spatial = 1 OPTION    ( RECOMPILE );

            RAISERROR(N'check_id 63: Compressed indexes', 0,1) WITH NOWAIT;
                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
                    SELECT    63 AS check_id, 
                            i.index_sanity_id,
                            150 AS Priority,
                            N'Abnormal Psychology' AS findings_group,
                            N'Compressed indexes' AS finding,
                            [database_name] AS [Database Name], 
                            N'http://BrentOzar.com/go/AbnormalPsychology' AS URL,
                            i.db_schema_object_indexid  + N'. COMPRESSION: ' + sz.data_compression_desc AS details, 
                            i.index_definition,
                            i.secret_columns,
                            i.index_usage_summary,
                            ISNULL(sz.index_size_summary,'') AS index_size_summary
                    FROM    #IndexSanity AS i
                    JOIN #IndexSanitySize sz ON i.index_sanity_id = sz.index_sanity_id
                    WHERE sz.data_compression_desc LIKE '%PAGE%' OR sz.data_compression_desc LIKE '%ROW%' OPTION    ( RECOMPILE );

            RAISERROR(N'check_id 64: Partitioned', 0,1) WITH NOWAIT;
                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
                    SELECT    64 AS check_id, 
                            i.index_sanity_id,
                            150 AS Priority,
                            N'Abnormal Psychology' AS findings_group,
                            N'Partitioned indexes' AS finding,
                            [database_name] AS [Database Name], 
                            N'http://BrentOzar.com/go/AbnormalPsychology' AS URL,
                            i.db_schema_object_indexid AS details, 
                            i.index_definition,
                            i.secret_columns,
                            i.index_usage_summary,
                            ISNULL(sz.index_size_summary,'') AS index_size_summary
                    FROM    #IndexSanity AS i
                    JOIN #IndexSanitySize sz ON i.index_sanity_id = sz.index_sanity_id
                    WHERE i.partition_key_column_name IS NOT NULL OPTION    ( RECOMPILE );

            RAISERROR(N'check_id 65: Non-Aligned Partitioned', 0,1) WITH NOWAIT;
                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
                    SELECT    65 AS check_id, 
                            i.index_sanity_id,
                            150 AS Priority,
                            N'Abnormal Psychology' AS findings_group,
                            N'Non-Aligned index on a partitioned table' AS finding,
                            i.[database_name] AS [Database Name], 
                            N'http://BrentOzar.com/go/AbnormalPsychology' AS URL,
                            i.db_schema_object_indexid AS details, 
                            i.index_definition,
                            i.secret_columns,
                            i.index_usage_summary,
                            ISNULL(sz.index_size_summary,'') AS index_size_summary
                    FROM    #IndexSanity AS i
                    JOIN #IndexSanity AS iParent ON
                        i.[object_id]=iParent.[object_id]
                        AND iParent.index_id IN (0,1) /* could be a partitioned heap or clustered table */
                        AND iParent.partition_key_column_name IS NOT NULL /* parent is partitioned*/         
                    JOIN #IndexSanitySize sz ON i.index_sanity_id = sz.index_sanity_id
                    WHERE i.partition_key_column_name IS NULL 
                        OPTION    ( RECOMPILE );

            RAISERROR(N'check_id 66: Recently created tables/indexes (1 week)', 0,1) WITH NOWAIT;
                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
                    SELECT    66 AS check_id, 
                            i.index_sanity_id,
                            200 AS Priority,
                            N'Abnormal Psychology' AS findings_group,
                            N'Recently created tables/indexes (1 week)' AS finding,
                            [database_name] AS [Database Name], 
                            N'http://BrentOzar.com/go/AbnormalPsychology' AS URL,
                            i.db_schema_object_indexid + N' was created on ' + 
                                CONVERT(NVARCHAR(16),i.create_date,121) + 
                                N'. Tables/indexes which are dropped/created regularly require special methods for index tuning.'
                                     AS details, 
                            i.index_definition,
                            i.secret_columns,
                            i.index_usage_summary,
                            ISNULL(sz.index_size_summary,'') AS index_size_summary
                    FROM    #IndexSanity AS i
                    JOIN #IndexSanitySize sz ON i.index_sanity_id = sz.index_sanity_id
                    WHERE i.create_date &gt;= DATEADD(dd,-7,GETDATE()) 
                    AND NOT (@GetAllDatabases = 1 OR @Mode = 0)
                        OPTION    ( RECOMPILE );

            RAISERROR(N'check_id 67: Recently modified tables/indexes (2 days)', 0,1) WITH NOWAIT;
                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
                    SELECT    67 AS check_id, 
                            i.index_sanity_id,
                            200 AS Priority,
                            N'Abnormal Psychology' AS findings_group,
                            N'Recently modified tables/indexes (2 days)' AS finding,
                            [database_name] AS [Database Name], 
                            N'http://BrentOzar.com/go/AbnormalPsychology' AS URL,
                            i.db_schema_object_indexid + N' was modified on ' + 
                                CONVERT(NVARCHAR(16),i.modify_date,121) + 
                                N'. A large amount of recently modified indexes may mean a lot of rebuilds are occurring each night.'
                                     AS details, 
                            i.index_definition,
                            i.secret_columns,
                            i.index_usage_summary,
                            ISNULL(sz.index_size_summary,'') AS index_size_summary
                    FROM    #IndexSanity AS i
                    JOIN #IndexSanitySize sz ON i.index_sanity_id = sz.index_sanity_id
                    WHERE i.modify_date &gt; DATEADD(dd,-2,GETDATE()) 
                    AND NOT (@GetAllDatabases = 1 OR @Mode = 0)
                    AND /*Exclude recently created tables.*/
                    i.create_date &lt; DATEADD(dd,-7,GETDATE()) 
                        OPTION    ( RECOMPILE );

            RAISERROR(N'check_id 68: Identity columns within 30 percent of the end of range', 0,1) WITH NOWAIT;
            -- Allowed Ranges: 
                --int -2,147,483,648 to 2,147,483,647
                --smallint -32,768 to 32,768
                --tinyint 0 to 255

                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
                        SELECT    68 AS check_id, 
                                i.index_sanity_id, 
                                200 AS Priority,
                                N'Abnormal Psychology' AS findings_group,
                                N'Identity column within ' +                                     
                                    CAST (calc1.percent_remaining AS NVARCHAR(256))
                                    + N' percent  end of range' AS finding,
                                [database_name] AS [Database Name],
                                N'http://BrentOzar.com/go/AbnormalPsychology' AS URL,
                                i.db_schema_object_name + N'.' +  QUOTENAME(ic.column_name)
                                    + N' is an identity with type ' + ic.system_type_name 
                                    + N', last value of ' 
                                        + ISNULL(REPLACE(CONVERT(NVARCHAR(256),CAST(CAST(ic.last_value AS BIGINT) AS MONEY), 1), '.00', ''),N'NULL')
                                    + N', seed of '
                                        + ISNULL(REPLACE(CONVERT(NVARCHAR(256),CAST(CAST(ic.seed_value AS BIGINT) AS MONEY), 1), '.00', ''),N'NULL')
                                    + N', increment of ' + CAST(ic.increment_value AS NVARCHAR(256)) 
                                    + N', and range of ' +
                                        CASE ic.system_type_name WHEN 'int' THEN N'+/- 2,147,483,647'
                                            WHEN 'smallint' THEN N'+/- 32,768'
                                            WHEN 'tinyint' THEN N'0 to 255'
                                        END
                                        AS details,
                                i.index_definition,
                                secret_columns, 
                                ISNULL(i.index_usage_summary,''),
                                ISNULL(ip.index_size_summary,'')
                        FROM    #IndexSanity i
                        JOIN    #IndexColumns ic ON
                            i.object_id=ic.object_id
                            AND i.index_id IN (0,1) /* heaps and cx only */
                            AND ic.is_identity=1
                            AND ic.system_type_name IN ('tinyint', 'smallint', 'int')
                        JOIN    #IndexSanitySize ip ON i.index_sanity_id = ip.index_sanity_id
                        CROSS APPLY (
                            SELECT CAST(CASE WHEN ic.increment_value &gt;= 0
                                    THEN
                                        CASE ic.system_type_name 
                                            WHEN 'int' THEN (2147483647 - (ISNULL(ic.last_value,ic.seed_value) + ic.increment_value)) / 2147483647.*100
                                            WHEN 'smallint' THEN (32768 - (ISNULL(ic.last_value,ic.seed_value) + ic.increment_value)) / 32768.*100
                                            WHEN 'tinyint' THEN ( 255 - (ISNULL(ic.last_value,ic.seed_value) + ic.increment_value)) / 255.*100
                                            ELSE 999
                                        END
                                ELSE --ic.increment_value is negative
                                        CASE ic.system_type_name 
                                            WHEN 'int' THEN ABS(-2147483647 - (ISNULL(ic.last_value,ic.seed_value) + ic.increment_value)) / 2147483647.*100
                                            WHEN 'smallint' THEN ABS(-32768 - (ISNULL(ic.last_value,ic.seed_value) + ic.increment_value)) / 32768.*100
                                            WHEN 'tinyint' THEN ABS( 0 - (ISNULL(ic.last_value,ic.seed_value) + ic.increment_value)) / 255.*100
                                            ELSE -1
                                        END 
                                END AS NUMERIC(5,1)) AS percent_remaining
                                ) AS calc1
                        WHERE    i.index_id IN (1,0)
                            AND calc1.percent_remaining &lt;= 30
                        UNION ALL
                        SELECT    68 AS check_id, 
                                i.index_sanity_id, 
                                200 AS Priority,
                                N'Abnormal Psychology' AS findings_group,
                                N'Identity column using a negative seed or increment other than 1' AS finding,
                                [database_name] AS [Database Name],
                                N'http://BrentOzar.com/go/AbnormalPsychology' AS URL,
                                i.db_schema_object_name + N'.' +  QUOTENAME(ic.column_name)
                                    + N' is an identity with type ' + ic.system_type_name 
                                    + N', last value of ' 
                                        + ISNULL(REPLACE(CONVERT(NVARCHAR(256),CAST(CAST(ic.last_value AS BIGINT) AS MONEY), 1), '.00', ''),N'NULL')
                                    + N', seed of '
                                        + ISNULL(REPLACE(CONVERT(NVARCHAR(256),CAST(CAST(ic.seed_value AS BIGINT) AS MONEY), 1), '.00', ''),N'NULL')
                                    + N', increment of ' + CAST(ic.increment_value AS NVARCHAR(256)) 
                                    + N', and range of ' +
                                        CASE ic.system_type_name WHEN 'int' THEN N'+/- 2,147,483,647'
                                            WHEN 'smallint' THEN N'+/- 32,768'
                                            WHEN 'tinyint' THEN N'0 to 255'
                                        END
                                        AS details,
                                i.index_definition,
                                secret_columns, 
                                ISNULL(i.index_usage_summary,''),
                                ISNULL(ip.index_size_summary,'')
                        FROM    #IndexSanity i
                        JOIN    #IndexColumns ic ON
                            i.object_id=ic.object_id
                            AND i.index_id IN (0,1) /* heaps and cx only */
                            AND ic.is_identity=1
                            AND ic.system_type_name IN ('tinyint', 'smallint', 'int')
                        JOIN    #IndexSanitySize ip ON i.index_sanity_id = ip.index_sanity_id
                        WHERE    i.index_id IN (1,0)
                            AND (ic.seed_value &lt; 0 OR ic.increment_value &lt;&gt; 1)
                        ORDER BY finding, details DESC OPTION    ( RECOMPILE );

            RAISERROR(N'check_id 69: Column collation does not match database collation', 0,1) WITH NOWAIT;
                WITH count_columns AS (
                            SELECT [object_id],
                                COUNT(*) AS column_count
                            FROM #IndexColumns ic
                            WHERE index_id IN (1,0) /*Heap or clustered only*/
                                AND collation_name &lt;&gt; @collation
                            GROUP BY object_id
                            )
                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
                        SELECT    69 AS check_id, 
                                i.index_sanity_id, 
                                150 AS Priority,
                                N'Abnormal Psychology' AS findings_group,
                                N'Column collation does not match database collation' AS finding,
                                [database_name] AS [Database Name],
                                N'http://BrentOzar.com/go/AbnormalPsychology' AS URL,
                                i.db_schema_object_name 
                                    + N' has ' + CAST(column_count AS NVARCHAR(20))
                                    + N' column' + CASE WHEN column_count &gt; 1 THEN 's' ELSE '' END
                                    + N' with a different collation than the db collation of '
                                    + @collation    AS details,
                                i.index_definition,
                                secret_columns, 
                                ISNULL(i.index_usage_summary,''),
                                ISNULL(ip.index_size_summary,'')
                        FROM    #IndexSanity i
                        JOIN    #IndexSanitySize ip ON i.index_sanity_id = ip.index_sanity_id
                        JOIN    count_columns AS cc ON i.[object_id]=cc.[object_id]
                        WHERE    i.index_id IN (1,0)
                        AND NOT (@GetAllDatabases = 1 OR @Mode = 0)
                        ORDER BY i.db_schema_object_name DESC OPTION    ( RECOMPILE );

            RAISERROR(N'check_id 70: Replicated columns', 0,1) WITH NOWAIT;
                WITH count_columns AS (
                            SELECT [object_id],
                                COUNT(*) AS column_count,
                                SUM(CASE is_replicated WHEN 1 THEN 1 ELSE 0 END) AS replicated_column_count
                            FROM #IndexColumns ic
                            WHERE index_id IN (1,0) /*Heap or clustered only*/
                            GROUP BY object_id
                            )
                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
                        SELECT    70 AS check_id, 
                                i.index_sanity_id,
                                200 AS Priority, 
                                N'Abnormal Psychology' AS findings_group,
                                N'Replicated columns' AS finding,
                                [database_name] AS [Database Name],
                                N'http://BrentOzar.com/go/AbnormalPsychology' AS URL,
                                i.db_schema_object_name 
                                    + N' has ' + CAST(replicated_column_count AS NVARCHAR(20))
                                    + N' out of ' + CAST(column_count AS NVARCHAR(20))
                                    + N' column' + CASE WHEN column_count &gt; 1 THEN 's' ELSE '' END
                                    + N' in one or more publications.'
                                        AS details,
                                i.index_definition,
                                secret_columns, 
                                ISNULL(i.index_usage_summary,''),
                                ISNULL(ip.index_size_summary,'')
                        FROM    #IndexSanity i
                        JOIN    #IndexSanitySize ip ON i.index_sanity_id = ip.index_sanity_id
                        JOIN    count_columns AS cc ON i.[object_id]=cc.[object_id]
                        WHERE    i.index_id IN (1,0)
                            AND replicated_column_count &gt; 0
                            AND NOT (@GetAllDatabases = 1 OR @Mode = 0)
                        ORDER BY i.db_schema_object_name DESC OPTION    ( RECOMPILE );

            RAISERROR(N'check_id 71: Cascading updates or cascading deletes.', 0,1) WITH NOWAIT;
                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary, more_info )
            SELECT    71 AS check_id, 
                    NULL AS index_sanity_id,
                    150 AS Priority,
                    N'Abnormal Psychology' AS findings_group,
                    N'Cascading Updates or Deletes' AS finding, 
                    [database_name] AS [Database Name],
                    N'http://BrentOzar.com/go/AbnormalPsychology' AS URL,
                    N'Foreign Key ' + foreign_key_name +
                    N' on ' + QUOTENAME(parent_object_name)  + N'(' + LTRIM(parent_fk_columns) + N')'
                        + N' referencing ' + QUOTENAME(referenced_object_name) + N'(' + LTRIM(referenced_fk_columns) + N')'
                        + N' has settings:'
                        + CASE [delete_referential_action_desc] WHEN N'NO_ACTION' THEN N'' ELSE N' ON DELETE ' +[delete_referential_action_desc] END
                        + CASE [update_referential_action_desc] WHEN N'NO_ACTION' THEN N'' ELSE N' ON UPDATE ' + [update_referential_action_desc] END
                            AS details, 
                    [fk].[database_name] 
                            AS index_definition, 
                    N'N/A' AS secret_columns,
                    N'N/A' AS index_usage_summary,
                    N'N/A' AS index_size_summary,
                    (SELECT TOP 1 more_info FROM #IndexSanity i WHERE i.object_id=fk.parent_object_id)
                        AS more_info
            FROM #ForeignKeys fk
            WHERE ([delete_referential_action_desc] &lt;&gt; N'NO_ACTION'
            OR [update_referential_action_desc] &lt;&gt; N'NO_ACTION')
            AND NOT (@GetAllDatabases = 1 OR @Mode = 0)

			RAISERROR(N'check_id 72: Columnstore indexes with Trace Flag 834', 0,1) WITH NOWAIT;
                IF EXISTS (SELECT * FROM #IndexSanity WHERE index_type IN (5,6))
				AND EXISTS (SELECT * FROM #TraceStatus WHERE TraceFlag = 834 AND status = 1)
				BEGIN
				INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
                    SELECT    72 AS check_id, 
                            i.index_sanity_id,
                            150 AS Priority,
                            N'Abnormal Psychology' AS findings_group,
                            'Columnstore Indexes are being used in conjunction with trace flag 834. Visit the link to see why this can be a bad idea' AS finding, 
                            [database_name] AS [Database Name],
                            N'https://support.microsoft.com/en-us/kb/3210239' AS URL,
                            i.db_schema_object_indexid AS details, 
                            i.index_definition,
                            i.secret_columns,
                            i.index_usage_summary,
                            ISNULL(sz.index_size_summary,'') AS index_size_summary
                    FROM    #IndexSanity AS i
                    JOIN #IndexSanitySize sz ON i.index_sanity_id = sz.index_sanity_id
                    WHERE i.index_type IN (5,6)
                    OPTION    ( RECOMPILE )
				END

    END

         ----------------------------------------
        --Workaholics: Check_id 80-89
        ----------------------------------------
    BEGIN

        RAISERROR(N'check_id 80: Most scanned indexes (index_usage_stats)', 0,1) WITH NOWAIT;
                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )

        --Workaholics according to index_usage_stats
        --This isn't perfect: it mentions the number of scans present in a plan
        --A "scan" isn't necessarily a full scan, but hey, we gotta do the best with what we've got.
        --in the case of things like indexed views, the operator might be in the plan but never executed
        SELECT TOP 5 
            80 AS check_id,
            i.index_sanity_id AS index_sanity_id,
            200 AS Priority,
            N'Workaholics' AS findings_group,
            N'Scan-a-lots (index_usage_stats)' AS finding,
            [database_name] AS [Database Name],
            N'http://BrentOzar.com/go/Workaholics' AS URL,
            REPLACE(CONVERT( NVARCHAR(50),CAST(i.user_scans AS MONEY),1),'.00','')
                + N' scans against ' + i.db_schema_object_indexid
                + N'. Latest scan: ' + ISNULL(CAST(i.last_user_scan AS NVARCHAR(128)),'?') + N'. ' 
                + N'ScanFactor=' + CAST(((i.user_scans * iss.total_reserved_MB)/1000000.) AS NVARCHAR(256)) AS details,
            ISNULL(i.key_column_names_with_sort_order,'N/A') AS index_definition,
            ISNULL(i.secret_columns,'') AS secret_columns,
            i.index_usage_summary AS index_usage_summary,
            iss.index_size_summary AS index_size_summary
        FROM #IndexSanity i
        JOIN #IndexSanitySize iss ON i.index_sanity_id=iss.index_sanity_id
        WHERE ISNULL(i.user_scans,0) &gt; 0
        AND NOT (@GetAllDatabases = 1 OR @Mode = 0)
        ORDER BY  i.user_scans * iss.total_reserved_MB DESC;

        RAISERROR(N'check_id 81: Top recent accesses (op stats)', 0,1) WITH NOWAIT;
                INSERT    #BlitzIndexResults ( check_id, index_sanity_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
        --Workaholics according to index_operational_stats
        --This isn't perfect either: range_scan_count contains full scans, partial scans, even seeks in nested loop ops
        --But this can help bubble up some most-accessed tables 
        SELECT TOP 5 
            81 AS check_id,
            i.index_sanity_id AS index_sanity_id,
            200 AS Priority,
            N'Workaholics' AS findings_group,
            N'Top recent accesses (index_op_stats)' AS finding,
            [database_name] AS [Database Name],
            N'http://BrentOzar.com/go/Workaholics' AS URL,
            ISNULL(REPLACE(
                    CONVERT(NVARCHAR(50),CAST((iss.total_range_scan_count + iss.total_singleton_lookup_count) AS MONEY),1),
                    N'.00',N'') 
                + N' uses of ' + i.db_schema_object_indexid + N'. '
                + REPLACE(CONVERT(NVARCHAR(50), CAST(iss.total_range_scan_count AS MONEY),1),N'.00',N'') + N' scans or seeks. '
                + REPLACE(CONVERT(NVARCHAR(50), CAST(iss.total_singleton_lookup_count AS MONEY), 1),N'.00',N'') + N' singleton lookups. '
                + N'OpStatsFactor=' + CAST(((((iss.total_range_scan_count + iss.total_singleton_lookup_count) * iss.total_reserved_MB))/1000000.) AS VARCHAR(256)),'') AS details,
            ISNULL(i.key_column_names_with_sort_order,'N/A') AS index_definition,
            ISNULL(i.secret_columns,'') AS secret_columns,
            i.index_usage_summary AS index_usage_summary,
            iss.index_size_summary AS index_size_summary
        FROM #IndexSanity i
        JOIN #IndexSanitySize iss ON i.index_sanity_id=iss.index_sanity_id
        WHERE (ISNULL(iss.total_range_scan_count,0)  &gt; 0 OR ISNULL(iss.total_singleton_lookup_count,0) &gt; 0)
        AND NOT (@GetAllDatabases = 1 OR @Mode = 0)
        ORDER BY ((iss.total_range_scan_count + iss.total_singleton_lookup_count) * iss.total_reserved_MB) DESC;


    END

         ----------------------------------------
        --Statistics Info: Check_id 90-99
        ----------------------------------------
    BEGIN

        RAISERROR(N'check_id 90: Outdated statistics', 0,1) WITH NOWAIT;
                INSERT    #BlitzIndexResults ( check_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
		SELECT  90 AS check_id, 
				200 AS Priority,
				'Functioning Statistaholics' AS findings_group,
				'Statistic Abandonment Issues',
				s.database_name,
				'' AS URL,
				'Statistics on this table were last updated ' + 
					CASE s.last_statistics_update WHEN NULL THEN N' NEVER '
					ELSE CONVERT(NVARCHAR(20), s.last_statistics_update) + 
						' have had ' + CONVERT(NVARCHAR(100), s.modification_counter) +
						' modifications in that time, which is ' +
						CONVERT(NVARCHAR(100), s.percent_modifications) + 
						'% of the table.'
					END,
				QUOTENAME(database_name) + '.' + QUOTENAME(s.schema_name) + '.' + QUOTENAME(s.table_name) + '.' + QUOTENAME(s.index_name) + '.' + QUOTENAME(s.statistics_name) + '.' + QUOTENAME(s.column_name) AS index_definition,
				'N/A' AS secret_columns,
				'N/A' AS index_usage_summary,
				'N/A' AS index_size_summary
		FROM #Statistics AS s
		WHERE s.last_statistics_update &lt;= CONVERT(DATETIME, GETDATE() - 7) 
		AND s.percent_modifications &gt;= 10. 
		AND s.rows &gt;= 10000

        RAISERROR(N'check_id 91: Statistics with a low sample rate', 0,1) WITH NOWAIT;
                INSERT    #BlitzIndexResults ( check_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
		SELECT  91 AS check_id, 
				200 AS Priority,
				'Functioning Statistaholics' AS findings_group,
				'Antisocial Samples',
				s.database_name,
				'' AS URL,
				'Only ' + CONVERT(NVARCHAR(100), s.percent_sampled) + '% of the rows were sampled during the last statistics update. This may lead to poor cardinality estimates.' ,
				QUOTENAME(database_name) + '.' + QUOTENAME(s.schema_name) + '.' + QUOTENAME(s.table_name) + '.' + QUOTENAME(s.index_name) + '.' + QUOTENAME(s.statistics_name) + '.' + QUOTENAME(s.column_name) AS index_definition,
				'N/A' AS secret_columns,
				'N/A' AS index_usage_summary,
				'N/A' AS index_size_summary
		FROM #Statistics AS s
		WHERE s.rows_sampled &lt; 1.
		AND s.rows &gt;= 10000

        RAISERROR(N'check_id 92: Statistics with NO RECOMPUTE', 0,1) WITH NOWAIT;
                INSERT    #BlitzIndexResults ( check_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
		SELECT  92 AS check_id, 
				200 AS Priority,
				'Functioning Statistaholics' AS findings_group,
				'Cyberphobic Samples',
				s.database_name,
				'' AS URL,
				'The statistic ' + QUOTENAME(s.statistics_name) +  ' is set to not recompute. This can be helpful if data is really skewed, but harmful if you expect automatic statistics updates.' ,
				QUOTENAME(database_name) + '.' + QUOTENAME(s.schema_name) + '.' + QUOTENAME(s.table_name) + '.' + QUOTENAME(s.index_name) + '.' + QUOTENAME(s.statistics_name) + '.' + QUOTENAME(s.column_name) AS index_definition,
				'N/A' AS secret_columns,
				'N/A' AS index_usage_summary,
				'N/A' AS index_size_summary
		FROM #Statistics AS s
		WHERE s.no_recompute = 1

        RAISERROR(N'check_id 93: Statistics with filters', 0,1) WITH NOWAIT;
                INSERT    #BlitzIndexResults ( check_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
		SELECT  93 AS check_id, 
				200 AS Priority,
				'Functioning Statistaholics' AS findings_group,
				'Filter Fixation',
				s.database_name,
				'' AS URL,
				'The statistic ' + QUOTENAME(s.statistics_name) +  ' is filtered on [' + s.filter_definition + ']. It could be part of a filtered index, or just a filtered statistic. This is purely informational.' ,
				QUOTENAME(database_name) + '.' + QUOTENAME(s.schema_name) + '.' + QUOTENAME(s.table_name) + '.' + QUOTENAME(s.index_name) + '.' + QUOTENAME(s.statistics_name) + '.' + QUOTENAME(s.column_name) AS index_definition,
				'N/A' AS secret_columns,
				'N/A' AS index_usage_summary,
				'N/A' AS index_size_summary
		FROM #Statistics AS s
		WHERE s.has_filter = 1

		END 

         ----------------------------------------
        --Computed Column Info: Check_id 90-99
        ----------------------------------------
    BEGIN

	     RAISERROR(N'check_id 99: Computed Columns That Reference Functions', 0,1) WITH NOWAIT;
                INSERT    #BlitzIndexResults ( check_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
		SELECT  99 AS check_id, 
				50 AS Priority,
				'Cold Calculators' AS findings_group,
				'Serial Forcer' AS finding,
				cc.database_name,
				'' AS URL,
				'The computed column ' + QUOTENAME(cc.column_name) + ' on ' + QUOTENAME(cc.schema_name) + '.' + QUOTENAME(cc.table_name) + ' is based on ' + cc.definition 
				+ '. That indicates it may reference a scalar function, or a CLR function with data access, which can cause all queries and maintenance to run serially.' AS details,
				cc.column_definition,
				'N/A' AS secret_columns,
				'N/A' AS index_usage_summary,
				'N/A' AS index_size_summary
		FROM #ComputedColumns AS cc
		WHERE cc.is_function = 1

		RAISERROR(N'check_id 100: Computed Columns that are not Persisted.', 0,1) WITH NOWAIT;
        INSERT    #BlitzIndexResults ( check_id, Priority, findings_group, finding, [database_name], URL, details, index_definition,
                                               secret_columns, index_usage_summary, index_size_summary )
		SELECT  100 AS check_id, 
				200 AS Priority,
				'Cold Calculators' AS findings_group,
				'Definition Defeatists' AS finding,
				cc.database_name,
				'' AS URL,
				'The computed column ' + QUOTENAME(cc.column_name) + ' on ' + QUOTENAME(cc.schema_name) + '.' + QUOTENAME(cc.table_name) + ' is not persisted, which means it will be calculated when a query runs.' + 
				'You can change this with the following command, if the definition is deterministic: ALTER TABLE ' + QUOTENAME(cc.schema_name) + '.' + QUOTENAME(cc.table_name) + ' ALTER COLUMN ' + cc.column_name +
				' ADD PERSISTED'  AS details,
				cc.column_definition,
				'N/A' AS secret_columns,
				'N/A' AS index_usage_summary,
				'N/A' AS index_size_summary
		FROM #ComputedColumns AS cc
		WHERE cc.is_persisted = 0

	END 
 
        RAISERROR(N'Insert a row to help people find help', 0,1) WITH NOWAIT;
        IF DATEDIFF(MM, @VersionDate, GETDATE()) &gt; 6
		BEGIN
            INSERT    #BlitzIndexResults ( Priority, check_id, findings_group, finding, URL, details, index_definition,
                                            index_usage_summary, index_size_summary )
            VALUES  ( -1, 0 , 
		           'Outdated sp_BlitzIndex', 'sp_BlitzIndex is Over 6 Months Old', 'http://FirstResponderKit.org/', 
                   'Fine wine gets better with age, but this ' + @ScriptVersionName + ' is more like bad cheese. Time to get a new one.',
                    N'',N'',N''
                    );
        END

        IF EXISTS(SELECT * FROM #BlitzIndexResults)
		BEGIN
            INSERT    #BlitzIndexResults ( Priority, check_id, findings_group, finding, URL, details, index_definition,
                                            index_usage_summary, index_size_summary )
            VALUES  ( -1, 0 , 
		            @ScriptVersionName,
                    CASE WHEN @GetAllDatabases = 1 THEN N'All Databases' ELSE N'Database ' + QUOTENAME(@DatabaseName) + N' as of ' + CONVERT(NVARCHAR(16),GETDATE(),121) END, 
                    N'From Your Community Volunteers' ,   N'http://FirstResponderKit.org' ,
                    N''
                    , N'',N''
                    );
        END
        ELSE IF @Mode = 0 OR (@GetAllDatabases = 1 AND @Mode &lt;&gt; 4)
        BEGIN
            INSERT    #BlitzIndexResults ( Priority, check_id, findings_group, finding, URL, details, index_definition,
                                            index_usage_summary, index_size_summary )
            VALUES  ( -1, 0 , 
		            @ScriptVersionName,
                    CASE WHEN @GetAllDatabases = 1 THEN N'All Databases' ELSE N'Database ' + QUOTENAME(@DatabaseName) + N' as of ' + CONVERT(NVARCHAR(16),GETDATE(),121) END, 
                    N'From Your Community Volunteers' ,   N'http://FirstResponderKit.org' ,
                    N''
                    , N'',N''
                    );
            INSERT    #BlitzIndexResults ( Priority, check_id, findings_group, finding, URL, details, index_definition,
                                            index_usage_summary, index_size_summary )
            VALUES  ( 1, 0 , 
		           'No Major Problems Found',
                   'Nice Work!',
                   'http://FirstResponderKit.org', 'Consider running with @Mode = 4 in individual databases (not all) for more detailed diagnostics.', 'The new default Mode 0 only looks for very serious index issues.', '', ''
                    );

        END
        ELSE
        BEGIN
            INSERT    #BlitzIndexResults ( Priority, check_id, findings_group, finding, URL, details, index_definition,
                                            index_usage_summary, index_size_summary )
            VALUES  ( -1, 0 , 
		            @ScriptVersionName,
                    CASE WHEN @GetAllDatabases = 1 THEN N'All Databases' ELSE N'Database ' + QUOTENAME(@DatabaseName) + N' as of ' + CONVERT(NVARCHAR(16),GETDATE(),121) END, 
                    N'From Your Community Volunteers' ,   N'http://www.BrentOzar.com/BlitzIndex' ,
                    N''
                    , N'',N''
                    );
            INSERT    #BlitzIndexResults ( Priority, check_id, findings_group, finding, URL, details, index_definition,
                                            index_usage_summary, index_size_summary )
            VALUES  ( 1, 0 , 
		           'No Problems Found',
                   'Nice job! Or more likely, you have a nearly empty database.',
                   'http://FirstResponderKit.org', 'Time to go read some blog posts.', '', '', ''
                    );

        END

        RAISERROR(N'Returning results.', 0,1) WITH NOWAIT;
            
        /*Return results.*/
        IF (@Mode = 0)
        BEGIN

            SELECT Priority, ISNULL(br.findings_group,N'') + 
                    CASE WHEN ISNULL(br.finding,N'') &lt;&gt; N'' THEN N': ' ELSE N'' END
                    + br.finding AS [Finding], 
                br.[database_name] AS [Database Name],
                br.details AS [Details: schema.table.index(indexid)], 
                br.index_definition AS [Definition: [Property]] ColumnName {datatype maxbytes}], 
                ISNULL(br.secret_columns,'') AS [Secret Columns],          
                br.index_usage_summary AS [Usage], 
                br.index_size_summary AS [Size],
                COALESCE(br.more_info,sn.more_info,'') AS [More Info],
                br.URL, 
                COALESCE(br.create_tsql,ts.create_tsql,'') AS [Create TSQL]
            FROM #BlitzIndexResults br
            LEFT JOIN #IndexSanity sn ON 
                br.index_sanity_id=sn.index_sanity_id
            LEFT JOIN #IndexCreateTsql ts ON 
                br.index_sanity_id=ts.index_sanity_id
            WHERE br.check_id IN (0, 1, 11, 22, 43, 68, 50, 60, 61, 62, 63, 64, 65, 72)
            ORDER BY br.Priority ASC, br.check_id ASC, br.blitz_result_id ASC, br.findings_group ASC
			OPTION (RECOMPILE);

        END
        ELSE IF (@Mode = 4)
            SELECT Priority, ISNULL(br.findings_group,N'') + 
                    CASE WHEN ISNULL(br.finding,N'') &lt;&gt; N'' THEN N': ' ELSE N'' END
                    + br.finding AS [Finding], 
				br.[database_name] AS [Database Name],
                br.details AS [Details: schema.table.index(indexid)], 
                br.index_definition AS [Definition: [Property]] ColumnName {datatype maxbytes}], 
                ISNULL(br.secret_columns,'') AS [Secret Columns],          
                br.index_usage_summary AS [Usage], 
                br.index_size_summary AS [Size],
                COALESCE(br.more_info,sn.more_info,'') AS [More Info],
                br.URL, 
                COALESCE(br.create_tsql,ts.create_tsql,'') AS [Create TSQL]
            FROM #BlitzIndexResults br
            LEFT JOIN #IndexSanity sn ON 
                br.index_sanity_id=sn.index_sanity_id
            LEFT JOIN #IndexCreateTsql ts ON 
                br.index_sanity_id=ts.index_sanity_id
            ORDER BY br.Priority ASC, br.check_id ASC, br.blitz_result_id ASC, br.findings_group ASC
			OPTION (RECOMPILE);

    END; /* End @Mode=0 or 4 (diagnose)*/
    ELSE IF @Mode=1 /*Summarize*/
    BEGIN
    --This mode is to give some overall stats on the database.
        RAISERROR(N'@Mode=1, we are summarizing.', 0,1) WITH NOWAIT;

        SELECT DB_NAME(i.database_id) AS [Database Name],
            CAST((COUNT(*)) AS NVARCHAR(256)) AS [Number Objects],
            CAST(CAST(SUM(sz.total_reserved_MB)/
                1024. AS NUMERIC(29,1)) AS NVARCHAR(500)) AS [All GB],
            CAST(CAST(SUM(sz.total_reserved_LOB_MB)/
                1024. AS NUMERIC(29,1)) AS NVARCHAR(500)) AS [LOB GB],
            CAST(CAST(SUM(sz.total_reserved_row_overflow_MB)/
                1024. AS NUMERIC(29,1)) AS NVARCHAR(500)) AS [Row Overflow GB],
            CAST(SUM(CASE WHEN index_id=1 THEN 1 ELSE 0 END)AS NVARCHAR(50)) AS [Clustered Tables],
            CAST(SUM(CASE WHEN index_id=1 THEN sz.total_reserved_MB ELSE 0 END)
                /1024. AS NUMERIC(29,1)) AS [Clustered Tables GB],
            SUM(CASE WHEN index_id NOT IN (0,1) THEN 1 ELSE 0 END) AS [NC Indexes],
            CAST(SUM(CASE WHEN index_id NOT IN (0,1) THEN sz.total_reserved_MB ELSE 0 END)
                /1024. AS NUMERIC(29,1)) AS [NC Indexes GB],
            CASE WHEN SUM(CASE WHEN index_id NOT IN (0,1) THEN sz.total_reserved_MB ELSE 0 END)  &gt; 0 THEN
                CAST(SUM(CASE WHEN index_id IN (0,1) THEN sz.total_reserved_MB ELSE 0 END)
                    / SUM(CASE WHEN index_id NOT IN (0,1) THEN sz.total_reserved_MB ELSE 0 END) AS NUMERIC(29,1)) 
                ELSE 0 END AS [ratio table: NC Indexes],
            SUM(CASE WHEN index_id=0 THEN 1 ELSE 0 END) AS [Heaps],
            CAST(SUM(CASE WHEN index_id=0 THEN sz.total_reserved_MB ELSE 0 END)
                /1024. AS NUMERIC(29,1)) AS [Heaps GB],
            SUM(CASE WHEN index_id IN (0,1) AND partition_key_column_name IS NOT NULL THEN 1 ELSE 0 END) AS [Partitioned Tables],
            SUM(CASE WHEN index_id NOT IN (0,1) AND  partition_key_column_name IS NOT NULL THEN 1 ELSE 0 END) AS [Partitioned NCs],
            CAST(SUM(CASE WHEN partition_key_column_name IS NOT NULL THEN sz.total_reserved_MB ELSE 0 END)/1024. AS NUMERIC(29,1)) AS [Partitioned GB],
            SUM(CASE WHEN filter_definition &lt;&gt; '' THEN 1 ELSE 0 END) AS [Filtered Indexes],
            SUM(CASE WHEN is_indexed_view=1 THEN 1 ELSE 0 END) AS [Indexed Views],
            MAX(total_rows) AS [Max Row Count],
            CAST(MAX(CASE WHEN index_id IN (0,1) THEN sz.total_reserved_MB ELSE 0 END)
                /1024. AS NUMERIC(29,1)) AS [Max Table GB],
            CAST(MAX(CASE WHEN index_id NOT IN (0,1) THEN sz.total_reserved_MB ELSE 0 END)
                /1024. AS NUMERIC(29,1)) AS [Max NC Index GB],
            SUM(CASE WHEN index_id IN (0,1) AND sz.total_reserved_MB &gt; 1024 THEN 1 ELSE 0 END) AS [Count Tables &gt; 1GB],
            SUM(CASE WHEN index_id IN (0,1) AND sz.total_reserved_MB &gt; 10240 THEN 1 ELSE 0 END) AS [Count Tables &gt; 10GB],
            SUM(CASE WHEN index_id IN (0,1) AND sz.total_reserved_MB &gt; 102400 THEN 1 ELSE 0 END) AS [Count Tables &gt; 100GB],    
            SUM(CASE WHEN index_id NOT IN (0,1) AND sz.total_reserved_MB &gt; 1024 THEN 1 ELSE 0 END) AS [Count NCs &gt; 1GB],
            SUM(CASE WHEN index_id NOT IN (0,1) AND sz.total_reserved_MB &gt; 10240 THEN 1 ELSE 0 END) AS [Count NCs &gt; 10GB],
            SUM(CASE WHEN index_id NOT IN (0,1) AND sz.total_reserved_MB &gt; 102400 THEN 1 ELSE 0 END) AS [Count NCs &gt; 100GB],
            MIN(create_date) AS [Oldest Create Date],
            MAX(create_date) AS [Most Recent Create Date],
            MAX(modify_date) AS [Most Recent Modify Date],
            1 AS [Display Order]
        FROM #IndexSanity AS i
        --left join here so we don't lose disabled nc indexes
        LEFT JOIN #IndexSanitySize AS sz 
            ON i.index_sanity_id=sz.index_sanity_id
		GROUP BY DB_NAME(i.database_id)	 
        UNION ALL
        SELECT  CASE WHEN @GetAllDatabases = 1 THEN N'All Databases' ELSE N'Database ' + N' as of ' + CONVERT(NVARCHAR(16),GETDATE(),121) END,        
                @ScriptVersionName,   
                N'From Your Community Volunteers' ,   
                N'http://FirstResponderKit.org' ,
                N'',
                NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,
                NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,
                NULL,NULL,0 AS display_order
        ORDER BY [Display Order] ASC
        OPTION (RECOMPILE);
           
    END /* End @Mode=1 (summarize)*/
    ELSE IF @Mode=2 /*Index Detail*/
    BEGIN
        --This mode just spits out all the detail without filters.
        --This supports slicing AND dicing in Excel
        RAISERROR(N'@Mode=2, here''s the details on existing indexes.', 0,1) WITH NOWAIT;

        SELECT  [database_name] AS [Database Name], 
                [schema_name] AS [Schema Name], 
                [object_name] AS [Object Name], 
                ISNULL(index_name, '') AS [Index Name], 
                CAST(index_id AS VARCHAR(10))AS [Index ID],
                db_schema_object_indexid AS [Details: schema.table.index(indexid)], 
                CASE    WHEN index_id IN ( 1, 0 ) THEN 'TABLE'
                    ELSE 'NonClustered'
                    END AS [Object Type], 
                index_definition AS [Definition: [Property]] ColumnName {datatype maxbytes}],
                ISNULL(LTRIM(key_column_names_with_sort_order), '') AS [Key Column Names With Sort],
                ISNULL(count_key_columns, 0) AS [Count Key Columns],
                ISNULL(include_column_names, '') AS [Include Column Names], 
                ISNULL(count_included_columns,0) AS [Count Included Columns],
                ISNULL(secret_columns,'') AS [Secret Column Names], 
                ISNULL(count_secret_columns,0) AS [Count Secret Columns],
                ISNULL(partition_key_column_name, '') AS [Partition Key Column Name],
                ISNULL(filter_definition, '') AS [Filter Definition], 
                is_indexed_view AS [Is Indexed View], 
                is_primary_key AS [Is Primary Key],
                is_XML AS [Is XML],
                is_spatial AS [Is Spatial],
                is_NC_columnstore AS [Is NC Columnstore],
                is_CX_columnstore AS [Is CX Columnstore],
                is_disabled AS [Is Disabled], 
                is_hypothetical AS [Is Hypothetical],
                is_padded AS [Is Padded], 
                fill_factor AS [Fill Factor], 
                is_referenced_by_foreign_key AS [Is Reference by Foreign Key], 
                last_user_seek AS [Last User Seek], 
                last_user_scan AS [Last User Scan], 
                last_user_lookup AS [Last User Lookup],
                last_user_update AS [Last User Update], 
                total_reads AS [Total Reads], 
                user_updates AS [User Updates], 
                reads_per_write AS [Reads Per Write], 
                index_usage_summary AS [Index Usage], 
                sz.partition_count AS [Partition Count],
                sz.total_rows AS [Rows], 
                sz.total_reserved_MB AS [Reserved MB], 
                sz.total_reserved_LOB_MB AS [Reserved LOB MB], 
                sz.total_reserved_row_overflow_MB AS [Reserved Row Overflow MB],
                sz.index_size_summary AS [Index Size], 
                sz.total_row_lock_count AS [Row Lock Count],
                sz.total_row_lock_wait_count AS [Row Lock Wait Count],
                sz.total_row_lock_wait_in_ms AS [Row Lock Wait ms],
                sz.avg_row_lock_wait_in_ms AS [Avg Row Lock Wait ms],
                sz.total_page_lock_count AS [Page Lock Count],
                sz.total_page_lock_wait_count AS [Page Lock Wait Count],
                sz.total_page_lock_wait_in_ms AS [Page Lock Wait ms],
                sz.avg_page_lock_wait_in_ms AS [Avg Page Lock Wait ms],
                sz.total_index_lock_promotion_attempt_count AS [Lock Escalation Attempts],
                sz.total_index_lock_promotion_count AS [Lock Escalations],
                sz.data_compression_desc AS [Data Compression],
                i.create_date AS [Create Date],
                i.modify_date AS [Modify Date],
                more_info AS [More Info],
                1 AS [Display Order]
        FROM    #IndexSanity AS i --left join here so we don't lose disabled nc indexes
                LEFT JOIN #IndexSanitySize AS sz ON i.index_sanity_id = sz.index_sanity_id
        ORDER BY [Database Name], [Schema Name], [Object Name], [Index ID]
        OPTION (RECOMPILE);



    END /* End @Mode=2 (index detail)*/
    ELSE IF @Mode=3 /*Missing index Detail*/
    BEGIN
        SELECT 
            database_name AS [Database Name], 
            [schema_name] AS [Schema], 
            table_name AS [Table], 
            CAST((magic_benefit_number/@DaysUptime) AS BIGINT)
                AS [Magic Benefit Number], 
            missing_index_details AS [Missing Index Details], 
            avg_total_user_cost AS [Avg Query Cost], 
            avg_user_impact AS [Est Index Improvement], 
            user_seeks AS [Seeks], 
            user_scans AS [Scans],
            unique_compiles AS [Compiles], 
            equality_columns AS [Equality Columns], 
            inequality_columns AS [Inequality Columns], 
            included_columns AS [Included Columns], 
            index_estimated_impact AS [Estimated Impact], 
            create_tsql AS [Create TSQL], 
            more_info AS [More Info],
            1 AS [Display Order]
        FROM #MissingIndexes
        /* Minimum benefit threshold = 100k/day of uptime */
        WHERE (magic_benefit_number/@DaysUptime) &gt;= 100000
        UNION ALL
        SELECT                 
            @ScriptVersionName,   
            N'From Your Community Volunteers' ,   
            N'http://FirstResponderKit.org' ,
            100000000000,
            N'',
            NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,
            NULL, 0 AS display_order
        ORDER BY [Display Order] ASC, [Magic Benefit Number] DESC
		OPTION (RECOMPILE);

    END /* End @Mode=3 (index detail)*/
END
END TRY

BEGIN CATCH
        RAISERROR (N'Failure analyzing temp tables.', 0,1) WITH NOWAIT;

        SELECT    @msg = ERROR_MESSAGE(), @ErrorSeverity = ERROR_SEVERITY(), @ErrorState = ERROR_STATE();

        RAISERROR (@msg, 
               @ErrorSeverity, 
               @ErrorState 
               );
        
        WHILE @@trancount &gt; 0 
            ROLLBACK;

        RETURN;
    END CATCH;
GO
</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>CODE_Empty_ASP.NET_Core_Web_Application</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>CORE_01__Views/App/Shop.cshtml</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>CODE_Empty_ASP.NET_Core_Web_Application</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>CORE_01__Views/App/Shop.cshtml</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>CORE_01__Views/App/Shop.cshtml

//could use with the entire name space: 
										@model IEnumerable&lt;DutchTreat.Data.Entities.Product&gt;
// but instead add the namespace (DutchTreat.Data.Entities) into _ViewImports.cshtml
// Putting the model in the first line is not necessary, but it gives us Intellisense on the model
// "@Model" in the razor code represents the IEnumerable object that is passed into the page
---------------------------- v1: ran this before I seeded the database, and it works, but there is no data

@model IEnumerable&lt;Product&gt;
&lt;h1&gt;Shop&lt;/h1&gt;
&lt;p&gt;Count: @Model.Count()&lt;/p&gt;
@foreach (var p in Model)
{

        &lt;li&gt;Title: @p.Title&lt;/li&gt;

}



----------------------------

@model IEnumerable&lt;Product&gt;
&lt;h1&gt;Shop&lt;/h1&gt;
&lt;p&gt;Count: @Model.Count()&lt;/p&gt;
@foreach (var p in Model)
{
  &lt;div class="col-md-3"&gt;
    &lt;div class="well well-sm"&gt;
      &lt;img src="~/img/@(p.ArtId).jpg" class="img-responsive" alt="@p.Title" /&gt;
      &lt;h3&gt;@p.Category - @p.Size&lt;/h3&gt;
      &lt;ul class="product-props"&gt;
        &lt;li&gt;Price: @p.Price&lt;/li&gt;
        &lt;li&gt;Artist: @p.Artist&lt;/li&gt;
        &lt;li&gt;Title: @p.Title&lt;/li&gt;
        &lt;li&gt;Description: @p.ArtDescription&lt;/li&gt;
      &lt;/ul&gt;
      &lt;button id="buyButton"&gt;Buy&lt;/button&gt;
    &lt;/div&gt;
  &lt;/div&gt;
}</Code>
      </SnippetValue>
    </value>
  </item>
  <item>
    <key>
      <SnippetKey xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Category>CODE_Empty_ASP.NET_Core_Web_Application</Category>
        <Language>C#</Language>
        <Public>false</Public>
        <Name>CORE_01_about_SeedingTheDatabase</Name>
        <Group>My Snippets</Group>
      </SnippetKey>
    </key>
    <value>
      <SnippetValue xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
        <Key>
          <Category>CODE_Empty_ASP.NET_Core_Web_Application</Category>
          <Language>C#</Language>
          <Public>false</Public>
          <Name>CORE_01_about_SeedingTheDatabase</Name>
          <Group>My Snippets</Group>
        </Key>
        <Keywords />
        <Imports />
        <Code>CORE_01_about_SeedingTheDatabase

/*
1] Create a class: DutchSeeder, in the Data folder
	then create a constructor that takes a DbContext (ctx), and a local variable
</Code>
      </SnippetValue>
    </value>
  </item>
</dictionary>